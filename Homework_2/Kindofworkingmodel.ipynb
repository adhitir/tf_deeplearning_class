{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.legacy_seq2seq as seq2seq\n",
    "from utilities import show_graph\n",
    "#from util import inv_sigmoid, linear_decay, dec_print_train, dec_print_val, dec_print_test\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import collections\n",
    "import json\n",
    "import string\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "n_inputs        = 4096\n",
    "n_hidden        = 600\n",
    "val_batch_size  = 100 #100\n",
    "n_frames        = 80\n",
    "max_caption_len = 50\n",
    "forget_bias_red = 1.0\n",
    "forget_bias_gre = 1.0\n",
    "dropout_prob    = 0.5\n",
    "n_attention     = n_hidden\n",
    "\n",
    "special_tokens  = {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "phases = {'train': 0, 'val': 1, 'test': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2VT:\n",
    "    def __init__(self, vocab_num = 0,lr = 1e-4):\n",
    "\n",
    "        self.vocab_num = vocab_num\n",
    "        self.learning_rate = lr\n",
    "        self.saver = None\n",
    "\n",
    "    def set_saver(self, saver):\n",
    "        self.saver = saver\n",
    "     \n",
    "    def build_model(self, feat, captions=None, cap_len=None, sampling=None, phase=0):\n",
    "\n",
    "        weights = {\n",
    "            'W_feat': tf.Variable( tf.random_uniform([n_inputs, n_hidden], -0.1, 0.1), name='W_feat'), \n",
    "            'W_dec': tf.Variable(tf.random_uniform([n_hidden, self.vocab_num], -0.1, 0.1), name='W_dec')\n",
    "        }\n",
    "        biases = {\n",
    "            'b_feat':  tf.Variable( tf.zeros([n_hidden]), name='b_feat'),\n",
    "            'b_dec': tf.Variable(tf.zeros([self.vocab_num]), name='b_dec')\n",
    "        }   \n",
    "        embeddings = {\n",
    "         'emb': tf.Variable(tf.random_uniform([self.vocab_num, n_hidden], -0.1, 0.1), name='emb')\n",
    "        }\n",
    "\n",
    "        batch_size = tf.shape(feat)[0]\n",
    "\n",
    "        if phase != phases['test']:\n",
    "            # cap_len: (250, 1) -> (250, 50)\n",
    "            cap_mask = tf.sequence_mask(cap_len, max_caption_len, dtype=tf.float32)\n",
    "     \n",
    "        if phase == phases['train']: #  add noise\n",
    "            noise = tf.random_uniform(tf.shape(feat), -0.1, 0.1, dtype=tf.float32)\n",
    "            feat = feat + noise\n",
    "\n",
    "        if phase == phases['train']:\n",
    "            feat = tf.nn.dropout(feat, dropout_prob)\n",
    "\n",
    "        feat = tf.reshape(feat, [-1, n_inputs])\n",
    "        image_emb = tf.matmul(feat, weights['W_feat']) + biases['b_feat']\n",
    "        image_emb = tf.reshape(image_emb, [-1, n_frames, n_hidden])\n",
    "        image_emb = tf.transpose(image_emb, perm=[1, 0, 2])\n",
    "        \n",
    "        with tf.variable_scope('LSTM1'):\n",
    "            lstm_red = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=forget_bias_red, state_is_tuple=True)\n",
    "            if phase == phases['train']:\n",
    "                lstm_red = tf.contrib.rnn.DropoutWrapper(lstm_red, output_keep_prob=dropout_prob)    \n",
    "        with tf.variable_scope('LSTM2'):\n",
    "            lstm_gre = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=forget_bias_gre, state_is_tuple=True)\n",
    "            if phase == phases['train']:\n",
    "                lstm_gre = tf.contrib.rnn.DropoutWrapper(lstm_gre, output_keep_prob=dropout_prob)    \n",
    "\n",
    "        state_red = lstm_red.zero_state(batch_size, dtype=tf.float32)\n",
    "        state_gre = lstm_gre.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "        padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "        h_src = []\n",
    "        for i in range(0, n_frames):\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output_red, state_red = lstm_red(image_emb[i,:,:], state_red)\n",
    "            \n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output_gre, state_gre = lstm_gre(tf.concat([padding, output_red], axis=1), state_gre)\n",
    "                h_src.append(output_gre) # even though padding is augmented, output_gre/state_gre's shape not change\n",
    "\n",
    "        h_src = tf.stack(h_src, axis = 0)\n",
    "\n",
    "        bos = tf.ones([batch_size, n_hidden])\n",
    "        padding_in = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "        logits = []\n",
    "        max_prob_index = None\n",
    "\n",
    "        \n",
    "\n",
    "        cross_ent_list = []\n",
    "        for i in range(0, max_caption_len):\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output_red, state_red = lstm_red(padding_in, state_red)\n",
    "\n",
    "            if i == 0:\n",
    "                with tf.variable_scope(\"LSTM2\"):\n",
    "                    con = tf.concat([bos, output_red], axis=1)\n",
    "                    output_gre, state_gre = lstm_gre(con, state_gre)\n",
    "            else:\n",
    "                if phase == phases['train']:\n",
    "                    if sampling[i] == True:\n",
    "                        feed_in = captions[:, i - 1]\n",
    "                    else:\n",
    "                        feed_in = tf.argmax(logit_words, 1)\n",
    "                else:\n",
    "                    feed_in = tf.argmax(logit_words, 1)\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    embed_result = tf.nn.embedding_lookup(embeddings['emb'], feed_in)\n",
    "                with tf.variable_scope(\"LSTM2\"):\n",
    "                    con = tf.concat([embed_result, output_red], axis=1)\n",
    "                    output_gre, state_gre = lstm_gre(con, state_gre)\n",
    "\n",
    "            logit_words = tf.matmul(output_gre, weights['W_dec']) + biases['b_dec']\n",
    "            logits.append(logit_words)\n",
    "\n",
    "            if phase != phases['test']:\n",
    "                labels = captions[:, i]\n",
    "                one_hot_labels = tf.one_hot(labels, self.vocab_num, on_value = 1, off_value = None, axis = 1) \n",
    "                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=one_hot_labels)\n",
    "                cross_entropy = cross_entropy * cap_mask[:, i]\n",
    "                cross_ent_list.append(cross_entropy)\n",
    "        \n",
    "        loss = 0.0\n",
    "        if phase != phases['test']:\n",
    "            cross_entropy_tensor = tf.stack(cross_ent_list, 1)\n",
    "            loss = tf.reduce_sum(cross_entropy_tensor, axis=1)\n",
    "            loss = tf.divide(loss, tf.cast(cap_len, tf.float32))\n",
    "            loss = tf.reduce_mean(loss, axis=0)\n",
    "\n",
    "        logits = tf.stack(logits, axis = 0)\n",
    "        logits = tf.reshape(logits, (max_caption_len, batch_size, self.vocab_num))\n",
    "        logits = tf.transpose(logits, [1, 0, 2])\n",
    "        \n",
    "        summary = None\n",
    "        if phase == phases['train']:\n",
    "            summary = tf.summary.scalar('training_loss', loss)\n",
    "        elif phase == phases['val']:\n",
    "            summary = tf.summary.scalar('validation_loss', loss)\n",
    "\n",
    "        return logits, loss, summary\n",
    "\n",
    "    def inference(self, logits):\n",
    "        \n",
    "        #print('using greedy search...')\n",
    "        dec_pred = tf.argmax(logits, 2)\n",
    "        return dec_pred\n",
    "\n",
    "    def optimize(self, loss_op):\n",
    "\n",
    "        params = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)#.minimize(loss_op)\n",
    "        gradients, variables = zip(*optimizer.compute_gradients(loss_op))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        train_op = optimizer.apply_gradients(zip(gradients, params))\n",
    "\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line,token='word'):\n",
    "    if token == 'word':\n",
    "        return [line.split(' ')]\n",
    "    elif token == 'char':\n",
    "        return [list(line)]\n",
    "    else:\n",
    "        print('ERROR: unknown token type '+token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokanized_sentences):\n",
    "    # Flatten a list of token lists into a list of tokens\n",
    "    tokens = [tk for line in tokanized_sentences for tk in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_vid_data_into_batches(filename,batch_size,feat_filepath):\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "        \n",
    "    batches = len(datastore)/batch_size\n",
    "    batches = int(batches)\n",
    "        \n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    vid_batch = {}\n",
    "    sentence_set = {}\n",
    "    \n",
    "    for data in datastore:\n",
    "        \n",
    "        #### Extracting all feature vectors per video\n",
    "\n",
    "        \n",
    "        #vid_feat_list = []\n",
    "        \n",
    "        video_id = data[\"id\"]\n",
    "        features = np.load(feat_filepath.format(video_id))\n",
    "\n",
    "        vid_framefeats = [] #list of all feature vectors per video. Shape = [80,4096]\n",
    "        \n",
    "        for array in features:\n",
    "            vid_framefeats.append(array)\n",
    "\n",
    "        if j not in vid_batch:\n",
    "            vid_batch[j] = []\n",
    "\n",
    "        vid_batch[j].append(vid_framefeats)\n",
    "        \n",
    "        \n",
    "        #### Extracting only a single sentence per video into a standalone dict\n",
    "\n",
    "        sentences = data[\"caption\"]\n",
    "        sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "        table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "        sentences = [word.translate(table) for word in sentences]\n",
    "\n",
    "        sentence_set[i] = sentences[0] #0 for only the first sentence\\\n",
    "        \n",
    "        i = i+1\n",
    "\n",
    "        if i%batch_size == 0:\n",
    "            j = j+1            \n",
    "            \n",
    "    return vid_batch, batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename, feat_filepath):\n",
    "    \n",
    "    sentence_set = {}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "        \n",
    "    i = 0\n",
    "    for data in datastore:\n",
    "        \n",
    "        #### Extracting only a single sentence per video into a standalone dict\n",
    "\n",
    "        sentences = data[\"caption\"]\n",
    "        sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "        table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "        sentences = [word.translate(table) for word in sentences]\n",
    "\n",
    "        sentence_set[i] = sentences[0] #0 for only the first sentence\\\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "    return sentence_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping string tokens to numertical indices.\n",
    "def listVocab(sentence_set):\n",
    "    \n",
    "    PAD_token = 0\n",
    "    BOS_token = 1\n",
    "    EOS_token = 2\n",
    "    UNK_token = 3\n",
    "    \n",
    "    all_tokens = []\n",
    "    word_count = {}\n",
    "    token2index = {\"<PAD>\": 0,\"<BOS>\":1,\"<EOS>\":2,\"<UNK>\":3}\n",
    "    index2token = {PAD_token: \"<PAD>\", BOS_token: \"<BOS>\", EOS_token: \"<EOS>\", UNK_token: \"<UNK>\"}\n",
    "    \n",
    "    #for set_i in vid_sentence_set:\n",
    "    #    sentence_set = vid_sentence_set[set_i]\n",
    "    #    for line in sentence_set: \n",
    "    \n",
    "    for n in sentence_set:\n",
    "        line = sentence_set[n]\n",
    "        tokenized_captions = tokenize(line) #Seperate the words\n",
    "        all_tokens += tokenized_captions\n",
    "    \n",
    "    counter = count_tokens(all_tokens) #Count the word repeatitions in each set\n",
    "    \n",
    "    counter_dict = counter.items()\n",
    "    counter_sort = sorted(counter_dict, key=lambda x:x[1],reverse=True) #sort by frequency of occurance \n",
    "    #print(counter_sort)\n",
    "\n",
    "    i = len(index2token)\n",
    "    values = [0,1,2,3]\n",
    "    tokens = [\"<PAD>\",\"<BOS>\",\"<EOS>\",\"<UNK>\"]\n",
    "    for token, freq in counter_sort:\n",
    "        word_count[token] = freq\n",
    "        index2token[i] = token\n",
    "        token2index[token] = i\n",
    "        values += [i]\n",
    "        tokens += [token]\n",
    "        i+=1\n",
    "        \n",
    "    word_count['<PAD>'] = i\n",
    "    word_count['<BOS>'] = i\n",
    "    word_count['<EOS>'] = i\n",
    "    word_count['<UNK>'] = i\n",
    "    \n",
    "    bias_init_vector = np.array([1.0 * word_count[ index2token[i] ] for i in index2token])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
    "    \n",
    "    return [word_count, tokens, values, token2index, index2token, len(index2token),bias_init_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenList(nestedList,output): \n",
    "    for i in nestedList: \n",
    "        if type(i) == list: \n",
    "            flattenList(i,output) \n",
    "        else: \n",
    "            output.append(i) \n",
    "            \n",
    "    return output\n",
    "\n",
    "def num_encode(test_sentence,index2token,tokens,tokenized_sentence=[],num_encoded_sentence=[]):\n",
    "    \n",
    "    tokenized_sentence.clear()\n",
    "    num_encoded_sentence.clear()\n",
    "    \n",
    "    tokenized_sentence = [\"<BOS>\"] + tokenize(test_sentence) + [\"<EOS>\"]\n",
    "    #print(tokenized_sentence)\n",
    "    output=[]\n",
    "    tokenized_sentence = flattenList(tokenized_sentence,output)\n",
    "    \n",
    "    cap_len = len(tokenized_sentence)\n",
    "    \n",
    "    while len(tokenized_sentence) < MAX_WORDS:\n",
    "        tokenized_sentence.append(\"<PAD>\")    \n",
    "    \n",
    "    #print(len(tokenized_sentence))\n",
    "    \n",
    "    for ind, token in enumerate(tokenized_sentence):\n",
    "        if token in tokens:\n",
    "            for i in range(0,len(index2token)):\n",
    "                if token == index2token[i]: \n",
    "                    num_encoded_sentence.append(i) \n",
    "                    \n",
    "            #print(\"token exists\")\n",
    "        else:\n",
    "            num_encoded_sentence.append(3)\n",
    "            tokenized_sentence[ind] = tokens[3]\n",
    "            #print(\"token unknown\")\n",
    "            \n",
    "            \n",
    "                \n",
    "    #print(len(num_encoded_sentence))\n",
    "\n",
    "        \n",
    "    return tokenized_sentence, num_encoded_sentence, cap_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_sentence_data_into_batches(sentence_set, index2token,tokens,batch_size):\n",
    "\n",
    "    tokenizedsentence_batch = {}\n",
    "    intencode_batch = {}\n",
    "    cap_len_batch = {}\n",
    "\n",
    "    ii = 0\n",
    "    jj = 0  \n",
    "\n",
    "    for n in sentence_set:\n",
    "        sentence = sentence_set[n]\n",
    "\n",
    "        tokenized_sentence,encoded_sentence, cap_len = num_encode(sentence,index2token,tokens)\n",
    "        \n",
    "        #print(np.shape(encoded_sentence))\n",
    "\n",
    "        tokenized_sentence = list(tokenized_sentence)\n",
    "        encoded_sentence = list(encoded_sentence)\n",
    "\n",
    "        if jj not in intencode_batch:\n",
    "            #onehot_batch[jj] = []\n",
    "            intencode_batch[jj] = []\n",
    "            tokenizedsentence_batch[jj] = []\n",
    "            cap_len_batch[jj] = []\n",
    "\n",
    "        #print(np.shape(onehot_encoded_sentence))    \n",
    "        #onehot_batch[jj].append(onehot_encoded_sentence)\n",
    "        intencode_batch[jj].append(encoded_sentence)\n",
    "        tokenizedsentence_batch[jj].append(tokenized_sentence)\n",
    "        cap_len_batch[jj].append(cap_len)\n",
    "\n",
    "        ii = ii+1\n",
    "\n",
    "        if ii%batch_size == 0:\n",
    "            jj = jj+1\n",
    "            \n",
    "        \n",
    "    return tokenizedsentence_batch, intencode_batch, cap_len_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of videos in the training set are 1450 and each video has 80 frames with 4096 features/units each\n",
      "There are 1988 unique words in the captions dataset\n"
     ]
    }
   ],
   "source": [
    "filename_train = 'MLDS_hw2_1_data/training_label.json'\n",
    "filename_test = 'MLDS_hw2_1_data/testing_label.json'\n",
    "feat_filepath_train = \"MLDS_hw2_1_data/training_data/feat/{}.npy\"\n",
    "feat_filepath_test = \"MLDS_hw2_1_data/testing_data/feat/{}.npy\"\n",
    "\n",
    "ckpt_path = 'saved_model/trained_model.ckpt'\n",
    "\n",
    "# forget_bias_red = 1.0\n",
    "# forget_bias_gre = 1.0\n",
    "# dropout_prob    = 0.5\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "MAX_WORDS = max_caption_len #max number of words in a caption\n",
    "n_features = n_inputs\n",
    "no_of_frames = n_frames\n",
    "sizeof_sentence= MAX_WORDS\n",
    "learning_rate = 0.0001\n",
    "n_hidden = n_hidden\n",
    "\n",
    "#### PARSE TRAINING DATA #####\n",
    "\n",
    "#Parse Training Data into batches\n",
    "vid_batch, n_batches = parse_vid_data_into_batches(filename_train,batch_size,feat_filepath_train)\n",
    "print(\"The number of videos in the training set are %d and each video has 80 frames with 4096 features/units each\" % (n_batches*batch_size))\n",
    "\n",
    "# Extracting captions for each video\n",
    "sentence_set = extract_sentences(filename_train,feat_filepath_train)\n",
    "\n",
    "word_count, tokens, values, token2index, index2token, n_words,bias_init_vector = listVocab(sentence_set)\n",
    "print(\"There are %d unique words in the captions dataset\" % n_words)\n",
    "\n",
    "tokenizedsentence_batch, intencode_batch, cap_len_batch = parse_sentence_data_into_batches(sentence_set,index2token,tokens,batch_size)\n",
    "\n",
    "# # integer encode\n",
    "# label_encoder = LabelEncoder()\n",
    "# integer_encoded = label_encoder.fit_transform(values)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "# integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of videos in the test set are 100 and each video has 80 frames with 4096 features/units each\n"
     ]
    }
   ],
   "source": [
    "#### PARSE TESTING DATA #####\n",
    "\n",
    "#Parse Testing Data into batches\n",
    "vid_batch_test, n_batches_test = parse_vid_data_into_batches(filename_test,batch_size,feat_filepath_test)\n",
    "print(\"The number of videos in the test set are %d and each video has 80 frames with 4096 features/units each\" % (n_batches_test*batch_size))\n",
    "\n",
    "# Extracting captions for each video\n",
    "sentence_set_test = extract_sentences(filename_test,feat_filepath_test)\n",
    "tokenizedsentence_batch_test, intencode_batch_test, cap_len_batch_test = parse_sentence_data_into_batches(sentence_set_test,index2token,tokens,batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_sampling(sampling_prob, cap_len_batch):\n",
    "\n",
    "        sampling = np.ones(max_caption_len, dtype = bool)\n",
    "        for l in range(max_caption_len):\n",
    "            if np.random.uniform(0,1,1) < sampling_prob:\n",
    "                sampling[l] = True\n",
    "            else:\n",
    "                sampling[l] = False\n",
    "         \n",
    "        sampling[0] = True\n",
    "        return sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sigmoid(num_epo):\n",
    "\n",
    "    # 0.88 to 0.12 (-2.0 to 2.0)\n",
    "    x = np.arange(-2.0, 2.0, (4.0/num_epo))\n",
    "    y = 1/(1 + np.e**x)\n",
    "    #y = np.ones(num_epo)\n",
    "    print(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_print_train(pred, cap_len, label, idx2word, batch_size, values):\n",
    "    \n",
    "    i = np.random.randint(0, batch_size)\n",
    "    eos_pred = max_caption_len - 1\n",
    "    eos = cap_len[i] - 1\n",
    "    for j in range(0, max_caption_len):\n",
    "            if pred[i][j] == special_tokens['<EOS>']:\n",
    "                eos_pred = j\n",
    "                break\n",
    "    \n",
    "    pre = list( map (lambda x: idx2word[x] , pred[i][0:eos_pred])  )\n",
    "    lab = list( map (lambda x: idx2word[x] , label[i][0:eos])  )\n",
    "    print('\\nid: ' + str(values[i]) + '\\nanswer: ' + str(lab) + '\\nprediction: ' + str(pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2b15d84c684f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamp_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msamp_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# samp_prob = inv_sigmoid(num_epochs)\n",
    "# samp_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp = schedule_sampling(samp_prob[epo], caption_lens_batch)\n",
    "# samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_graph: start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88079708 0.83201839 0.76852478 0.68997448 0.59868766 0.5\n",
      " 0.40131234 0.31002552 0.23147522 0.16798161]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0, step 0/29, (Training Loss: 7.6513, samp_prob: 0.8808):   0%|          | 0/10 [00:40<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 1/29, (Training Loss: 7.5223, samp_prob: 0.8808):   0%|          | 0/10 [01:20<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 0, step 1/29......\n",
      "\n",
      "id: 40\n",
      "answer: ['<BOS>', 'a', 'baby', 'is', 'repeatedly', 'kissing', 'his', 'reflection', 'in', 'a', 'mirror']\n",
      "prediction: ['patted', 'saxaphone', 'petted', 'saxaphone', 'patted', 'microwave', 'microwave', 'chin', 'tatoos', 'quickly', 'show', 'pinkish', 'pealing', 'story', 'its', 'adds', 'hip', 'squeezes', 'dish', 'lotion', 'instruments', 'his', 'bolt', 'steps', 'singing', 'lady', 'animal', 'hair', 'drills', 'nicholson', 'backpack', 'tricycle', 'biking', 'pulp', 'sprinkles', 'bra', 'fenced', 'kick', 'close', 'yard', 'each', 'add', 'skull', 'target', 'noose', 'add', 'puddle', 'putting', 'processed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0, step 2/29, (Training Loss: 7.3753, samp_prob: 0.8808):   0%|          | 0/10 [01:22<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 3/29, (Training Loss: 7.2473, samp_prob: 0.8808):   0%|          | 0/10 [01:24<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 4/29, (Training Loss: 7.1124, samp_prob: 0.8808):   0%|          | 0/10 [01:26<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 5/29, (Training Loss: 6.9959, samp_prob: 0.8808):   0%|          | 0/10 [01:29<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 6/29, (Training Loss: 6.8193, samp_prob: 0.8808):   0%|          | 0/10 [01:31<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 7/29, (Training Loss: 6.6973, samp_prob: 0.8808):   0%|          | 0/10 [01:33<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 8/29, (Training Loss: 6.5132, samp_prob: 0.8808):   0%|          | 0/10 [01:35<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 9/29, (Training Loss: 6.2832, samp_prob: 0.8808):   0%|          | 0/10 [01:37<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 10/29, (Training Loss: 6.0948, samp_prob: 0.8808):   0%|          | 0/10 [01:39<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 11/29, (Training Loss: 5.9239, samp_prob: 0.8808):   0%|          | 0/10 [01:41<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 12/29, (Training Loss: 5.7439, samp_prob: 0.8808):   0%|          | 0/10 [01:44<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 13/29, (Training Loss: 5.6357, samp_prob: 0.8808):   0%|          | 0/10 [01:46<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 14/29, (Training Loss: 5.6476, samp_prob: 0.8808):   0%|          | 0/10 [01:48<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 15/29, (Training Loss: 5.5503, samp_prob: 0.8808):   0%|          | 0/10 [01:50<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 16/29, (Training Loss: 5.6616, samp_prob: 0.8808):   0%|          | 0/10 [01:52<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 0, step 16/29......\n",
      "\n",
      "id: 47\n",
      "answer: ['<BOS>', 'two', 'men', 'are', 'walking', 'down', 'a', 'street', 'holding', 'their', 'jackets', 'and', 'talking']\n",
      "prediction: ['<BOS>', 'a', 'a', 'a', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0, step 17/29, (Training Loss: 5.5791, samp_prob: 0.8808):   0%|          | 0/10 [01:55<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 18/29, (Training Loss: 5.5023, samp_prob: 0.8808):   0%|          | 0/10 [01:57<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 19/29, (Training Loss: 5.3681, samp_prob: 0.8808):   0%|          | 0/10 [01:59<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 20/29, (Training Loss: 5.6569, samp_prob: 0.8808):   0%|          | 0/10 [02:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 21/29, (Training Loss: 5.6055, samp_prob: 0.8808):   0%|          | 0/10 [02:03<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 22/29, (Training Loss: 5.4339, samp_prob: 0.8808):   0%|          | 0/10 [02:05<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 23/29, (Training Loss: 5.4650, samp_prob: 0.8808):   0%|          | 0/10 [02:07<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 24/29, (Training Loss: 5.0805, samp_prob: 0.8808):   0%|          | 0/10 [02:09<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 25/29, (Training Loss: 5.2738, samp_prob: 0.8808):   0%|          | 0/10 [02:12<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 26/29, (Training Loss: 5.0738, samp_prob: 0.8808):   0%|          | 0/10 [02:14<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 27/29, (Training Loss: 5.1970, samp_prob: 0.8808):   0%|          | 0/10 [02:16<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 28/29, (Training Loss: 5.2236, samp_prob: 0.8808):   0%|          | 0/10 [02:18<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 0, step 28/29, (Training Loss: 5.2236, samp_prob: 0.8808):  10%|█         | 1/10 [02:18<20:46, 138.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISHED] Epoch 0, (Training Loss (per epoch): 174.9346 samp_prob: 0.8808)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1, step 0/29, (Training Loss: 5.1056, samp_prob: 0.8320):  10%|█         | 1/10 [02:20<20:46, 138.55s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 1/29, (Training Loss: 4.9625, samp_prob: 0.8320):  10%|█         | 1/10 [02:22<20:46, 138.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 1, step 1/29......\n",
      "\n",
      "id: 28\n",
      "answer: ['<BOS>', 'a', 'boy', 'takes', 'a', 'drink', 'from', 'a', 'plastic', 'cup', 'makes', 'a', 'face', 'and', 'tosses', 'the', 'liquid', 'toward', 'some', 'plants', 'hitting', 'a', 'camera', 'sitting', 'on', 'the', 'ledge']\n",
      "prediction: ['<BOS>', 'a', 'a', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1, step 2/29, (Training Loss: 4.9682, samp_prob: 0.8320):  10%|█         | 1/10 [02:25<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 3/29, (Training Loss: 4.9703, samp_prob: 0.8320):  10%|█         | 1/10 [02:27<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 4/29, (Training Loss: 4.9985, samp_prob: 0.8320):  10%|█         | 1/10 [02:29<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 5/29, (Training Loss: 4.8432, samp_prob: 0.8320):  10%|█         | 1/10 [02:31<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 6/29, (Training Loss: 5.0794, samp_prob: 0.8320):  10%|█         | 1/10 [02:33<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 7/29, (Training Loss: 4.9403, samp_prob: 0.8320):  10%|█         | 1/10 [02:35<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 8/29, (Training Loss: 5.0638, samp_prob: 0.8320):  10%|█         | 1/10 [02:37<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 9/29, (Training Loss: 4.9186, samp_prob: 0.8320):  10%|█         | 1/10 [02:40<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 10/29, (Training Loss: 4.7393, samp_prob: 0.8320):  10%|█         | 1/10 [02:42<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 11/29, (Training Loss: 4.8698, samp_prob: 0.8320):  10%|█         | 1/10 [02:44<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 12/29, (Training Loss: 4.8059, samp_prob: 0.8320):  10%|█         | 1/10 [02:46<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 13/29, (Training Loss: 4.8430, samp_prob: 0.8320):  10%|█         | 1/10 [02:48<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 14/29, (Training Loss: 4.9117, samp_prob: 0.8320):  10%|█         | 1/10 [02:50<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 15/29, (Training Loss: 4.8139, samp_prob: 0.8320):  10%|█         | 1/10 [02:52<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 16/29, (Training Loss: 5.0824, samp_prob: 0.8320):  10%|█         | 1/10 [02:55<20:46, 138.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 1, step 16/29......\n",
      "\n",
      "id: 30\n",
      "answer: ['<BOS>', 'a', 'kid', 'gets', 'knocked', 'down', 'by', 'an', 'animal']\n",
      "prediction: ['<BOS>', 'a', 'a', 'man', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1, step 17/29, (Training Loss: 4.8104, samp_prob: 0.8320):  10%|█         | 1/10 [02:57<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 18/29, (Training Loss: 4.7955, samp_prob: 0.8320):  10%|█         | 1/10 [02:59<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 19/29, (Training Loss: 4.7082, samp_prob: 0.8320):  10%|█         | 1/10 [03:01<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 20/29, (Training Loss: 5.0966, samp_prob: 0.8320):  10%|█         | 1/10 [03:03<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 21/29, (Training Loss: 5.0640, samp_prob: 0.8320):  10%|█         | 1/10 [03:05<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 22/29, (Training Loss: 4.8599, samp_prob: 0.8320):  10%|█         | 1/10 [03:07<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 23/29, (Training Loss: 4.9470, samp_prob: 0.8320):  10%|█         | 1/10 [03:09<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 24/29, (Training Loss: 4.6353, samp_prob: 0.8320):  10%|█         | 1/10 [03:12<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 25/29, (Training Loss: 4.8287, samp_prob: 0.8320):  10%|█         | 1/10 [03:14<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 26/29, (Training Loss: 4.6357, samp_prob: 0.8320):  10%|█         | 1/10 [03:16<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 27/29, (Training Loss: 4.7424, samp_prob: 0.8320):  10%|█         | 1/10 [03:18<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 28/29, (Training Loss: 4.9162, samp_prob: 0.8320):  10%|█         | 1/10 [03:20<20:46, 138.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 1, step 28/29, (Training Loss: 4.9162, samp_prob: 0.8320):  20%|██        | 2/10 [03:20<15:24, 115.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISHED] Epoch 1, (Training Loss (per epoch): 141.9564 samp_prob: 0.8320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 2, step 0/29, (Training Loss: 4.7063, samp_prob: 0.7685):  20%|██        | 2/10 [03:22<15:24, 115.60s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 1/29, (Training Loss: 4.6019, samp_prob: 0.7685):  20%|██        | 2/10 [03:25<15:24, 115.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 2, step 1/29......\n",
      "\n",
      "id: 28\n",
      "answer: ['<BOS>', 'a', 'boy', 'takes', 'a', 'drink', 'from', 'a', 'plastic', 'cup', 'makes', 'a', 'face', 'and', 'tosses', 'the', 'liquid', 'toward', 'some', 'plants', 'hitting', 'a', 'camera', 'sitting', 'on', 'the', 'ledge']\n",
      "prediction: ['<BOS>', 'a', 'a', 'a', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 2, step 2/29, (Training Loss: 4.6039, samp_prob: 0.7685):  20%|██        | 2/10 [03:27<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 3/29, (Training Loss: 4.6417, samp_prob: 0.7685):  20%|██        | 2/10 [03:29<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 4/29, (Training Loss: 4.7206, samp_prob: 0.7685):  20%|██        | 2/10 [03:31<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 5/29, (Training Loss: 4.4727, samp_prob: 0.7685):  20%|██        | 2/10 [03:33<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 6/29, (Training Loss: 4.6912, samp_prob: 0.7685):  20%|██        | 2/10 [03:35<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 7/29, (Training Loss: 4.6470, samp_prob: 0.7685):  20%|██        | 2/10 [03:37<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 8/29, (Training Loss: 4.7772, samp_prob: 0.7685):  20%|██        | 2/10 [03:39<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 9/29, (Training Loss: 4.6451, samp_prob: 0.7685):  20%|██        | 2/10 [03:42<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 10/29, (Training Loss: 4.4056, samp_prob: 0.7685):  20%|██        | 2/10 [03:44<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 11/29, (Training Loss: 4.6230, samp_prob: 0.7685):  20%|██        | 2/10 [03:46<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 12/29, (Training Loss: 4.5422, samp_prob: 0.7685):  20%|██        | 2/10 [03:48<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 13/29, (Training Loss: 4.5916, samp_prob: 0.7685):  20%|██        | 2/10 [03:50<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 14/29, (Training Loss: 4.7745, samp_prob: 0.7685):  20%|██        | 2/10 [03:52<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 15/29, (Training Loss: 4.5850, samp_prob: 0.7685):  20%|██        | 2/10 [03:54<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 16/29, (Training Loss: 4.8615, samp_prob: 0.7685):  20%|██        | 2/10 [03:56<15:24, 115.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 2, step 16/29......\n",
      "\n",
      "id: 37\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'riding', 'a', 'motorcycle', 'with', 'a', 'woman', 'riding', 'behind', 'him', 'as', 'a', 'passenger']\n",
      "prediction: ['<BOS>', 'a', 'man', 'man', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 2, step 17/29, (Training Loss: 4.5613, samp_prob: 0.7685):  20%|██        | 2/10 [03:59<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 18/29, (Training Loss: 4.5519, samp_prob: 0.7685):  20%|██        | 2/10 [04:01<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 19/29, (Training Loss: 4.5450, samp_prob: 0.7685):  20%|██        | 2/10 [04:03<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 20/29, (Training Loss: 4.8421, samp_prob: 0.7685):  20%|██        | 2/10 [04:05<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 21/29, (Training Loss: 4.8389, samp_prob: 0.7685):  20%|██        | 2/10 [04:07<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 22/29, (Training Loss: 4.6863, samp_prob: 0.7685):  20%|██        | 2/10 [04:09<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 23/29, (Training Loss: 4.7569, samp_prob: 0.7685):  20%|██        | 2/10 [04:11<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 24/29, (Training Loss: 4.5265, samp_prob: 0.7685):  20%|██        | 2/10 [04:13<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 25/29, (Training Loss: 4.6325, samp_prob: 0.7685):  20%|██        | 2/10 [04:16<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 26/29, (Training Loss: 4.4420, samp_prob: 0.7685):  20%|██        | 2/10 [04:18<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 27/29, (Training Loss: 4.6046, samp_prob: 0.7685):  20%|██        | 2/10 [04:20<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 28/29, (Training Loss: 4.7157, samp_prob: 0.7685):  20%|██        | 2/10 [04:22<15:24, 115.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 2, step 28/29, (Training Loss: 4.7157, samp_prob: 0.7685):  30%|███       | 3/10 [04:22<11:36, 99.45s/it] \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISHED] Epoch 2, (Training Loss (per epoch): 134.5950 samp_prob: 0.7685)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 3, step 0/29, (Training Loss: 4.4651, samp_prob: 0.6900):  30%|███       | 3/10 [04:24<11:36, 99.45s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 1/29, (Training Loss: 4.3919, samp_prob: 0.6900):  30%|███       | 3/10 [04:26<11:36, 99.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 3, step 1/29......\n",
      "\n",
      "id: 41\n",
      "answer: ['<BOS>', 'a', 'cartoon', 'is', 'swinging']\n",
      "prediction: ['<BOS>', 'a', 'is', 'is', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 3, step 2/29, (Training Loss: 4.4435, samp_prob: 0.6900):  30%|███       | 3/10 [04:28<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 3/29, (Training Loss: 4.4851, samp_prob: 0.6900):  30%|███       | 3/10 [04:31<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 4/29, (Training Loss: 4.5470, samp_prob: 0.6900):  30%|███       | 3/10 [04:33<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 5/29, (Training Loss: 4.4076, samp_prob: 0.6900):  30%|███       | 3/10 [04:35<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 6/29, (Training Loss: 4.6167, samp_prob: 0.6900):  30%|███       | 3/10 [04:37<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 7/29, (Training Loss: 4.4688, samp_prob: 0.6900):  30%|███       | 3/10 [04:39<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 8/29, (Training Loss: 4.7145, samp_prob: 0.6900):  30%|███       | 3/10 [04:41<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 9/29, (Training Loss: 4.5214, samp_prob: 0.6900):  30%|███       | 3/10 [04:43<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 10/29, (Training Loss: 4.2878, samp_prob: 0.6900):  30%|███       | 3/10 [04:45<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 11/29, (Training Loss: 4.4866, samp_prob: 0.6900):  30%|███       | 3/10 [04:48<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 12/29, (Training Loss: 4.4297, samp_prob: 0.6900):  30%|███       | 3/10 [04:50<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 13/29, (Training Loss: 4.4453, samp_prob: 0.6900):  30%|███       | 3/10 [04:52<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 14/29, (Training Loss: 4.6109, samp_prob: 0.6900):  30%|███       | 3/10 [04:54<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 15/29, (Training Loss: 4.4466, samp_prob: 0.6900):  30%|███       | 3/10 [04:56<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 16/29, (Training Loss: 4.7598, samp_prob: 0.6900):  30%|███       | 3/10 [04:58<11:36, 99.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 3, step 16/29......\n",
      "\n",
      "id: 1\n",
      "answer: ['<BOS>', 'a', 'chef', 'pealing', 'a', 'onion', 'for', 'a', 'dish']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'a', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 3, step 17/29, (Training Loss: 4.4513, samp_prob: 0.6900):  30%|███       | 3/10 [05:00<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 18/29, (Training Loss: 4.4800, samp_prob: 0.6900):  30%|███       | 3/10 [05:03<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 19/29, (Training Loss: 4.4768, samp_prob: 0.6900):  30%|███       | 3/10 [05:05<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 20/29, (Training Loss: 4.7669, samp_prob: 0.6900):  30%|███       | 3/10 [05:07<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 21/29, (Training Loss: 4.7495, samp_prob: 0.6900):  30%|███       | 3/10 [05:09<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 22/29, (Training Loss: 4.5687, samp_prob: 0.6900):  30%|███       | 3/10 [05:11<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 23/29, (Training Loss: 4.6284, samp_prob: 0.6900):  30%|███       | 3/10 [05:13<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 24/29, (Training Loss: 4.4396, samp_prob: 0.6900):  30%|███       | 3/10 [05:15<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 25/29, (Training Loss: 4.5445, samp_prob: 0.6900):  30%|███       | 3/10 [05:17<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 26/29, (Training Loss: 4.2937, samp_prob: 0.6900):  30%|███       | 3/10 [05:19<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 27/29, (Training Loss: 4.4640, samp_prob: 0.6900):  30%|███       | 3/10 [05:22<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 28/29, (Training Loss: 4.6328, samp_prob: 0.6900):  30%|███       | 3/10 [05:24<11:36, 99.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 3, step 28/29, (Training Loss: 4.6328, samp_prob: 0.6900):  40%|████      | 4/10 [05:24<08:48, 88.16s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISHED] Epoch 3, (Training Loss (per epoch): 131.0247 samp_prob: 0.6900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 4, step 0/29, (Training Loss: 4.3718, samp_prob: 0.5987):  40%|████      | 4/10 [05:26<08:48, 88.16s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 1/29, (Training Loss: 4.3259, samp_prob: 0.5987):  40%|████      | 4/10 [05:28<08:48, 88.16s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 4, step 1/29......\n",
      "\n",
      "id: 4\n",
      "answer: ['<BOS>', 'a', 'man', 'playing', 'drums']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 4, step 2/29, (Training Loss: 4.3677, samp_prob: 0.5987):  40%|████      | 4/10 [05:30<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 3/29, (Training Loss: 4.3510, samp_prob: 0.5987):  40%|████      | 4/10 [05:32<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 4/29, (Training Loss: 4.4381, samp_prob: 0.5987):  40%|████      | 4/10 [05:35<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 5/29, (Training Loss: 4.2893, samp_prob: 0.5987):  40%|████      | 4/10 [05:37<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 6/29, (Training Loss: 4.4677, samp_prob: 0.5987):  40%|████      | 4/10 [05:39<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 7/29, (Training Loss: 4.3891, samp_prob: 0.5987):  40%|████      | 4/10 [05:41<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 8/29, (Training Loss: 4.6505, samp_prob: 0.5987):  40%|████      | 4/10 [05:43<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 9/29, (Training Loss: 4.4511, samp_prob: 0.5987):  40%|████      | 4/10 [05:45<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 10/29, (Training Loss: 4.2584, samp_prob: 0.5987):  40%|████      | 4/10 [05:47<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 11/29, (Training Loss: 4.3970, samp_prob: 0.5987):  40%|████      | 4/10 [05:50<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 12/29, (Training Loss: 4.3786, samp_prob: 0.5987):  40%|████      | 4/10 [05:52<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 13/29, (Training Loss: 4.3832, samp_prob: 0.5987):  40%|████      | 4/10 [05:54<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 14/29, (Training Loss: 4.5445, samp_prob: 0.5987):  40%|████      | 4/10 [05:56<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 15/29, (Training Loss: 4.3710, samp_prob: 0.5987):  40%|████      | 4/10 [05:58<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 16/29, (Training Loss: 4.7265, samp_prob: 0.5987):  40%|████      | 4/10 [06:00<08:48, 88.16s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 4, step 16/29......\n",
      "\n",
      "id: 15\n",
      "answer: ['<BOS>', 'a', 'car', 'is', 'driving']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 4, step 17/29, (Training Loss: 4.4379, samp_prob: 0.5987):  40%|████      | 4/10 [06:02<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 18/29, (Training Loss: 4.3486, samp_prob: 0.5987):  40%|████      | 4/10 [06:05<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 19/29, (Training Loss: 4.3936, samp_prob: 0.5987):  40%|████      | 4/10 [06:07<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 20/29, (Training Loss: 4.7151, samp_prob: 0.5987):  40%|████      | 4/10 [06:09<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 21/29, (Training Loss: 4.7047, samp_prob: 0.5987):  40%|████      | 4/10 [06:11<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 22/29, (Training Loss: 4.5438, samp_prob: 0.5987):  40%|████      | 4/10 [06:13<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 23/29, (Training Loss: 4.6031, samp_prob: 0.5987):  40%|████      | 4/10 [06:15<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 24/29, (Training Loss: 4.3387, samp_prob: 0.5987):  40%|████      | 4/10 [06:17<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 25/29, (Training Loss: 4.4631, samp_prob: 0.5987):  40%|████      | 4/10 [06:20<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 26/29, (Training Loss: 4.2634, samp_prob: 0.5987):  40%|████      | 4/10 [06:22<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 27/29, (Training Loss: 4.4759, samp_prob: 0.5987):  40%|████      | 4/10 [06:24<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 28/29, (Training Loss: 4.5470, samp_prob: 0.5987):  40%|████      | 4/10 [06:26<08:48, 88.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 4, step 28/29, (Training Loss: 4.5470, samp_prob: 0.5987):  50%|█████     | 5/10 [06:26<06:41, 80.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISHED] Epoch 4, (Training Loss (per epoch): 128.9962 samp_prob: 0.5987)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 5, step 0/29, (Training Loss: 4.3278, samp_prob: 0.5000):  50%|█████     | 5/10 [06:28<06:41, 80.35s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 1/29, (Training Loss: 4.2125, samp_prob: 0.5000):  50%|█████     | 5/10 [06:30<06:41, 80.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 5, step 1/29......\n",
      "\n",
      "id: 41\n",
      "answer: ['<BOS>', 'a', 'cartoon', 'is', 'swinging']\n",
      "prediction: ['<BOS>', 'a', 'is', 'is', 'is']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 5, step 2/29, (Training Loss: 4.2432, samp_prob: 0.5000):  50%|█████     | 5/10 [06:32<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 3/29, (Training Loss: 4.3247, samp_prob: 0.5000):  50%|█████     | 5/10 [06:34<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 4/29, (Training Loss: 4.3481, samp_prob: 0.5000):  50%|█████     | 5/10 [06:37<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 5/29, (Training Loss: 4.1663, samp_prob: 0.5000):  50%|█████     | 5/10 [06:39<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 6/29, (Training Loss: 4.3850, samp_prob: 0.5000):  50%|█████     | 5/10 [06:41<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 7/29, (Training Loss: 4.2901, samp_prob: 0.5000):  50%|█████     | 5/10 [06:43<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 8/29, (Training Loss: 4.5620, samp_prob: 0.5000):  50%|█████     | 5/10 [06:45<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 9/29, (Training Loss: 4.3278, samp_prob: 0.5000):  50%|█████     | 5/10 [06:47<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 10/29, (Training Loss: 4.1292, samp_prob: 0.5000):  50%|█████     | 5/10 [06:49<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 11/29, (Training Loss: 4.3402, samp_prob: 0.5000):  50%|█████     | 5/10 [06:52<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 12/29, (Training Loss: 4.2773, samp_prob: 0.5000):  50%|█████     | 5/10 [06:54<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 13/29, (Training Loss: 4.3106, samp_prob: 0.5000):  50%|█████     | 5/10 [06:56<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 14/29, (Training Loss: 4.4827, samp_prob: 0.5000):  50%|█████     | 5/10 [06:58<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 15/29, (Training Loss: 4.3007, samp_prob: 0.5000):  50%|█████     | 5/10 [07:00<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 16/29, (Training Loss: 4.5994, samp_prob: 0.5000):  50%|█████     | 5/10 [07:02<06:41, 80.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 5, step 16/29......\n",
      "\n",
      "id: 38\n",
      "answer: ['<BOS>', 'a', 'person', 'cutting', 'up', 'vegatables']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 5, step 17/29, (Training Loss: 4.3304, samp_prob: 0.5000):  50%|█████     | 5/10 [07:05<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 18/29, (Training Loss: 4.2453, samp_prob: 0.5000):  50%|█████     | 5/10 [07:07<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 19/29, (Training Loss: 4.2703, samp_prob: 0.5000):  50%|█████     | 5/10 [07:09<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 20/29, (Training Loss: 4.5995, samp_prob: 0.5000):  50%|█████     | 5/10 [07:11<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 21/29, (Training Loss: 4.5685, samp_prob: 0.5000):  50%|█████     | 5/10 [07:13<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 22/29, (Training Loss: 4.4919, samp_prob: 0.5000):  50%|█████     | 5/10 [07:15<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 23/29, (Training Loss: 4.4734, samp_prob: 0.5000):  50%|█████     | 5/10 [07:17<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 24/29, (Training Loss: 4.2423, samp_prob: 0.5000):  50%|█████     | 5/10 [07:19<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 25/29, (Training Loss: 4.4181, samp_prob: 0.5000):  50%|█████     | 5/10 [07:22<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 26/29, (Training Loss: 4.1720, samp_prob: 0.5000):  50%|█████     | 5/10 [07:24<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 27/29, (Training Loss: 4.3778, samp_prob: 0.5000):  50%|█████     | 5/10 [07:26<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 28/29, (Training Loss: 4.4748, samp_prob: 0.5000):  50%|█████     | 5/10 [07:28<06:41, 80.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 5, step 28/29, (Training Loss: 4.4748, samp_prob: 0.5000):  60%|██████    | 6/10 [07:28<04:59, 74.90s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISHED] Epoch 5, (Training Loss (per epoch): 126.2917 samp_prob: 0.5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 6, step 0/29, (Training Loss: 4.2749, samp_prob: 0.4013):  60%|██████    | 6/10 [07:30<04:59, 74.90s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 1/29, (Training Loss: 4.1802, samp_prob: 0.4013):  60%|██████    | 6/10 [07:32<04:59, 74.90s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 6, step 1/29......\n",
      "\n",
      "id: 21\n",
      "answer: ['<BOS>', 'a', 'man', 'holding', 'an', 'umbrella', 'has', 'jumped', 'a', 'hurdle', 'and', 'a', 'wall']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 6, step 2/29, (Training Loss: 4.1878, samp_prob: 0.4013):  60%|██████    | 6/10 [07:35<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 3/29, (Training Loss: 4.2511, samp_prob: 0.4013):  60%|██████    | 6/10 [07:37<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 4/29, (Training Loss: 4.2856, samp_prob: 0.4013):  60%|██████    | 6/10 [07:39<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 5/29, (Training Loss: 4.1226, samp_prob: 0.4013):  60%|██████    | 6/10 [07:41<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 6/29, (Training Loss: 4.3789, samp_prob: 0.4013):  60%|██████    | 6/10 [07:43<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 7/29, (Training Loss: 4.2560, samp_prob: 0.4013):  60%|██████    | 6/10 [07:45<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 8/29, (Training Loss: 4.4255, samp_prob: 0.4013):  60%|██████    | 6/10 [07:47<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 9/29, (Training Loss: 4.2851, samp_prob: 0.4013):  60%|██████    | 6/10 [07:49<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 10/29, (Training Loss: 4.0205, samp_prob: 0.4013):  60%|██████    | 6/10 [07:52<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 11/29, (Training Loss: 4.2445, samp_prob: 0.4013):  60%|██████    | 6/10 [07:54<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 12/29, (Training Loss: 4.1961, samp_prob: 0.4013):  60%|██████    | 6/10 [07:56<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 13/29, (Training Loss: 4.2003, samp_prob: 0.4013):  60%|██████    | 6/10 [07:58<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 14/29, (Training Loss: 4.3897, samp_prob: 0.4013):  60%|██████    | 6/10 [08:00<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 15/29, (Training Loss: 4.2366, samp_prob: 0.4013):  60%|██████    | 6/10 [08:02<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 16/29, (Training Loss: 4.6094, samp_prob: 0.4013):  60%|██████    | 6/10 [08:05<04:59, 74.90s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 6, step 16/29......\n",
      "\n",
      "id: 43\n",
      "answer: ['<BOS>', 'a', 'boy', 'kneeling', 'in', 'front', 'of', 'a', 'bench', 'is', 'moving', 'his', 'arms', 'and', 'body', 'rhythmically', 'and', 'then', 'he', 'begins', 'to', 'dance', 'on', 'and', 'around', 'the', 'bench']\n",
      "prediction: ['<BOS>', 'a', 'is', 'is', 'is', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 6, step 17/29, (Training Loss: 4.2646, samp_prob: 0.4013):  60%|██████    | 6/10 [08:07<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 18/29, (Training Loss: 4.2106, samp_prob: 0.4013):  60%|██████    | 6/10 [08:09<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 19/29, (Training Loss: 4.2364, samp_prob: 0.4013):  60%|██████    | 6/10 [08:11<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 20/29, (Training Loss: 4.6084, samp_prob: 0.4013):  60%|██████    | 6/10 [08:13<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 21/29, (Training Loss: 4.5034, samp_prob: 0.4013):  60%|██████    | 6/10 [08:15<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 22/29, (Training Loss: 4.4040, samp_prob: 0.4013):  60%|██████    | 6/10 [08:17<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 23/29, (Training Loss: 4.4571, samp_prob: 0.4013):  60%|██████    | 6/10 [08:19<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 24/29, (Training Loss: 4.1726, samp_prob: 0.4013):  60%|██████    | 6/10 [08:22<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 25/29, (Training Loss: 4.3282, samp_prob: 0.4013):  60%|██████    | 6/10 [08:24<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 26/29, (Training Loss: 4.0668, samp_prob: 0.4013):  60%|██████    | 6/10 [08:26<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 27/29, (Training Loss: 4.3472, samp_prob: 0.4013):  60%|██████    | 6/10 [08:28<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 28/29, (Training Loss: 4.4685, samp_prob: 0.4013):  60%|██████    | 6/10 [08:30<04:59, 74.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 6, step 28/29, (Training Loss: 4.4685, samp_prob: 0.4013):  70%|███████   | 7/10 [08:30<03:33, 71.05s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISHED] Epoch 6, (Training Loss (per epoch): 124.6124 samp_prob: 0.4013)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 7, step 0/29, (Training Loss: 4.1741, samp_prob: 0.3100):  70%|███████   | 7/10 [08:32<03:33, 71.05s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 1/29, (Training Loss: 4.1009, samp_prob: 0.3100):  70%|███████   | 7/10 [08:34<03:33, 71.05s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 7, step 1/29......\n",
      "\n",
      "id: 43\n",
      "answer: ['<BOS>', 'a', 'young', 'man', 'is', 'hammering', 'nails', 'into', 'a', 'strip', 'of', 'wood', 'with', 'a', 'camera']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 7, step 2/29, (Training Loss: 4.1210, samp_prob: 0.3100):  70%|███████   | 7/10 [08:37<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 3/29, (Training Loss: 4.1853, samp_prob: 0.3100):  70%|███████   | 7/10 [08:39<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 4/29, (Training Loss: 4.2327, samp_prob: 0.3100):  70%|███████   | 7/10 [08:41<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 5/29, (Training Loss: 4.0579, samp_prob: 0.3100):  70%|███████   | 7/10 [08:43<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 6/29, (Training Loss: 4.2757, samp_prob: 0.3100):  70%|███████   | 7/10 [08:45<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 7/29, (Training Loss: 4.1824, samp_prob: 0.3100):  70%|███████   | 7/10 [08:47<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 8/29, (Training Loss: 4.4081, samp_prob: 0.3100):  70%|███████   | 7/10 [08:49<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 9/29, (Training Loss: 4.2084, samp_prob: 0.3100):  70%|███████   | 7/10 [08:51<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 10/29, (Training Loss: 3.9972, samp_prob: 0.3100):  70%|███████   | 7/10 [08:54<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 11/29, (Training Loss: 4.2213, samp_prob: 0.3100):  70%|███████   | 7/10 [08:56<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 12/29, (Training Loss: 4.1246, samp_prob: 0.3100):  70%|███████   | 7/10 [08:58<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 13/29, (Training Loss: 4.1601, samp_prob: 0.3100):  70%|███████   | 7/10 [09:00<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 14/29, (Training Loss: 4.3404, samp_prob: 0.3100):  70%|███████   | 7/10 [09:02<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 15/29, (Training Loss: 4.1516, samp_prob: 0.3100):  70%|███████   | 7/10 [09:04<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 16/29, (Training Loss: 4.5064, samp_prob: 0.3100):  70%|███████   | 7/10 [09:07<03:33, 71.05s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 7, step 16/29......\n",
      "\n",
      "id: 22\n",
      "answer: ['<BOS>', 'a', 'boy', 'is', 'singing', 'into', 'a', 'mic']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 7, step 17/29, (Training Loss: 4.2344, samp_prob: 0.3100):  70%|███████   | 7/10 [09:09<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 18/29, (Training Loss: 4.1712, samp_prob: 0.3100):  70%|███████   | 7/10 [09:11<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 19/29, (Training Loss: 4.1624, samp_prob: 0.3100):  70%|███████   | 7/10 [09:13<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 20/29, (Training Loss: 4.4953, samp_prob: 0.3100):  70%|███████   | 7/10 [09:15<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 21/29, (Training Loss: 4.4343, samp_prob: 0.3100):  70%|███████   | 7/10 [09:17<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 22/29, (Training Loss: 4.3511, samp_prob: 0.3100):  70%|███████   | 7/10 [09:19<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 23/29, (Training Loss: 4.3630, samp_prob: 0.3100):  70%|███████   | 7/10 [09:22<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 24/29, (Training Loss: 4.1038, samp_prob: 0.3100):  70%|███████   | 7/10 [09:24<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 25/29, (Training Loss: 4.2630, samp_prob: 0.3100):  70%|███████   | 7/10 [09:26<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 26/29, (Training Loss: 4.0174, samp_prob: 0.3100):  70%|███████   | 7/10 [09:28<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 27/29, (Training Loss: 4.2837, samp_prob: 0.3100):  70%|███████   | 7/10 [09:30<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 28/29, (Training Loss: 4.3588, samp_prob: 0.3100):  70%|███████   | 7/10 [09:32<03:33, 71.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 7, step 28/29, (Training Loss: 4.3588, samp_prob: 0.3100):  80%|████████  | 8/10 [09:32<02:16, 68.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISHED] Epoch 7, (Training Loss (per epoch): 122.6865 samp_prob: 0.3100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 8, step 0/29, (Training Loss: 4.1456, samp_prob: 0.2315):  80%|████████  | 8/10 [09:34<02:16, 68.36s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 1/29, (Training Loss: 4.0571, samp_prob: 0.2315):  80%|████████  | 8/10 [09:37<02:16, 68.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 8, step 1/29......\n",
      "\n",
      "id: 11\n",
      "answer: ['<BOS>', 'a', 'boy', 'sitting', 'at', 'a', 'picnic', 'table', 'watches', 'a', 'man', 'hurdle', 'over', 'the', 'table', 'and', 'do', 'a', 'back', 'flip']\n",
      "prediction: ['<BOS>', 'a', 'man', 'a', 'is', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 8, step 2/29, (Training Loss: 4.0920, samp_prob: 0.2315):  80%|████████  | 8/10 [09:39<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 3/29, (Training Loss: 4.1351, samp_prob: 0.2315):  80%|████████  | 8/10 [09:41<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 4/29, (Training Loss: 4.2552, samp_prob: 0.2315):  80%|████████  | 8/10 [09:43<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 5/29, (Training Loss: 4.0080, samp_prob: 0.2315):  80%|████████  | 8/10 [09:45<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 6/29, (Training Loss: 4.1859, samp_prob: 0.2315):  80%|████████  | 8/10 [09:47<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 7/29, (Training Loss: 4.0661, samp_prob: 0.2315):  80%|████████  | 8/10 [09:49<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 8/29, (Training Loss: 4.3613, samp_prob: 0.2315):  80%|████████  | 8/10 [09:52<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 9/29, (Training Loss: 4.1085, samp_prob: 0.2315):  80%|████████  | 8/10 [09:54<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 10/29, (Training Loss: 3.9214, samp_prob: 0.2315):  80%|████████  | 8/10 [09:56<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 11/29, (Training Loss: 4.1044, samp_prob: 0.2315):  80%|████████  | 8/10 [09:58<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 12/29, (Training Loss: 4.0946, samp_prob: 0.2315):  80%|████████  | 8/10 [10:00<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 13/29, (Training Loss: 4.1649, samp_prob: 0.2315):  80%|████████  | 8/10 [10:02<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 14/29, (Training Loss: 4.2620, samp_prob: 0.2315):  80%|████████  | 8/10 [10:04<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 15/29, (Training Loss: 4.0612, samp_prob: 0.2315):  80%|████████  | 8/10 [10:07<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 16/29, (Training Loss: 4.4793, samp_prob: 0.2315):  80%|████████  | 8/10 [10:09<02:16, 68.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 8, step 16/29......\n",
      "\n",
      "id: 5\n",
      "answer: ['<BOS>', 'a', 'man', 'shoots', 'in', 'a', 'practice', 'range']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'a', 'a', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 8, step 17/29, (Training Loss: 4.1076, samp_prob: 0.2315):  80%|████████  | 8/10 [10:11<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 18/29, (Training Loss: 4.0751, samp_prob: 0.2315):  80%|████████  | 8/10 [10:13<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 19/29, (Training Loss: 4.1204, samp_prob: 0.2315):  80%|████████  | 8/10 [10:15<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 20/29, (Training Loss: 4.4291, samp_prob: 0.2315):  80%|████████  | 8/10 [10:17<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 21/29, (Training Loss: 4.4133, samp_prob: 0.2315):  80%|████████  | 8/10 [10:20<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 22/29, (Training Loss: 4.3278, samp_prob: 0.2315):  80%|████████  | 8/10 [10:22<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 23/29, (Training Loss: 4.2951, samp_prob: 0.2315):  80%|████████  | 8/10 [10:24<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 24/29, (Training Loss: 4.0918, samp_prob: 0.2315):  80%|████████  | 8/10 [10:26<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 25/29, (Training Loss: 4.2404, samp_prob: 0.2315):  80%|████████  | 8/10 [10:28<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 26/29, (Training Loss: 3.8657, samp_prob: 0.2315):  80%|████████  | 8/10 [10:30<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 27/29, (Training Loss: 4.2343, samp_prob: 0.2315):  80%|████████  | 8/10 [10:32<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 28/29, (Training Loss: 4.3544, samp_prob: 0.2315):  80%|████████  | 8/10 [10:34<02:16, 68.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 8, step 28/29, (Training Loss: 4.3544, samp_prob: 0.2315):  90%|█████████ | 9/10 [10:34<01:06, 66.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISHED] Epoch 8, (Training Loss (per epoch): 121.0575 samp_prob: 0.2315)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 9, step 0/29, (Training Loss: 4.0550, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:37<01:06, 66.52s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 1/29, (Training Loss: 3.9690, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:39<01:06, 66.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 9, step 1/29......\n",
      "\n",
      "id: 1\n",
      "answer: ['<BOS>', 'a', 'woman', 'is', 'dancing']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 9, step 2/29, (Training Loss: 4.0585, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:41<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 3/29, (Training Loss: 4.0366, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:43<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 4/29, (Training Loss: 4.0889, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:45<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 5/29, (Training Loss: 3.8871, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:47<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 6/29, (Training Loss: 4.1065, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:49<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 7/29, (Training Loss: 4.0142, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:52<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 8/29, (Training Loss: 4.3185, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:54<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 9/29, (Training Loss: 4.0799, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:56<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 10/29, (Training Loss: 3.8034, samp_prob: 0.1680):  90%|█████████ | 9/10 [10:58<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 11/29, (Training Loss: 4.0640, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:00<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 12/29, (Training Loss: 4.0072, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:02<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 13/29, (Training Loss: 4.0614, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:04<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 14/29, (Training Loss: 4.2416, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:06<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 15/29, (Training Loss: 4.0379, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:09<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 16/29, (Training Loss: 4.4423, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:11<01:06, 66.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train. Prediction] Epoch 9, step 16/29......\n",
      "\n",
      "id: 3\n",
      "answer: ['<BOS>', 'five', 'kittens', 'in', 'a', 'row', 'and', 'then', 'in', 'a', 'circle', 'are', 'eating', 'off', 'of', 'plates']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'on']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 9, step 17/29, (Training Loss: 4.0351, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:13<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 18/29, (Training Loss: 4.0366, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:15<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 19/29, (Training Loss: 4.0383, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:17<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 20/29, (Training Loss: 4.3465, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:19<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 21/29, (Training Loss: 4.3445, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:22<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 22/29, (Training Loss: 4.2996, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:24<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 23/29, (Training Loss: 4.2624, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:26<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 24/29, (Training Loss: 4.0026, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:28<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 25/29, (Training Loss: 4.1877, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:30<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 26/29, (Training Loss: 3.9274, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:32<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 27/29, (Training Loss: 4.2161, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:34<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 28/29, (Training Loss: 4.2552, samp_prob: 0.1680):  90%|█████████ | 9/10 [11:36<01:06, 66.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 9, step 28/29, (Training Loss: 4.2552, samp_prob: 0.1680): 100%|██████████| 10/10 [11:36<00:00, 69.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISHED] Epoch 9, (Training Loss (per epoch): 119.2239 samp_prob: 0.1680)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_graph = tf.Graph()\n",
    "gpu_config = tf.ConfigProto()\n",
    "gpu_config.gpu_options.allow_growth = True\n",
    "\n",
    "print('train_graph: start')\n",
    "\n",
    "vocab_num = n_words\n",
    "num_epochs = 10\n",
    "num_display_steps = 15\n",
    "\n",
    "with train_graph.as_default():\n",
    "    feat = tf.placeholder(tf.float32, [None, n_frames, n_inputs], name='video_features')\n",
    "    captions = tf.placeholder(tf.int32, [None, max_caption_len], name='captions')\n",
    "    sampling = tf.placeholder(tf.bool, [max_caption_len], name='sampling')\n",
    "    cap_len = tf.placeholder(tf.int32, [None], name='cap_len')\n",
    "    model = S2VT(vocab_num=vocab_num, lr=learning_rate)\n",
    "    logits, loss_op, summary = model.build_model(feat, captions, cap_len, sampling, phases['train'])\n",
    "    dec_pred = model.inference(logits)\n",
    "    train_op = model.optimize(loss_op)\n",
    "\n",
    "    model.set_saver(tf.train.Saver(max_to_keep = 3))\n",
    "    init = tf.global_variables_initializer()\n",
    "train_sess = tf.Session(graph=train_graph, config=gpu_config)\n",
    "\n",
    "\n",
    "train_sess.run(init)\n",
    "\n",
    "samp_prob = inv_sigmoid(num_epochs)\n",
    "pbar = tqdm(range(0, num_epochs))\n",
    "\n",
    "for epo in pbar:\n",
    "    num_steps = n_batches\n",
    "    epo_loss = 0\n",
    "    for i in range(0, num_steps):\n",
    "        data_batch = np.array(vid_batch[i])\n",
    "        label_batch = np.array(intencode_batch[i])\n",
    "        caption_lens_batch = np.array(cap_len_batch[i])\n",
    "        \n",
    "        #data_batch, label_batch, caption_lens_batch, id_batch = datasetTrain.next_batch()\n",
    "        \n",
    "        samp = schedule_sampling(samp_prob[epo], caption_lens_batch)\n",
    "        \n",
    "        if i % num_display_steps == 1:\n",
    "            # training \n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            _, loss, p, summ = train_sess.run([train_op, loss_op, dec_pred, summary], \n",
    "                            feed_dict={feat: data_batch,\n",
    "                                       captions: label_batch,\n",
    "                                       cap_len: caption_lens_batch,\n",
    "                                       sampling: samp},\n",
    "                            options=run_options)\n",
    "            \n",
    "            #summary_writer.add_summary(summ, global_step=(epo * num_steps) + i)\n",
    "            print(\"\\n[Train. Prediction] Epoch \" + str(epo) + \", step \" + str(i) + \"/\" + str(num_steps) + \"......\",)\n",
    "            \n",
    "            dec_print_train(p, caption_lens_batch, label_batch, index2token, batch_size, values)\n",
    "\n",
    "        else:\n",
    "            _, loss, p = train_sess.run([train_op, loss_op, dec_pred], \n",
    "                            feed_dict={feat: data_batch,\n",
    "                                       captions: label_batch,\n",
    "                                       cap_len: caption_lens_batch,\n",
    "                                       sampling: samp})\n",
    "\n",
    "        epo_loss += loss\n",
    "        pbar.set_description(\"Epoch \" + str(epo) + \", step \" + str(i) + \"/\" + str(num_steps) + \\\n",
    "            \", (Training Loss: \" + \"{:.4f}\".format(loss) + \\\n",
    "            \", samp_prob: \" + \"{:.4f}\".format(samp_prob[epo]) + \")\" )\n",
    "\n",
    "    print(\"\\n[FINISHED] Epoch \" + str(epo) + \", (Training Loss (per epoch): \" + \"{:.4f}\".format(epo_loss) + \" samp_prob: \" + \"{:.4f}\".format(samp_prob[epo]) + \")\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Video_Caption_Generator(dim_image=n_features, \n",
    "#                                 n_words = n_words, \n",
    "#                                 dim_hidden = n_hidden, \n",
    "#                                 batch_size=batch_size, \n",
    "#                                 n_lstm_steps=80,\n",
    "#                                 n_video_lstm_step=80,\n",
    "#                                 n_caption_lstm_step=80,\n",
    "#                                 bias_init_vector=bias_init_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_loss, tf_video, tf_video_mask, tf_caption, tf_caption_mask, tf_probs = model.build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_words = n_words\n",
    "\n",
    "# with tf.Graph().as_default() as graph:\n",
    "    \n",
    "\n",
    "#     weights_enc = tf.Variable(tf.random_uniform([n_features, n_hidden],-0.1,0.1),name=\"weights_enc\")\n",
    "#     bias_enc = tf.Variable(tf.zeros([n_hidden]),name=\"bias_enc\")\n",
    "\n",
    "#     weights_dec = tf.Variable(tf.random_uniform([n_hidden, n_words],-0.1,0.1),name=\"weights_dec\")\n",
    "#     bias_dec = tf.Variable(tf.zeros([n_words]),name=\"bias_dec\")\n",
    "\n",
    "\n",
    "#     x_video = tf.placeholder(tf.float32, (None, no_of_frames, n_features),'video_features') #inputs\n",
    "\n",
    "#     batch_size = tf.shape(x_video)[0]\n",
    "    \n",
    "#     x_video_drop = tf.nn.dropout(x_video, 0.5)\n",
    "    \n",
    "#     x_video_flat = tf.reshape(x_video_drop,[-1,n_features])\n",
    "\n",
    "#     y_label = tf.placeholder(tf.int32,(None, sizeof_sentence),'captions') #outputs\n",
    "\n",
    "\n",
    "#     #sampling = tf.placeholder(tf.bool, [sizeof_sentence], name='sampling')\n",
    "#     padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "#     loss = 0.0\n",
    "\n",
    "#     ########## DATA ###########\n",
    "#     # Example: For i = 0\n",
    "#     #batch_x = np.array(vid_batch[0])\n",
    "#     #batch_y = np.array(intencode_batch[0])\n",
    "#     ###########################\n",
    "\n",
    "#     input_embedding = tf.matmul(x_video_flat,weights_enc) + bias_enc\n",
    "#     input_embedding = tf.reshape(input_embedding,[-1, no_of_frames,n_hidden])\n",
    "#     input_embed = tf.transpose(input_embedding, perm=[1, 0, 2])\n",
    "\n",
    "#     with tf.device(\"/cpu:0\"):\n",
    "#         output_embedding = tf.Variable(tf.random_uniform((n_words, n_hidden),-0.1,0.1), name='dec_embedding')\n",
    "#     # output_embed = tf.nn.embedding_lookup(output_embedding,y_label)\n",
    "    \n",
    "#     ## ENCODING #################################\n",
    "    \n",
    "#     with tf.variable_scope(\"LSTM1\"):\n",
    "#         lstm1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden,state_is_tuple=True)\n",
    "#         lstm1 = tf.contrib.rnn.DropoutWrapper(lstm1, output_keep_prob=0.5)    \n",
    "\n",
    "#     with tf.variable_scope(\"LSTM2\"):\n",
    "#         lstm2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "#         lstm2 = tf.contrib.rnn.DropoutWrapper(lstm2, output_keep_prob=0.5)    \n",
    "\n",
    "\n",
    "#     state1 = lstm1.zero_state(batch_size, dtype=tf.float32)\n",
    "#     state2 = lstm2.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "#     for i in range(0, no_of_frames):\n",
    "        \n",
    "#         if i > 0:\n",
    "#                 tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "#         with tf.variable_scope(\"LSTM1\"):\n",
    "#             output1, state1 = lstm1(input_embed[i,:,:], state1)\n",
    "\n",
    "#         with tf.variable_scope(\"LSTM2\"):\n",
    "#             output2, state2 = lstm2(tf.concat([padding, output1], axis=1), state2)\n",
    "    \n",
    "#     ## DECODING ##################################\n",
    "    \n",
    "#     bos = tf.ones([batch_size, n_hidden])\n",
    "#     padding_in = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "#     logits = []\n",
    "#     cross_ent_list=[]\n",
    "#     max_prob_index = None\n",
    "\n",
    "\n",
    "#     for i in range(0, MAX_WORDS):\n",
    "        \n",
    "#         tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        \n",
    "#         with tf.variable_scope(\"LSTM1\"):\n",
    "#             output1, state1 = lstm1(padding_in, state1)\n",
    "            \n",
    "#         if i == 0:\n",
    "            \n",
    "#             with tf.variable_scope(\"LSTM2\"):\n",
    "#                 con = tf.concat([bos, output1], axis=1)\n",
    "#                 output2, state2 = lstm2(con, state2)\n",
    "                \n",
    "#         else:\n",
    "            \n",
    "#             with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "#                 feed_in = y_label[:,i]\n",
    "#                 #feed_in = tf.argmax()\n",
    "#                 output_embed = tf.nn.embedding_lookup(output_embedding,feed_in)\n",
    "                \n",
    "#             with tf.variable_scope(\"LSTM2\"):\n",
    "#                 con = tf.concat([output_embed, output1], axis=1)\n",
    "#                 output2, state2 = lstm2(con, state2)\n",
    "\n",
    "#         logit_words = tf.matmul(output2, weights_dec) + bias_dec\n",
    "#         logits.append(logit_words)\n",
    "\n",
    "#         word_i = y_label[:,i]\n",
    "\n",
    "#         one_hot_labels = tf.one_hot(word_i, n_words, on_value = 1, off_value = None, axis = 1) \n",
    "#         cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=one_hot_labels)\n",
    "#         cross_ent_list.append(cross_entropy)\n",
    "        \n",
    "        \n",
    "#         #current_loss = tf.reduce_sum(cross_entropy)/batch_size\n",
    "#         #loss = loss + current_loss\n",
    "\n",
    "#     cross_entropy_tensor = tf.stack(cross_ent_list, 1)\n",
    "#     loss = tf.reduce_sum(cross_entropy_tensor, axis=1)\n",
    "#     loss = tf.divide(loss, tf.cast(tf.Variable(sizeof_sentence), tf.float32))\n",
    "\n",
    "#     loss = tf.reduce_mean(loss, axis=0)\n",
    "    \n",
    "#     summary = tf.summary.scalar('training_loss', loss)\n",
    "\n",
    "#     params = tf.trainable_variables()\n",
    "#     #optimizer = tf.train.AdamOptimizer(learning_rate)#.minimize(loss_op)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     train_op = optimizer.minimize(loss)\n",
    "\n",
    "#     #train_step = optimizer.minimize(loss)\n",
    "    \n",
    "# #     gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "# #     gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "# #     train_op = optimizer.apply_gradients(zip(gradients, params))\n",
    "    \n",
    "# #     logits = tf.stack(logits, axis = 0)\n",
    "# #     logits = tf.reshape(logits, (sizeof_sentence, batch_size, n_words))\n",
    "# #     logits = tf.transpose(logits, [1, 0, 2])\n",
    "# #     preds = tf.argmax(logits,2)\n",
    "# #     correct_pred = tf.equal(tf.argmax(preds,1), tf.argmax(y_label,1))\n",
    "# #     accuracy = tf.reduce_mean(correct_pred)\n",
    "\n",
    "#     logits = tf.stack(logits,axis=0)\n",
    "#     logits = tf.transpose(logits, [1, 0, 2])\n",
    "#     output_preds = tf.argmax(logits,2)\n",
    "    \n",
    "#     #correct_pred = tf.equal(tf.argmax(output_preds, 1), tf.argmax(y_label, 1))\n",
    "#     #accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "#     saver = tf.train.Saver(max_to_keep=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "# gpu_config = tf.ConfigProto()\n",
    "\n",
    "# with tf.Session(graph=graph,config=gpu_config) as sess:\n",
    "\n",
    "#     loss_list_train = []\n",
    "#     loss_list_test = []\n",
    "#     preds_dict = {}\n",
    "\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     epochs = 10\n",
    "\n",
    "#     #training\n",
    "#     n=0\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "\n",
    "#         for i in range(n_batches):\n",
    "\n",
    "#             batch_x = np.array(vid_batch[i])\n",
    "#             #batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "#             batch_y = np.array(intencode_batch[i])\n",
    "\n",
    "#             _, batch_loss, preds = sess.run([train_op, loss, logits], feed_dict = {x_video: batch_x, y_label: batch_y})        \n",
    "\n",
    "#             loss_list_train.append(batch_loss)\n",
    "#             print(\"train: %f \" % (batch_loss))\n",
    "\n",
    "            \n",
    "#             n = n+1\n",
    "\n",
    "       \n",
    "#     #testing\n",
    "    \n",
    "#         saver.save(sess,ckpt_path, global_step=n)\n",
    "#         print('Model saved at ' + ckpt_path)\n",
    "        \n",
    "    \n",
    "#     for i in range(n_batches_test):\n",
    "\n",
    "#         batch_x_test = np.array(vid_batch_test[i])\n",
    "#         #batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "#         batch_y_test = np.array(intencode_batch_test[i])\n",
    "\n",
    "#         acc = sess.run(accuracy, feed_dict = {x_video: batch_x_test, y_label: batch_y_test})        \n",
    "#         print(\"accuracy %f\" % acc)\n",
    "\n",
    "# #         loss_list_test.append(batch_loss)\n",
    "# #         print(\"test:\", batch_loss)\n",
    "    \n",
    "# #         preds_dict[i] = batch_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"encoding\") as encoding_scope:\n",
    "#     lstm_enc = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "#     _, last_state = tf.nn.dynamic_rnn(lstm_enc, inputs=input_embed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "#     # TODO: create the decoder LSTMs, this is very similar to the above\n",
    "#     # you will need to set initial_state=last_state from the encoder\n",
    "#     lstm_dec = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "#     dec_outputs, _ = tf.nn.dynamic_rnn(lstm_dec,inputs=output_embed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #connect outputs to \n",
    "# logits = tf.contrib.layers.fully_connected(dec_outputs, num_outputs=len(index2token), activation_fn=None) \n",
    "\n",
    "# with tf.name_scope(\"optimization\"):\n",
    "#     # Loss function\n",
    "#     loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, sizeof_sentence]))\n",
    "#     # Optimizer\n",
    "#     optimizer = tf.train.RMSPropOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dec.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dec[0].get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_video.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from utilities import show_graph\n",
    "# show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def RNN(x, weights1, biases1):\n",
    "    \n",
    "#     x = tf.unstack(x,no_of_frames,1)\n",
    "    \n",
    "#     lstm_encoder = tf.keras.layers.LSTM(n_hidden, return_state=True) #reuse=tf.AUTO_REUSE)\n",
    "#     output_encoder,state_h,state_c = lstm_encoder(x) #,dtype=tf.float32)\n",
    "#     encoder_states = [state_h,state_c]\n",
    "    \n",
    "#     decoder\n",
    "    \n",
    "#     return tf.matmul(output1[-1],weights1) + bias1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(vid_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = RNN(x_video,weights1,bias1)\n",
    "# prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "# loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_label))\n",
    "\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "# train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# # Evaluate model (with test logits, for dropout to be disabled)\n",
    "# correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_label, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_y = np.array(intencode_batch[1])\n",
    "# np.shape(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_x = np.array(vid_batch[0])\n",
    "# print(np.shape(batch_x))\n",
    "# batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "# np.shape(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_dict[0]\n",
    "# def predicted_sentence(preds_dict):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_x = np.array(vid_batch[0])\n",
    "# batch_y = np.array(intencode_batch[0])\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "\n",
    "#     sess.run(train_op, feed_dict={x_video: batch_x, y_label: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# image_emb = tf.nn.xw_plus_b(x_video, weights1, bias1) \n",
    "# #image_emb = tf.reshape(image_emb, [batch_size, no_of_frames, n_hidden])\n",
    "\n",
    "# #lstm2 = tf.keras.layers.LSTMCell(n_hidden)\n",
    "\n",
    "# padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "\n",
    "# #Only read the frames\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "                \n",
    "# logit_words = tf.nn.xw_plus_b(output2, weights2, bias2)\n",
    "# cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logit_words,onehot_encoded)\n",
    "\n",
    "# loss = tf.reduce_sum(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     with sess.as_default():\n",
    "#         print(tf.nn.embedding_lookup(onehot_encoded,[1]).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Class",
   "language": "python",
   "name": "tf_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
