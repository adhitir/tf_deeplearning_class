{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.legacy_seq2seq as seq2seq\n",
    "from utilities import show_graph\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import collections\n",
    "import json\n",
    "import string\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line,token='word'):\n",
    "    if token == 'word':\n",
    "        return [line.split(' ')]\n",
    "    elif token == 'char':\n",
    "        return [list(line)]\n",
    "    else:\n",
    "        print('ERROR: unknown token type '+token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokanized_sentences):\n",
    "    # Flatten a list of token lists into a list of tokens\n",
    "    tokens = [tk for line in tokanized_sentences for tk in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_vid_data_into_batches(filename,batch_size,feat_filepath):\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "        \n",
    "    batches = len(datastore)/batch_size\n",
    "    batches = int(batches)\n",
    "        \n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    vid_batch = {}\n",
    "    sentence_set = {}\n",
    "    \n",
    "    for data in datastore:\n",
    "        \n",
    "        #### Extracting all feature vectors per video\n",
    "\n",
    "        \n",
    "        #vid_feat_list = []\n",
    "        \n",
    "        video_id = data[\"id\"]\n",
    "        features = np.load(feat_filepath.format(video_id))\n",
    "\n",
    "        vid_framefeats = [] #list of all feature vectors per video. Shape = [80,4096]\n",
    "        \n",
    "        for array in features:\n",
    "            vid_framefeats.append(array)\n",
    "\n",
    "        if j not in vid_batch:\n",
    "            vid_batch[j] = []\n",
    "\n",
    "        vid_batch[j].append(vid_framefeats)\n",
    "        \n",
    "        \n",
    "        #### Extracting only a single sentence per video into a standalone dict\n",
    "\n",
    "        sentences = data[\"caption\"]\n",
    "        sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "        table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "        sentences = [word.translate(table) for word in sentences]\n",
    "\n",
    "        sentence_set[i] = sentences[0] #0 for only the first sentence\\\n",
    "        \n",
    "        i = i+1\n",
    "\n",
    "        if i%batch_size == 0:\n",
    "            j = j+1            \n",
    "            \n",
    "    return vid_batch, batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename, feat_filepath):\n",
    "    \n",
    "    sentence_set = {}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "        \n",
    "    i = 0\n",
    "    for data in datastore:\n",
    "        \n",
    "        #### Extracting only a single sentence per video into a standalone dict\n",
    "\n",
    "        sentences = data[\"caption\"]\n",
    "        sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "        table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "        sentences = [word.translate(table) for word in sentences]\n",
    "\n",
    "        sentence_set[i] = sentences[0] #0 for only the first sentence\\\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "    return sentence_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping string tokens to numertical indices.\n",
    "def listVocab(sentence_set):\n",
    "    \n",
    "    PAD_token = 0\n",
    "    BOS_token = 1\n",
    "    EOS_token = 2\n",
    "    UNK_token = 3\n",
    "    \n",
    "    all_tokens = []\n",
    "    token2index = {\"<PAD>\": 0,\"<BOS>\":1,\"<EOS>\":2,\"<UNK>\":3}\n",
    "    index2token = {PAD_token: \"<PAD>\", BOS_token: \"<BOS>\", EOS_token: \"<EOS>\", UNK_token: \"<UNK>\"}\n",
    "    \n",
    "    #for set_i in vid_sentence_set:\n",
    "    #    sentence_set = vid_sentence_set[set_i]\n",
    "    #    for line in sentence_set: \n",
    "    \n",
    "    for n in sentence_set:\n",
    "        line = sentence_set[n]\n",
    "        tokenized_captions = tokenize(line) #Seperate the words\n",
    "        all_tokens += tokenized_captions\n",
    "    \n",
    "    counter = count_tokens(all_tokens) #Count the word repeatitions in each set\n",
    "    \n",
    "    counter_dict = counter.items()\n",
    "    counter_sort = sorted(counter_dict, key=lambda x:x[1],reverse=True) #sort by frequency of occurance \n",
    "    #print(counter_sort)\n",
    "\n",
    "    i = len(index2token)\n",
    "    values = [0,1,2,3]\n",
    "    tokens = [\"<PAD>\",\"<BOS>\",\"<EOS>\",\"<UNK>\"]\n",
    "    for token, freq in counter_sort:\n",
    "        index2token[i] = token\n",
    "        token2index[token] = i\n",
    "        values += [i]\n",
    "        tokens += [token]\n",
    "        i+=1\n",
    "    \n",
    "    return [tokens, values, token2index, index2token, len(index2token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenList(nestedList,output): \n",
    "    for i in nestedList: \n",
    "        if type(i) == list: \n",
    "            flattenList(i,output) \n",
    "        else: \n",
    "            output.append(i) \n",
    "            \n",
    "    return output\n",
    "\n",
    "def num_encode(test_sentence,index2token,tokens,tokenized_sentence=[],num_encoded_sentence=[]):\n",
    "    \n",
    "    tokenized_sentence.clear()\n",
    "    num_encoded_sentence.clear()\n",
    "    \n",
    "    tokenized_sentence = [\"<BOS>\"] + tokenize(test_sentence) + [\"<EOS>\"]\n",
    "    #print(tokenized_sentence)\n",
    "    output=[]\n",
    "    tokenized_sentence = flattenList(tokenized_sentence,output)\n",
    "\n",
    "    while len(tokenized_sentence) < MAX_WORDS:\n",
    "        tokenized_sentence.append(\"<PAD>\")    \n",
    "    #print(len(tokenized_sentence))\n",
    "    for ind, token in enumerate(tokenized_sentence):\n",
    "        if token in tokens:\n",
    "            for i in range(0,len(index2token)):\n",
    "                if token == index2token[i]: \n",
    "                    num_encoded_sentence.append(i) \n",
    "                    \n",
    "            #print(\"token exists\")\n",
    "        else:\n",
    "            num_encoded_sentence.append(3)\n",
    "            tokenized_sentence[ind] = tokens[3]\n",
    "            #print(\"token unknown\")\n",
    "            \n",
    "            \n",
    "                \n",
    "    #print(len(num_encoded_sentence))\n",
    "\n",
    "        \n",
    "    return tokenized_sentence, num_encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_sentence_data_into_batches(sentence_set, index2token,tokens,batch_size):\n",
    "\n",
    "    tokenizedsentence_batch = {}\n",
    "    intencode_batch = {}\n",
    "\n",
    "    ii = 0\n",
    "    jj = 0  \n",
    "\n",
    "    for n in sentence_set:\n",
    "        sentence = sentence_set[n]\n",
    "\n",
    "        tokenized_sentence,encoded_sentence = num_encode(sentence,index2token,tokens)\n",
    "        \n",
    "        #print(np.shape(encoded_sentence))\n",
    "\n",
    "        tokenized_sentence = list(tokenized_sentence)\n",
    "        encoded_sentence = list(encoded_sentence)\n",
    "\n",
    "        if jj not in intencode_batch:\n",
    "            #onehot_batch[jj] = []\n",
    "            intencode_batch[jj] = []\n",
    "            tokenizedsentence_batch[jj] = []\n",
    "\n",
    "        #print(np.shape(onehot_encoded_sentence))    \n",
    "        #onehot_batch[jj].append(onehot_encoded_sentence)\n",
    "        intencode_batch[jj].append(encoded_sentence)\n",
    "        tokenizedsentence_batch[jj].append(tokenized_sentence)\n",
    "\n",
    "        ii = ii+1\n",
    "\n",
    "        if ii%batch_size == 0:\n",
    "            jj = jj+1\n",
    "            \n",
    "        \n",
    "    return tokenizedsentence_batch, intencode_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of videos in the training set are 1450 and each video has 80 frames with 4096 features/units each\n",
      "There are 1988 unique words in the captions dataset\n"
     ]
    }
   ],
   "source": [
    "filename_train = 'MLDS_hw2_1_data/training_label.json'\n",
    "filename_test = 'MLDS_hw2_1_data/testing_label.json'\n",
    "feat_filepath_train = \"MLDS_hw2_1_data/training_data/feat/{}.npy\"\n",
    "feat_filepath_test = \"MLDS_hw2_1_data/testing_data/feat/{}.npy\"\n",
    "\n",
    "ckpt_path = 'saved_model/trained_model.ckpt'\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "MAX_WORDS = 80 #max number of words in a caption\n",
    "n_features = 4096\n",
    "no_of_frames = 80\n",
    "sizeof_sentence= MAX_WORDS\n",
    "learning_rate = 0.001\n",
    "\n",
    "#### PARSE TRAINING DATA #####\n",
    "\n",
    "#Parse Training Data into batches\n",
    "vid_batch, n_batches = parse_vid_data_into_batches(filename_train,batch_size,feat_filepath_train)\n",
    "print(\"The number of videos in the training set are %d and each video has 80 frames with 4096 features/units each\" % (n_batches*batch_size))\n",
    "\n",
    "# Extracting captions for each video\n",
    "sentence_set = extract_sentences(filename_train,feat_filepath_train)\n",
    "\n",
    "tokens, values, token2index, index2token, n_words = listVocab(sentence_set)\n",
    "print(\"There are %d unique words in the captions dataset\" % n_words)\n",
    "\n",
    "tokenizedsentence_batch, intencode_batch = parse_sentence_data_into_batches(sentence_set,index2token,tokens,batch_size)\n",
    "\n",
    "# # integer encode\n",
    "# label_encoder = LabelEncoder()\n",
    "# integer_encoded = label_encoder.fit_transform(values)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "# integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of videos in the test set are 100 and each video has 80 frames with 4096 features/units each\n"
     ]
    }
   ],
   "source": [
    "#### PARSE TESTING DATA #####\n",
    "\n",
    "#Parse Testing Data into batches\n",
    "vid_batch_test, n_batches_test = parse_vid_data_into_batches(filename_test,batch_size,feat_filepath_test)\n",
    "print(\"The number of videos in the test set are %d and each video has 80 frames with 4096 features/units each\" % (n_batches_test*batch_size))\n",
    "\n",
    "# Extracting captions for each video\n",
    "sentence_set_test = extract_sentences(filename_test,feat_filepath_test)\n",
    "tokenizedsentence_batch_test, intencode_batch_test = parse_sentence_data_into_batches(sentence_set_test,index2token,tokens,batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-1b5046e2612d>:18: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-11-1b5046e2612d>:46: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-11-1b5046e2612d>:110: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_words = n_words\n",
    "n_hidden = 600\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    \n",
    "\n",
    "    weights_enc = tf.Variable(tf.random_uniform([n_features, n_hidden],-0.1,0.1),name=\"weights_enc\")\n",
    "    bias_enc = tf.Variable(tf.zeros([n_hidden]),name=\"bias_enc\")\n",
    "\n",
    "    weights_dec = tf.Variable(tf.random_uniform([n_hidden, n_words],-0.1,0.1),name=\"weights_dec\")\n",
    "    bias_dec = tf.Variable(tf.zeros([n_words]),name=\"bias_dec\")\n",
    "\n",
    "\n",
    "    x_video = tf.placeholder(tf.float32, (None, no_of_frames, n_features),'video_features') #inputs\n",
    "\n",
    "    batch_size = tf.shape(x_video)[0]\n",
    "    \n",
    "    x_video_drop = tf.nn.dropout(x_video, 0.5)\n",
    "    \n",
    "    x_video_flat = tf.reshape(x_video_drop,[-1,n_features])\n",
    "\n",
    "    y_label = tf.placeholder(tf.int32,(None, sizeof_sentence),'captions') #outputs\n",
    "\n",
    "\n",
    "    #sampling = tf.placeholder(tf.bool, [sizeof_sentence], name='sampling')\n",
    "    padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    ########## DATA ###########\n",
    "    # Example: For i = 0\n",
    "    #batch_x = np.array(vid_batch[0])\n",
    "    #batch_y = np.array(intencode_batch[0])\n",
    "    ###########################\n",
    "\n",
    "    input_embedding = tf.matmul(x_video_flat,weights_enc) + bias_enc\n",
    "    input_embedding = tf.reshape(input_embedding,[-1, no_of_frames,n_hidden])\n",
    "    input_embed = tf.transpose(input_embedding, perm=[1, 0, 2])\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        output_embedding = tf.Variable(tf.random_uniform((n_words, n_hidden),-0.1,0.1), name='dec_embedding')\n",
    "    # output_embed = tf.nn.embedding_lookup(output_embedding,y_label)\n",
    "    \n",
    "    ## ENCODING #################################\n",
    "    \n",
    "    with tf.variable_scope(\"LSTM1\"):\n",
    "        lstm1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden,state_is_tuple=True)\n",
    "        lstm1 = tf.contrib.rnn.DropoutWrapper(lstm1, output_keep_prob=0.5)    \n",
    "\n",
    "    with tf.variable_scope(\"LSTM2\"):\n",
    "        lstm2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "        lstm2 = tf.contrib.rnn.DropoutWrapper(lstm2, output_keep_prob=0.5)    \n",
    "\n",
    "\n",
    "    state1 = lstm1.zero_state(batch_size, dtype=tf.float32)\n",
    "    state2 = lstm2.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    for i in range(0, no_of_frames):\n",
    "        \n",
    "        if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "        with tf.variable_scope(\"LSTM1\"):\n",
    "            output1, state1 = lstm1(input_embed[i,:,:], state1)\n",
    "\n",
    "        with tf.variable_scope(\"LSTM2\"):\n",
    "            output2, state2 = lstm2(tf.concat([padding, output1], axis=1), state2)\n",
    "    \n",
    "    ## DECODING ##################################\n",
    "    \n",
    "    bos = tf.ones([batch_size, n_hidden])\n",
    "    padding_in = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "    logits = []\n",
    "    cross_ent_list=[]\n",
    "    max_prob_index = None\n",
    "\n",
    "\n",
    "    for i in range(0, MAX_WORDS):\n",
    "        \n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        \n",
    "        with tf.variable_scope(\"LSTM1\"):\n",
    "            output1, state1 = lstm1(padding_in, state1)\n",
    "            \n",
    "        if i == 0:\n",
    "            \n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                con = tf.concat([bos, output1], axis=1)\n",
    "                output2, state2 = lstm2(con, state2)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "                feed_in = y_label[:,i]\n",
    "                #feed_in = tf.argmax()\n",
    "                output_embed = tf.nn.embedding_lookup(output_embedding,feed_in)\n",
    "                \n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                con = tf.concat([output_embed, output1], axis=1)\n",
    "                output2, state2 = lstm2(con, state2)\n",
    "\n",
    "        logit_words = tf.matmul(output2, weights_dec) + bias_dec\n",
    "        logits.append(logit_words)\n",
    "\n",
    "        word_i = y_label[:,i]\n",
    "\n",
    "        one_hot_labels = tf.one_hot(word_i, n_words, on_value = 1, off_value = None, axis = 1) \n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=one_hot_labels)\n",
    "        cross_ent_list.append(cross_entropy)\n",
    "        \n",
    "        \n",
    "        #current_loss = tf.reduce_sum(cross_entropy)/batch_size\n",
    "        #loss = loss + current_loss\n",
    "\n",
    "    cross_entropy_tensor = tf.stack(cross_ent_list, 1)\n",
    "    loss = tf.reduce_sum(cross_entropy_tensor, axis=1)\n",
    "    loss = tf.divide(loss, tf.cast(tf.Variable(sizeof_sentence), tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(loss, axis=0)\n",
    "    \n",
    "    summary = tf.summary.scalar('training_loss', loss)\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate)#.minimize(loss_op)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    #train_step = optimizer.minimize(loss)\n",
    "    \n",
    "#     gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "#     gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "#     train_op = optimizer.apply_gradients(zip(gradients, params))\n",
    "    \n",
    "#     logits = tf.stack(logits, axis = 0)\n",
    "#     logits = tf.reshape(logits, (sizeof_sentence, batch_size, n_words))\n",
    "#     logits = tf.transpose(logits, [1, 0, 2])\n",
    "#     preds = tf.argmax(logits,2)\n",
    "#     correct_pred = tf.equal(tf.argmax(preds,1), tf.argmax(y_label,1))\n",
    "#     accuracy = tf.reduce_mean(correct_pred)\n",
    "\n",
    "    logits = tf.stack(logits,axis=0)\n",
    "    logits = tf.transpose(logits, [1, 0, 2])\n",
    "    output_preds = tf.argmax(logits,2)\n",
    "    \n",
    "    #correct_pred = tf.equal(tf.argmax(output_preds, 1), tf.argmax(y_label, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 7.596560 \n",
      "train: 7.589375 \n",
      "train: 7.579945 \n",
      "train: 7.575838 \n",
      "train: 7.568577 \n",
      "train: 7.560453 \n",
      "train: 7.555425 \n",
      "train: 7.548172 \n",
      "train: 7.541787 \n",
      "train: 7.532041 \n",
      "train: 7.528676 \n",
      "train: 7.522356 \n",
      "train: 7.512043 \n",
      "train: 7.511115 \n",
      "train: 7.500875 \n",
      "train: 7.491843 \n",
      "train: 7.489013 \n",
      "train: 7.479468 \n",
      "train: 7.475438 \n",
      "train: 7.466541 \n",
      "train: 7.461650 \n",
      "train: 7.450930 \n",
      "train: 7.448647 \n",
      "train: 7.440543 \n",
      "train: 7.428465 \n",
      "train: 7.426885 \n",
      "train: 7.420441 \n",
      "train: 7.413893 \n",
      "train: 7.404820 \n",
      "Model saved at saved_model/trained_model.ckpt\n",
      "train: 7.398826 \n",
      "train: 7.393348 \n",
      "train: 7.380279 \n",
      "train: 7.371067 \n",
      "train: 7.371828 \n",
      "train: 7.363610 \n",
      "train: 7.355991 \n",
      "train: 7.353664 \n",
      "train: 7.345027 \n",
      "train: 7.332693 \n",
      "train: 7.328154 \n",
      "train: 7.319838 \n",
      "train: 7.309660 \n",
      "train: 7.310757 \n",
      "train: 7.299979 \n",
      "train: 7.291158 \n",
      "train: 7.293193 \n",
      "train: 7.277243 \n",
      "train: 7.271609 \n",
      "train: 7.262727 \n",
      "train: 7.265707 \n",
      "train: 7.255448 \n",
      "train: 7.250957 \n",
      "train: 7.236266 \n",
      "train: 7.221282 \n",
      "train: 7.223709 \n",
      "train: 7.209186 \n",
      "train: 7.207524 \n",
      "train: 7.197131 \n",
      "Model saved at saved_model/trained_model.ckpt\n",
      "train: 7.187214 \n",
      "train: 7.183674 \n",
      "train: 7.168092 \n",
      "train: 7.161187 \n",
      "train: 7.163852 \n",
      "train: 7.154275 \n",
      "train: 7.140050 \n",
      "train: 7.142984 \n",
      "train: 7.130803 \n",
      "train: 7.116169 \n",
      "train: 7.109544 \n",
      "train: 7.097243 \n",
      "train: 7.090673 \n",
      "train: 7.093690 \n",
      "train: 7.082947 \n",
      "train: 7.072678 \n",
      "train: 7.073851 \n",
      "train: 7.054083 \n",
      "train: 7.048632 \n",
      "train: 7.038770 \n",
      "train: 7.043842 \n",
      "train: 7.032833 \n",
      "train: 7.031461 \n",
      "train: 7.014094 \n",
      "train: 6.985504 \n",
      "train: 6.998162 \n",
      "train: 6.979480 \n",
      "train: 6.980598 \n",
      "train: 6.969759 \n",
      "Model saved at saved_model/trained_model.ckpt\n",
      "train: 6.954450 \n",
      "train: 6.948094 \n",
      "train: 6.934487 \n",
      "train: 6.919614 \n",
      "train: 6.925662 \n",
      "train: 6.915519 \n",
      "train: 6.902880 \n",
      "train: 6.905374 \n",
      "train: 6.890037 \n",
      "train: 6.875175 \n",
      "train: 6.861678 \n",
      "train: 6.846831 \n",
      "train: 6.836291 \n",
      "train: 6.839705 \n",
      "train: 6.831594 \n",
      "train: 6.820154 \n",
      "train: 6.829575 \n",
      "train: 6.793917 \n",
      "train: 6.793131 \n",
      "train: 6.778930 \n",
      "train: 6.788057 \n",
      "train: 6.773994 \n",
      "train: 6.767831 \n",
      "train: 6.741798 \n",
      "train: 6.715497 \n",
      "train: 6.729840 \n",
      "train: 6.701682 \n",
      "train: 6.711823 \n",
      "train: 6.694972 \n",
      "Model saved at saved_model/trained_model.ckpt\n",
      "train: 6.676648 \n",
      "train: 6.671216 \n",
      "train: 6.650709 \n",
      "train: 6.629992 \n",
      "train: 6.638737 \n",
      "train: 6.623178 \n",
      "train: 6.609774 \n",
      "train: 6.615143 \n",
      "train: 6.597023 \n",
      "train: 6.573755 \n",
      "train: 6.558552 \n",
      "train: 6.535131 \n",
      "train: 6.521737 \n",
      "train: 6.531717 \n",
      "train: 6.516204 \n",
      "train: 6.507557 \n",
      "train: 6.516500 \n",
      "train: 6.469828 \n",
      "train: 6.467638 \n",
      "train: 6.450367 \n",
      "train: 6.459304 \n",
      "train: 6.446374 \n",
      "train: 6.445940 \n",
      "train: 6.406576 \n",
      "train: 6.364712 \n",
      "train: 6.387641 \n",
      "train: 6.347880 \n",
      "train: 6.360682 \n",
      "train: 6.335836 \n",
      "Model saved at saved_model/trained_model.ckpt\n",
      "train: 6.308580 \n",
      "train: 6.301489 \n",
      "train: 6.274458 \n",
      "train: 6.251671 \n",
      "train: 6.261847 \n",
      "train: 6.247303 \n",
      "train: 6.227160 \n",
      "train: 6.229778 \n",
      "train: 6.203176 \n",
      "train: 6.168401 \n",
      "train: 6.151350 \n",
      "train: 6.122840 \n",
      "train: 6.102293 \n",
      "train: 6.115054 \n",
      "train: 6.091267 \n",
      "train: 6.070857 \n",
      "train: 6.088228 \n",
      "train: 6.021386 \n",
      "train: 6.015090 \n",
      "train: 5.991361 \n",
      "train: 6.009149 \n",
      "train: 5.978075 \n",
      "train: 5.979025 \n",
      "train: 5.918436 \n",
      "train: 5.861252 \n",
      "train: 5.896205 \n",
      "train: 5.835944 \n",
      "train: 5.852402 \n",
      "train: 5.822388 \n",
      "Model saved at saved_model/trained_model.ckpt\n",
      "train: 5.776581 \n",
      "train: 5.765614 \n",
      "train: 5.724539 \n",
      "train: 5.684264 \n",
      "train: 5.694325 \n",
      "train: 5.668523 \n",
      "train: 5.633951 \n",
      "train: 5.645068 \n",
      "train: 5.601952 \n",
      "train: 5.549983 \n",
      "train: 5.514447 \n",
      "train: 5.465929 \n",
      "train: 5.441981 \n",
      "train: 5.459684 \n",
      "train: 5.417076 \n",
      "train: 5.386791 \n",
      "train: 5.404595 \n",
      "train: 5.302598 \n",
      "train: 5.287187 \n",
      "train: 5.248659 \n",
      "train: 5.265101 \n",
      "train: 5.222593 \n",
      "train: 5.215164 \n",
      "train: 5.122279 \n",
      "train: 5.033487 \n",
      "train: 5.071638 \n",
      "train: 4.974137 \n",
      "train: 4.994940 \n",
      "train: 4.933146 \n",
      "Model saved at saved_model/trained_model.ckpt\n",
      "train: 4.858441 \n",
      "train: 4.837578 \n",
      "train: 4.764893 \n",
      "train: 4.692476 \n",
      "train: 4.706731 \n",
      "train: 4.654884 \n",
      "train: 4.577432 \n",
      "train: 4.595623 \n",
      "train: 4.528015 \n",
      "train: 4.435270 \n",
      "train: 4.380452 \n",
      "train: 4.295242 \n",
      "train: 4.247115 \n",
      "train: 4.241879 \n",
      "train: 4.172025 \n",
      "train: 4.121622 \n",
      "train: 4.140621 \n",
      "train: 3.958833 \n",
      "train: 3.940071 \n",
      "train: 3.856799 \n",
      "train: 3.864692 \n",
      "train: 3.800975 \n",
      "train: 3.792785 \n",
      "train: 3.636288 \n",
      "train: 3.469115 \n",
      "train: 3.531320 \n",
      "train: 3.369885 \n",
      "train: 3.403279 \n",
      "train: 3.311828 \n",
      "Model saved at saved_model/trained_model.ckpt\n",
      "train: 3.196116 \n",
      "train: 3.144387 \n",
      "train: 3.026014 \n",
      "train: 2.934062 \n",
      "train: 2.948545 \n",
      "train: 2.887110 \n",
      "train: 2.787687 \n",
      "train: 2.809117 \n",
      "train: 2.726300 \n",
      "train: 2.590802 \n",
      "train: 2.524354 \n",
      "train: 2.423809 \n",
      "train: 2.376239 \n",
      "train: 2.402787 \n",
      "train: 2.321413 \n",
      "train: 2.288544 \n",
      "train: 2.347547 \n",
      "train: 2.128590 \n",
      "train: 2.142827 \n",
      "train: 2.082519 \n",
      "train: 2.148281 \n",
      "train: 2.079629 \n",
      "train: 2.125004 \n",
      "train: 1.957093 \n",
      "train: 1.794250 \n",
      "train: 1.916711 \n",
      "train: 1.782518 \n",
      "train: 1.881932 \n",
      "train: 1.808027 \n",
      "Model saved at saved_model/trained_model.ckpt\n",
      "train: 1.716212 \n",
      "train: 1.718089 \n",
      "train: 1.658135 \n",
      "train: 1.597184 \n",
      "train: 1.676857 \n",
      "train: 1.642983 \n",
      "train: 1.612447 \n",
      "train: 1.687405 \n",
      "train: 1.633387 \n",
      "train: 1.557982 \n",
      "train: 1.517868 \n",
      "train: 1.440648 \n",
      "train: 1.442916 \n",
      "train: 1.539824 \n",
      "train: 1.507296 \n",
      "train: 1.498151 \n",
      "train: 1.600864 \n",
      "train: 1.417001 \n",
      "train: 1.453755 \n",
      "train: 1.445459 \n",
      "train: 1.551231 \n",
      "train: 1.534139 \n",
      "train: 1.600175 \n",
      "train: 1.439523 \n",
      "train: 1.308108 \n",
      "train: 1.475999 \n",
      "train: 1.351796 \n",
      "train: 1.475934 \n",
      "train: 1.441453 \n",
      "Model saved at saved_model/trained_model.ckpt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-76c3e29070cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mbatch_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintencode_batch_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_video\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "gpu_config = tf.ConfigProto()\n",
    "\n",
    "with tf.Session(graph=graph,config=gpu_config) as sess:\n",
    "\n",
    "    loss_list_train = []\n",
    "    loss_list_test = []\n",
    "    preds_dict = {}\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epochs = 10\n",
    "\n",
    "    #training\n",
    "    n=0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i in range(n_batches):\n",
    "\n",
    "            batch_x = np.array(vid_batch[i])\n",
    "            #batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "            batch_y = np.array(intencode_batch[i])\n",
    "\n",
    "            _, batch_loss, preds = sess.run([train_op, loss, logits], feed_dict = {x_video: batch_x, y_label: batch_y})        \n",
    "\n",
    "            loss_list_train.append(batch_loss)\n",
    "            print(\"train: %f \" % (batch_loss))\n",
    "\n",
    "            \n",
    "            n = n+1\n",
    "\n",
    "       \n",
    "    #testing\n",
    "    \n",
    "        saver.save(sess,ckpt_path, global_step=n)\n",
    "        print('Model saved at ' + ckpt_path)\n",
    "        \n",
    "    \n",
    "    for i in range(n_batches_test):\n",
    "\n",
    "        batch_x_test = np.array(vid_batch_test[i])\n",
    "        #batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "        batch_y_test = np.array(intencode_batch_test[i])\n",
    "\n",
    "        acc = sess.run(accuracy, feed_dict = {x_video: batch_x_test, y_label: batch_y_test})        \n",
    "        print(\"accuracy %f\" % acc)\n",
    "\n",
    "#         loss_list_test.append(batch_loss)\n",
    "#         print(\"test:\", batch_loss)\n",
    "    \n",
    "#         preds_dict[i] = batch_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.0200958e+00,  1.1233552e+00, -1.3844694e-01, ...,\n",
       "        -4.0627268e-01,  3.8382119e-01,  6.6204590e-01],\n",
       "       [ 6.8074985e+00,  5.2401006e-01,  8.9435023e-01, ...,\n",
       "        -2.1259536e-01,  5.9858853e-01,  1.4138797e-01],\n",
       "       [ 5.7976232e+00,  1.5898039e+00,  6.6757840e-01, ...,\n",
       "        -2.6962292e-01,  1.7390178e-01,  2.0177306e-03],\n",
       "       ...,\n",
       "       [ 8.2757206e+00,  7.3791459e-02,  8.7332702e-01, ...,\n",
       "        -5.5419064e-01,  3.6085778e-01,  6.4031482e-01],\n",
       "       [ 8.1912451e+00, -2.4131502e-01,  8.9898396e-01, ...,\n",
       "         5.2647275e-01,  2.8451374e-01,  7.5696123e-01],\n",
       "       [ 8.3191118e+00,  6.0896546e-01,  9.1250223e-01, ...,\n",
       "        -5.7238603e-01,  3.2245705e-01,  5.4325676e-01]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"encoding\") as encoding_scope:\n",
    "#     lstm_enc = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "#     _, last_state = tf.nn.dynamic_rnn(lstm_enc, inputs=input_embed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "#     # TODO: create the decoder LSTMs, this is very similar to the above\n",
    "#     # you will need to set initial_state=last_state from the encoder\n",
    "#     lstm_dec = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "#     dec_outputs, _ = tf.nn.dynamic_rnn(lstm_dec,inputs=output_embed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #connect outputs to \n",
    "# logits = tf.contrib.layers.fully_connected(dec_outputs, num_outputs=len(index2token), activation_fn=None) \n",
    "\n",
    "# with tf.name_scope(\"optimization\"):\n",
    "#     # Loss function\n",
    "#     loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, sizeof_sentence]))\n",
    "#     # Optimizer\n",
    "#     optimizer = tf.train.RMSPropOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dec.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dec[0].get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_video.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from utilities import show_graph\n",
    "# show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def RNN(x, weights1, biases1):\n",
    "    \n",
    "#     x = tf.unstack(x,no_of_frames,1)\n",
    "    \n",
    "#     lstm_encoder = tf.keras.layers.LSTM(n_hidden, return_state=True) #reuse=tf.AUTO_REUSE)\n",
    "#     output_encoder,state_h,state_c = lstm_encoder(x) #,dtype=tf.float32)\n",
    "#     encoder_states = [state_h,state_c]\n",
    "    \n",
    "#     decoder\n",
    "    \n",
    "#     return tf.matmul(output1[-1],weights1) + bias1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(vid_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = RNN(x_video,weights1,bias1)\n",
    "# prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "# loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_label))\n",
    "\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "# train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# # Evaluate model (with test logits, for dropout to be disabled)\n",
    "# correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_label, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_y = np.array(intencode_batch[1])\n",
    "# np.shape(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_x = np.array(vid_batch[0])\n",
    "# print(np.shape(batch_x))\n",
    "# batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "# np.shape(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dict[0]\n",
    "def predicted_sentence(preds_dict):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_x = np.array(vid_batch[0])\n",
    "# batch_y = np.array(intencode_batch[0])\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "\n",
    "#     sess.run(train_op, feed_dict={x_video: batch_x, y_label: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# image_emb = tf.nn.xw_plus_b(x_video, weights1, bias1) \n",
    "# #image_emb = tf.reshape(image_emb, [batch_size, no_of_frames, n_hidden])\n",
    "\n",
    "# #lstm2 = tf.keras.layers.LSTMCell(n_hidden)\n",
    "\n",
    "# padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "\n",
    "# #Only read the frames\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "                \n",
    "# logit_words = tf.nn.xw_plus_b(output2, weights2, bias2)\n",
    "# cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logit_words,onehot_encoded)\n",
    "\n",
    "# loss = tf.reduce_sum(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     with sess.as_default():\n",
    "#         print(tf.nn.embedding_lookup(onehot_encoded,[1]).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Class",
   "language": "python",
   "name": "tf_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
