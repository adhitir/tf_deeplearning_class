{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import collections\n",
    "import json\n",
    "import string\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line,token='word'):\n",
    "    if token == 'word':\n",
    "        return [line.split(' ')]\n",
    "    elif token == 'char':\n",
    "        return [list(line)]\n",
    "    else:\n",
    "        print('ERROR: unknown token type '+token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokanized_sentences):\n",
    "    # Flatten a list of token lists into a list of tokens\n",
    "    tokens = [tk for line in tokanized_sentences for tk in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'MLDS_hw2_1_data/training_label.json'\n",
    "with open(filename, 'r') as f:\n",
    "    datastore = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_feat_set = {}\n",
    "vid_sentence_set = {}\n",
    "\n",
    "sizeof_train = 0\n",
    "for data in datastore:\n",
    "    video_id = data[\"id\"]\n",
    "    vid_feat_set[video_id]=None\n",
    "    vid_sentence_set[video_id]=None\n",
    "    sizeof_train = sizeof_train+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in datastore:\n",
    "    video_id = data[\"id\"]\n",
    "    features = np.load(\"MLDS_hw2_1_data/training_data/feat/{}.npy\".format(video_id))\n",
    "    #print(video_id)\n",
    "    \n",
    "    vid_framefeats = []\n",
    "\n",
    "    for array in features:\n",
    "        vid_framefeats.append(array)\n",
    "    \n",
    "    vid_feat_set[video_id] = vid_framefeats\n",
    "    \n",
    "    #print(\"reading sentences in: %s\" % video_id)\n",
    "    sentences = data[\"caption\"]\n",
    "    sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "    table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "    sentences = [word.translate(table) for word in sentences]\n",
    "    vid_sentence_set[video_id] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of videos in the training set are 1450 and each video has 80 frames with 4096 features/units each\n"
     ]
    }
   ],
   "source": [
    "#sentence_set\n",
    "print(\"The number of videos in the training set are %d and each video has 80 frames with 4096 features/units each\" % len(vid_feat_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for set_i in vid_sentence_set:\n",
    "#     print(set_i)\n",
    "#     print(vid_sentence_set[set_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #vid_feat_set['WqQonRVs7WA_0_10.avi']\n",
    "\n",
    "# count = 0\n",
    "# for x in vid_sentence_set: \n",
    "#     if isinstance(vid_sentence_set[x], list): \n",
    "#         count += len(vid_sentence_set[x]) \n",
    "# print(count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping string tokens to numertical indices.\n",
    "def listVocab(vid_sentence_set):\n",
    "    \n",
    "    PAD_token = 0\n",
    "    BOS_token = 1\n",
    "    EOS_token = 2\n",
    "    UNK_token = 3\n",
    "    \n",
    "    all_tokens = []\n",
    "    values = [0,1,2,3]\n",
    "    token_index = [(PAD_token, \"<PAD>\"), (BOS_token, \"<BOS>\"), (EOS_token, \"<EOS>\"), (UNK_token, \"<UNK>\")]\n",
    "    for set_i in vid_sentence_set:\n",
    "        sentence_set = vid_sentence_set[set_i]\n",
    "        for line in sentence_set: \n",
    "            tokenized_captions = tokenize(line) #Seperate the words\n",
    "            all_tokens += tokenized_captions\n",
    "    \n",
    "    counter = count_tokens(all_tokens) #Count the word repeatitions in each set\n",
    "    \n",
    "    counter_dict = counter.items()\n",
    "    counter_sort = sorted(counter_dict, key=lambda x:x[1],reverse=True) #sort by frequency of occurance \n",
    "    #print(counter_sort)\n",
    "\n",
    "    i = len(token_index)\n",
    "    for token, freq in counter_sort:\n",
    "        token_index.append((i,token))\n",
    "        values+=[i]\n",
    "        i+=1\n",
    "    \n",
    "    return [values, token_index, len(token_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6061 unique words in the captions dataset\n"
     ]
    }
   ],
   "source": [
    "values, token_index, nums = listVocab(vid_sentence_set)\n",
    "print(\"There are %d unique words in the captions dataset\" % nums)\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shape(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenList(nestedList,output): \n",
    "    for i in nestedList: \n",
    "        if type(i) == list: \n",
    "            flattenList(i,output) \n",
    "        else: \n",
    "            output.append(i) \n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 50\n",
    "\n",
    "def num_encode(test_sentence,token_index,tokenized_sentence=[],num_encoded_sentence=[],onehot_encoded_sentence=[]):\n",
    "    \n",
    "    tokenized_sentence.clear()\n",
    "    num_encoded_sentence.clear()\n",
    "    onehot_encoded_sentence.clear()\n",
    "    \n",
    "    tokenized_sentence = [\"<BOS>\"] + tokenize(test_sentence) + [\"<EOS>\"]\n",
    "    #print(tokenized_sentence)\n",
    "    output=[]\n",
    "    tokenized_sentence = flattenList(tokenized_sentence,output)\n",
    "\n",
    "    while len(tokenized_sentence) < MAX_WORDS:\n",
    "        tokenized_sentence.append(\"<PAD>\")\n",
    "    #print(len(tokenized_sentence))\n",
    "   \n",
    "    for token in tokenized_sentence:\n",
    "        for i in range(0,len(token_index)):\n",
    "            if token == token_index[i][1]: \n",
    "                num_encoded_sentence.append(i) \n",
    "                onehot_encoded_sentence.append(onehot_encoded[i])\n",
    "        \n",
    "    return tokenized_sentence, num_encoded_sentence, onehot_encoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for set_i in sentence_set:\n",
    "num_encoded_dict = {}\n",
    "onehot_encoded_dict = {}\n",
    "\n",
    "for data in datastore:\n",
    "    video_id = data[\"id\"]\n",
    "    num_encoded_dict[video_id]=None\n",
    "    onehot_encoded_dict[video_id]=None\n",
    "  \n",
    "    \n",
    "for vid in vid_sentence_set:\n",
    "    sentence_set = vid_sentence_set[vid]\n",
    "    \n",
    "    num_encoded_per_set = []\n",
    "    onehot_encoded_per_set = []\n",
    "\n",
    "    for line in sentence_set:\n",
    "        #print(line)\n",
    "        _,encoded_sentence,onehot_encoded_sentence = num_encode(line,token_index)\n",
    "        #print(len(tokenized_sen))\n",
    "        encoded_sentence = list(encoded_sentence)\n",
    "        onehot_encoded_sentence = list(onehot_encoded_sentence)\n",
    "\n",
    "        #print(type(encoded_sentence)) \n",
    "        #print(\"nxt\")\n",
    "        num_encoded_per_set.append(encoded_sentence)\n",
    "        #print(num_encoded_per_set)\n",
    "\n",
    "        \n",
    "        onehot_encoded_per_set.append(onehot_encoded_sentence)\n",
    "\n",
    "    #print(num_encoded_per_set)\n",
    "    \n",
    "    num_encoded_dict[vid] = num_encoded_per_set\n",
    "    onehot_encoded_dict[vid] = onehot_encoded_per_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 1000\n",
    "vocab_size = nums\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_hidden,vocab_size]))\n",
    "bias = tf.Variable(tf.random_normal([vocab_size]))\n",
    "\n",
    "#the input is the feature set ( i think.) the label is the caption (i think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-f4d947218511>:13: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.nn.rnn_cell' has no attribute 'static_rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f4d947218511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-f4d947218511>\u001b[0m in \u001b[0;36mRNN\u001b[0;34m(x, weights, bias)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# generate prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# there are n_input outputs but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_class/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.nn.rnn_cell' has no attribute 'static_rnn'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "n_frames = 80\n",
    "data = datastore[1]\n",
    "x_input = vid_feat_set[datastore[1][\"id\"]]\n",
    "y_label = onehot_encoded_dict[datastore[1][\"id\"]]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[n_frames,None])\n",
    "\n",
    "def RNN(x, weights, bias):\n",
    "\n",
    "    x = tf.reshape(x, [-1, n_frames])\n",
    "\n",
    "    # 1-layer LSTM with n_hidden units.\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = tf.nn.rnn_cell.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights) + bias\n",
    "\n",
    "pred = RNN(x,weights,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, loss, onehot_pred = session.run([optimizer, cost, pred], feed_dict={x: x_input, y: })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 4096)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2.84397435, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        2.99545813]),\n",
       " array([2.90854335, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        2.98446941]),\n",
       " array([3.3231349 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        3.39244986]),\n",
       " array([3.49265981, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        2.94543219]),\n",
       " array([3.73490572, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        3.22727823]),\n",
       " array([3.83639526, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        3.28317118]),\n",
       " array([3.67887425, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        3.11239529]),\n",
       " array([3.79342461, 0.        , 0.        , ..., 0.        , 0.00770801,\n",
       "        3.01491785]),\n",
       " array([3.46713281, 0.        , 0.        , ..., 0.        , 0.37530333,\n",
       "        3.66016102]),\n",
       " array([2.77445221, 0.        , 0.        , ..., 0.        , 0.65369743,\n",
       "        3.74102354]),\n",
       " array([2.73554325, 0.        , 0.        , ..., 0.        , 0.71726948,\n",
       "        3.25793791]),\n",
       " array([2.74524117, 0.        , 0.        , ..., 0.        , 0.42325264,\n",
       "        3.45333481]),\n",
       " array([2.77905273, 0.        , 0.        , ..., 0.        , 0.82548738,\n",
       "        3.4675107 ]),\n",
       " array([3.00786877, 0.        , 0.        , ..., 0.        , 1.21641946,\n",
       "        3.66148853]),\n",
       " array([2.92304277, 0.        , 0.        , ..., 0.        , 0.97490317,\n",
       "        3.47319794]),\n",
       " array([3.0342834 , 0.        , 0.        , ..., 0.        , 1.10892439,\n",
       "        4.02698612]),\n",
       " array([2.27606845, 0.        , 0.        , ..., 0.        , 0.43903282,\n",
       "        4.5191164 ]),\n",
       " array([1.98057747, 0.        , 0.        , ..., 0.        , 0.38465083,\n",
       "        4.37058163]),\n",
       " array([2.31558585, 0.        , 0.        , ..., 0.        , 1.10329318,\n",
       "        4.14236784]),\n",
       " array([1.95616364, 0.        , 0.        , ..., 0.        , 0.06920534,\n",
       "        4.25341225]),\n",
       " array([2.00462222, 0.        , 0.        , ..., 0.        , 0.19613135,\n",
       "        3.22729349]),\n",
       " array([3.13720584, 0.        , 0.        , ..., 0.        , 0.33948022,\n",
       "        3.21678782]),\n",
       " array([2.74063063, 0.        , 0.        , ..., 0.        , 0.57707393,\n",
       "        3.50392723]),\n",
       " array([3.48857284, 0.        , 0.        , ..., 0.        , 1.26443887,\n",
       "        3.25404167]),\n",
       " array([4.34792662, 0.        , 0.        , ..., 0.        , 1.13327956,\n",
       "        3.62647343]),\n",
       " array([4.93247509, 0.        , 0.        , ..., 0.        , 1.3693831 ,\n",
       "        3.68966579]),\n",
       " array([3.45527959, 0.        , 0.        , ..., 0.        , 1.34383464,\n",
       "        3.69798803]),\n",
       " array([4.11409521, 0.        , 0.        , ..., 0.        , 1.56349993,\n",
       "        3.06324577]),\n",
       " array([3.25254846, 0.        , 0.        , ..., 0.        , 1.32370687,\n",
       "        3.1819911 ]),\n",
       " array([2.66558719, 0.        , 0.        , ..., 0.        , 1.09747171,\n",
       "        3.24113274]),\n",
       " array([2.91740751, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        2.54971337]),\n",
       " array([3.06314468, 0.        , 0.        , ..., 0.33818847, 1.29257464,\n",
       "        3.01800489]),\n",
       " array([2.75246429, 0.        , 0.        , ..., 0.        , 1.36574411,\n",
       "        3.64583111]),\n",
       " array([2.81293774, 0.        , 0.        , ..., 0.        , 0.9487893 ,\n",
       "        2.55642509]),\n",
       " array([1.96118045, 0.        , 0.        , ..., 0.53100646, 1.75962186,\n",
       "        1.574929  ]),\n",
       " array([0.82256997, 0.        , 0.        , ..., 1.2956388 , 1.21359277,\n",
       "        1.15489078]),\n",
       " array([1.96308851, 0.        , 0.        , ..., 1.21901631, 0.52209997,\n",
       "        0.64169544]),\n",
       " array([1.94883823, 0.        , 0.        , ..., 0.91087365, 1.170609  ,\n",
       "        1.78133714]),\n",
       " array([2.32886434, 0.        , 0.        , ..., 1.3067677 , 1.53199577,\n",
       "        2.39696836]),\n",
       " array([3.44241142, 0.        , 0.        , ..., 1.60794163, 0.93069714,\n",
       "        1.79219651]),\n",
       " array([3.37087655, 0.        , 0.        , ..., 1.82658339, 1.0539763 ,\n",
       "        1.95040035]),\n",
       " array([3.06156349, 0.        , 0.        , ..., 2.4835465 , 0.98559386,\n",
       "        1.99152219]),\n",
       " array([3.66015887, 0.        , 0.        , ..., 0.98453343, 0.        ,\n",
       "        0.90610343]),\n",
       " array([0.96214807, 0.        , 0.        , ..., 0.        , 0.48281571,\n",
       "        0.        ]),\n",
       " array([0.68748379, 0.        , 0.        , ..., 0.        , 1.61155963,\n",
       "        0.0244813 ]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.77479208,\n",
       "        0.21492198]),\n",
       " array([0.        , 0.        , 0.5507828 , ..., 0.        , 0.08299249,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.52563536,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.28541258,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.21826062, ..., 0.        , 0.36556423,\n",
       "        0.        ]),\n",
       " array([0.       , 0.       , 0.       , ..., 0.       , 0.1652441,\n",
       "        0.       ]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.09030241]),\n",
       " array([0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.1054374]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.58378303]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.41700071,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.48796853,\n",
       "        0.29686999]),\n",
       " array([0.        , 0.        , 0.01875103, ..., 0.        , 0.5574258 ,\n",
       "        1.24823976]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.52228028,\n",
       "        0.59023666]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.19381359]),\n",
       " array([0.55996633, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.27818349]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.92371386,\n",
       "        0.56293297]),\n",
       " array([1.03383124, 0.        , 0.        , ..., 0.        , 0.74957353,\n",
       "        0.93172932]),\n",
       " array([1.38087082, 0.        , 0.        , ..., 0.        , 0.37277696,\n",
       "        1.10525656]),\n",
       " array([1.32707489, 0.        , 0.        , ..., 0.        , 0.27557769,\n",
       "        1.12268281]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.62453133,\n",
       "        0.5183109 ]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.04942816,\n",
       "        0.32834435]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.63100505]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.06691492]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.36774266]),\n",
       " array([0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.5228495]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.24422896]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.07237059]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.65714645]),\n",
       " array([0.      , 0.      , 0.      , ..., 0.      , 0.      , 0.250081]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.17010605]),\n",
       " array([0.        , 0.        , 0.98395503, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0.        , 0.        , 0.06792653, ..., 0.        , 0.        ,\n",
       "        0.        ])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_feat_set[datastore[1][\"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
