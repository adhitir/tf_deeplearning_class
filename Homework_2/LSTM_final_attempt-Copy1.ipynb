{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.legacy_seq2seq as seq2seq\n",
    "from utilities import show_graph\n",
    "#from util import inv_sigmoid, linear_decay, dec_print_train, dec_print_val, dec_print_test\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import collections\n",
    "import json\n",
    "import string\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "n_inputs        = 4096\n",
    "n_hidden        = 600\n",
    "val_batch_size  = 100 #100\n",
    "n_frames        = 80\n",
    "max_caption_len = 50\n",
    "forget_bias_red = 1.0\n",
    "forget_bias_gre = 1.0\n",
    "dropout_prob    = 0.5\n",
    "n_attention     = n_hidden\n",
    "\n",
    "special_tokens  = {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "phases = {'train': 0, 'val': 1, 'test': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following function was taken from: https://github.com/AdrianHsu/S2VT-seq2seq-video-captioning-attention\n",
    "\n",
    "class S2VT:\n",
    "    def __init__(self, vocab_num = 0,lr = 1e-4):\n",
    "\n",
    "        self.vocab_num = vocab_num\n",
    "        self.learning_rate = lr\n",
    "\n",
    "     \n",
    "    def build_model(self, feat, captions=None, cap_len=None, sampling=None, phase=0):\n",
    "\n",
    "        weights = {\n",
    "            'W_feat': tf.Variable( tf.random_uniform([n_inputs, n_hidden], -0.1, 0.1), name='W_feat'), \n",
    "            'W_dec': tf.Variable(tf.random_uniform([n_hidden, self.vocab_num], -0.1, 0.1), name='W_dec')\n",
    "        }\n",
    "        biases = {\n",
    "            'b_feat':  tf.Variable( tf.zeros([n_hidden]), name='b_feat'),\n",
    "            'b_dec': tf.Variable(tf.zeros([self.vocab_num]), name='b_dec')\n",
    "        }   \n",
    "        embeddings = {\n",
    "         'emb': tf.Variable(tf.random_uniform([self.vocab_num, n_hidden], -0.1, 0.1), name='emb')\n",
    "        }\n",
    "\n",
    "        batch_size = tf.shape(feat)[0]\n",
    "\n",
    "        # cap_len: (250, 1) -> (250, 50)\n",
    "        cap_mask = tf.sequence_mask(cap_len, max_caption_len, dtype=tf.float32)\n",
    "     \n",
    "        if phase == phases['train']: #  add noise\n",
    "            noise = tf.random_uniform(tf.shape(feat), -0.1, 0.1, dtype=tf.float32)\n",
    "            feat = feat + noise\n",
    "\n",
    "        if phase == phases['train']:\n",
    "            feat = tf.nn.dropout(feat, dropout_prob)\n",
    "\n",
    "        feat = tf.reshape(feat, [-1, n_inputs])\n",
    "        image_emb = tf.matmul(feat, weights['W_feat']) + biases['b_feat']\n",
    "        image_emb = tf.reshape(image_emb, [-1, n_frames, n_hidden])\n",
    "        image_emb = tf.transpose(image_emb, perm=[1, 0, 2])\n",
    "        \n",
    "        with tf.variable_scope('LSTM1'):\n",
    "            lstm_red = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=forget_bias_red, state_is_tuple=True)\n",
    "            if phase == phases['train']:\n",
    "                lstm_red = tf.contrib.rnn.DropoutWrapper(lstm_red, output_keep_prob=dropout_prob)    \n",
    "        with tf.variable_scope('LSTM2'):\n",
    "            lstm_gre = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=forget_bias_gre, state_is_tuple=True)\n",
    "            if phase == phases['train']:\n",
    "                lstm_gre = tf.contrib.rnn.DropoutWrapper(lstm_gre, output_keep_prob=dropout_prob)    \n",
    "\n",
    "        state_red = lstm_red.zero_state(batch_size, dtype=tf.float32)\n",
    "        state_gre = lstm_gre.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "        padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "#         h_src = []\n",
    "        for i in range(0, n_frames):\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output_red, state_red = lstm_red(image_emb[i,:,:], state_red)\n",
    "            \n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output_gre, state_gre = lstm_gre(tf.concat([padding, output_red], axis=1), state_gre)\n",
    "#                 h_src.append(output_gre) # even though padding is augmented, output_gre/state_gre's shape not change\n",
    "\n",
    "#         h_src = tf.stack(h_src, axis = 0)\n",
    "\n",
    "        bos = tf.ones([batch_size, n_hidden])\n",
    "        padding_in = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "        logits = []\n",
    "        max_prob_index = None\n",
    "\n",
    "        \n",
    "\n",
    "        cross_ent_list = []\n",
    "        for i in range(0, max_caption_len):\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output_red, state_red = lstm_red(padding_in, state_red)\n",
    "\n",
    "            if i == 0:\n",
    "                with tf.variable_scope(\"LSTM2\"):\n",
    "                    con = tf.concat([bos, output_red], axis=1)\n",
    "                    output_gre, state_gre = lstm_gre(con, state_gre)\n",
    "            else:\n",
    "                if phase == phases['train']:\n",
    "                    if sampling[i] == True:\n",
    "                        feed_in = captions[:, i - 1]\n",
    "                    else:\n",
    "                        feed_in = tf.argmax(logit_words, 1)\n",
    "                else:\n",
    "                    feed_in = tf.argmax(logit_words, 1)\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    embed_result = tf.nn.embedding_lookup(embeddings['emb'], feed_in)\n",
    "                with tf.variable_scope(\"LSTM2\"):\n",
    "                    con = tf.concat([embed_result, output_red], axis=1)\n",
    "                    output_gre, state_gre = lstm_gre(con, state_gre)\n",
    "\n",
    "            logit_words = tf.matmul(output_gre, weights['W_dec']) + biases['b_dec']\n",
    "            logits.append(logit_words)\n",
    "\n",
    "#             if phase != phases['test']:\n",
    "            labels = captions[:, i]\n",
    "            one_hot_labels = tf.one_hot(labels, self.vocab_num, on_value = 1, off_value = None, axis = 1) \n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=one_hot_labels)\n",
    "            cross_entropy = cross_entropy * cap_mask[:, i]\n",
    "            cross_ent_list.append(cross_entropy)\n",
    "        \n",
    "        loss = 0.0\n",
    "#         if phase != phases['test']:\n",
    "        cross_entropy_tensor = tf.stack(cross_ent_list, 1)\n",
    "        loss = tf.reduce_sum(cross_entropy_tensor, axis=1)\n",
    "        loss = tf.divide(loss, tf.cast(cap_len, tf.float32))\n",
    "        loss = tf.reduce_mean(loss, axis=0)\n",
    "\n",
    "        logits = tf.stack(logits, axis = 0)\n",
    "        logits = tf.reshape(logits, (max_caption_len, batch_size, self.vocab_num))\n",
    "        logits = tf.transpose(logits, [1, 0, 2])\n",
    "        \n",
    "        summary = None\n",
    "        if phase == phases['train']:\n",
    "            summary = tf.summary.scalar('training_loss', loss)\n",
    "        elif phase == phases['val']:\n",
    "            summary = tf.summary.scalar('validation_loss', loss)\n",
    "            \n",
    "\n",
    "        return logits, loss, summary\n",
    "\n",
    "    def inference(self, logits):\n",
    "        \n",
    "        #print('using greedy search...')\n",
    "        dec_pred = tf.argmax(logits, 2)\n",
    "        return dec_pred\n",
    "\n",
    "    def optimize(self, loss_op):\n",
    "\n",
    "        params = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)#.minimize(loss_op)\n",
    "        gradients, variables = zip(*optimizer.compute_gradients(loss_op))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        train_op = optimizer.apply_gradients(zip(gradients, params))\n",
    "\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line,token='word'):\n",
    "    if token == 'word':\n",
    "        return [line.split(' ')]\n",
    "    elif token == 'char':\n",
    "        return [list(line)]\n",
    "    else:\n",
    "        print('ERROR: unknown token type '+token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokanized_sentences):\n",
    "    # Flatten a list of token lists into a list of tokens\n",
    "    tokens = [tk for line in tokanized_sentences for tk in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_into_lists(filename,batch_size,feat_filepath, index2token, tokens):\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "           \n",
    "    #sentence_set = extract_sentences(filename)\n",
    "    \n",
    "    mult_vids = []\n",
    "    all_sents = []\n",
    "    all_enc_sents = []\n",
    "    all_cap_len = []\n",
    "    all_ids = []\n",
    "    \n",
    "    for data in datastore:\n",
    "        \n",
    "        sentences = data[\"caption\"]\n",
    "        sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "        table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "        sentences = [word.translate(table) for word in sentences]\n",
    "        \n",
    "        num_sent = len(sentences)\n",
    "        \n",
    "        all_sents.extend(sentences)\n",
    "        \n",
    "        enc_sents = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            \n",
    "            #print(sentence)\n",
    "            tokenized_sentence, encoded_sentence, cap_len = num_encode(sentence,index2token,tokens)\n",
    "            #print(tokenized_sentence)\n",
    "            #print(encoded_sentence)\n",
    "            #print(cap_len)\n",
    "            #print(encoded_sentence)\n",
    "            encoded_sentence = list(encoded_sentence)\n",
    "\n",
    "            enc_sents.append(encoded_sentence)\n",
    "            all_cap_len.append(cap_len)\n",
    "            \n",
    "        all_enc_sents.extend(enc_sents)\n",
    "\n",
    "\n",
    "#         print(all_sents[0])\n",
    "#         print(all_enc_sents[0])\n",
    "#         print(np.shape(all_enc_sents))\n",
    "#         print(np.shape(all_sents))\n",
    "#         print(np.shape(all_cap_len))\n",
    "\n",
    "        #print(len(all_enc_sents))\n",
    "\n",
    "        #sentence_set[i] = sentences \n",
    "        #### Extracting all feature vectors per video\n",
    "        \n",
    "        video_id = data[\"id\"]\n",
    "        features = np.load(feat_filepath.format(video_id))\n",
    "        \n",
    "        for n in range(0,num_sent):\n",
    "            mult_vids.append(features)\n",
    "            all_ids.append(video_id)\n",
    "        \n",
    "        print(\"id: \" + str(video_id) + \" processed\")\n",
    "\n",
    "            \n",
    "    return mult_vids, all_sents, all_enc_sents, all_cap_len, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_data_into_batches(filename,batch_size,feat_filepath, index2token, tokens, mult_vids, all_sents, all_enc_sents, all_cap_len, all_ids):\n",
    "    #print(all_enc_sents[0:10])\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "           \n",
    "    vid_batch = {}\n",
    "    sent_batch = {}\n",
    "    enc_sent_batch = {}\n",
    "    cap_len_batch = {}\n",
    "    id_batch = {}\n",
    "    \n",
    "    #sentence_set = extract_sentences(filename)\n",
    "    \n",
    "#     mult_vids = []\n",
    "#     all_sents = []\n",
    "#     all_enc_sents = []\n",
    "#     all_cap_len = []\n",
    "    \n",
    "#     for data in datastore:\n",
    "        \n",
    "#         sentences = data[\"caption\"]\n",
    "#         sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "#         table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "#         sentences = [word.translate(table) for word in sentences]\n",
    "        \n",
    "#         num_sent = len(sentences)\n",
    "        \n",
    "#         all_sents.extend(sentences)\n",
    "        \n",
    "#         for sentence in sentences:\n",
    "#             tokenized_sentence, encoded_sentence, cap_len = num_encode(sentence,index2token,tokens)\n",
    "            \n",
    "#             #print(encoded_sentence)\n",
    "#             all_enc_sents.append(encoded_sentence)\n",
    "#             all_cap_len.append(cap_len)\n",
    "\n",
    "#         #print(all_enc_sents[0])\n",
    "#         #print(len(all_enc_sents))\n",
    "\n",
    "#         #sentence_set[i] = sentences \n",
    "#         #### Extracting all feature vectors per video\n",
    "        \n",
    "#         video_id = data[\"id\"]\n",
    "#         features = np.load(feat_filepath.format(video_id))\n",
    "        \n",
    "#         for n in range(0,num_sent):\n",
    "#             mult_vids.append(features)\n",
    "            \n",
    "#         #print(\"id: \" + str(data[\"id\"]) + \" processed\")\n",
    "\n",
    "\n",
    "    random.Random(30).shuffle(mult_vids)\n",
    "    random.Random(30).shuffle(all_sents)\n",
    "    random.Random(30).shuffle(all_enc_sents)\n",
    "    random.Random(30).shuffle(all_cap_len)\n",
    "    random.Random(30).shuffle(all_ids)\n",
    "    \n",
    "#     print(all_enc_sents[0:5])\n",
    "#     print(all_sents[0:5])\n",
    "    \n",
    "    batches = len(mult_vids)/batch_size\n",
    "    batches = int(batches)\n",
    "        \n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    for n in range(0,batches):\n",
    "        if j not in vid_batch:\n",
    "            vid_batch[j] = []\n",
    "            sent_batch[j] = []\n",
    "            enc_sent_batch[j]=[]\n",
    "            cap_len_batch[j] = []\n",
    "\n",
    "            \n",
    "        vid_batch[j] = mult_vids[i:i+batch_size]\n",
    "        sent_batch[j] = all_sents[i:i+batch_size]\n",
    "        enc_sent_batch[j] = all_enc_sents[i:i+batch_size]\n",
    "        cap_len_batch[j] = all_cap_len[i:i+batch_size]\n",
    "        id_batch[j] = all_ids[i:i+batch_size]\n",
    "\n",
    "        print('parsed batch %d' %j)\n",
    "        i = i+batch_size\n",
    "        j = j+1\n",
    "    \n",
    "#     if j not in vid_batch:\n",
    "#         vid_batch[j] = []\n",
    "\n",
    "#     vid_batch[j].append(features)\n",
    "                \n",
    "#     i = i+1\n",
    "\n",
    "#     if i%batch_size == 0:\n",
    "#         j = j+1 \n",
    "    #print(sent_batch[0])\n",
    "    #print(enc_sent_batch[0])\n",
    "            \n",
    "    return vid_batch, sent_batch, enc_sent_batch, cap_len_batch, batches, id_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename):\n",
    "    \n",
    "    sentence_set = {}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "        \n",
    "    i = 0\n",
    "    for data in datastore:\n",
    "        \n",
    "        #### Extracting only a single sentence per video into a standalone dict\n",
    "\n",
    "        sentences = data[\"caption\"]\n",
    "        sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "        table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "        sentences = [word.translate(table) for word in sentences]\n",
    "\n",
    "        sentence_set[i] = sentences #0 for only the first sentence\\\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "    return sentence_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping string tokens to numertical indices.\n",
    "def listVocab(sentence_set):\n",
    "    \n",
    "    PAD_token = 0\n",
    "    BOS_token = 1\n",
    "    EOS_token = 2\n",
    "    UNK_token = 3\n",
    "    \n",
    "    all_tokens = []\n",
    "    word_count = {}\n",
    "    token2index = {\"<PAD>\": 0,\"<BOS>\":1,\"<EOS>\":2,\"<UNK>\":3}\n",
    "    index2token = {PAD_token: \"<PAD>\", BOS_token: \"<BOS>\", EOS_token: \"<EOS>\", UNK_token: \"<UNK>\"}\n",
    "    \n",
    "    for set_i in sentence_set:\n",
    "        sentence_set_i = sentence_set[set_i]\n",
    "        for line in sentence_set_i:\n",
    "#             line = sentence_set[n]\n",
    "            tokenized_captions = tokenize(line) #Seperate the words\n",
    "            all_tokens += tokenized_captions\n",
    "    \n",
    "    counter = count_tokens(all_tokens) #Count the word repeatitions in each set\n",
    "    \n",
    "    counter_dict = counter.items()\n",
    "    counter_sort = sorted(counter_dict, key=lambda x:x[1],reverse=True) #sort by frequency of occurance \n",
    "    #print(counter_sort)\n",
    "\n",
    "    i = len(index2token)\n",
    "    values = [0,1,2,3]\n",
    "    tokens = [\"<PAD>\",\"<BOS>\",\"<EOS>\",\"<UNK>\"]\n",
    "    \n",
    "    for token, freq in counter_sort:\n",
    "        word_count[token] = freq\n",
    "        index2token[i] = token\n",
    "        token2index[token] = i\n",
    "        values += [i]\n",
    "        tokens += [token]\n",
    "        i+=1\n",
    "        \n",
    "    word_count['<PAD>'] = i\n",
    "    word_count['<BOS>'] = i\n",
    "    word_count['<EOS>'] = i\n",
    "    word_count['<UNK>'] = i\n",
    "    \n",
    "    bias_init_vector = np.array([1.0 * word_count[ index2token[i] ] for i in index2token])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
    "    \n",
    "    return [word_count, tokens, values, token2index, index2token, len(index2token),bias_init_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenList(nestedList,output): \n",
    "    for i in nestedList: \n",
    "        if type(i) == list: \n",
    "            flattenList(i,output) \n",
    "        else: \n",
    "            output.append(i) \n",
    "            \n",
    "    return output\n",
    "\n",
    "def num_encode(test_sentence,index2token,tokens,tokenized_sentence=[],num_encoded_sentence=[]):\n",
    "    \n",
    "    tokenized_sentence.clear()\n",
    "    num_encoded_sentence.clear()\n",
    "    \n",
    "    tokenized_sentence = [\"<BOS>\"] + tokenize(test_sentence) + [\"<EOS>\"]\n",
    "    #print(tokenized_sentence)\n",
    "    output=[]\n",
    "    tokenized_sentence = flattenList(tokenized_sentence,output)\n",
    "    \n",
    "    cap_len = len(tokenized_sentence)\n",
    "    \n",
    "    while len(tokenized_sentence) < MAX_WORDS:\n",
    "        tokenized_sentence.append(\"<PAD>\")    \n",
    "    \n",
    "    #print(len(tokenized_sentence))\n",
    "    \n",
    "    for ind, token in enumerate(tokenized_sentence):\n",
    "        if token in tokens:\n",
    "            for i in range(0,len(index2token)):\n",
    "                if token == index2token[i]: \n",
    "                    num_encoded_sentence.append(i) \n",
    "                    \n",
    "            #print(\"token exists\")\n",
    "        else:\n",
    "            num_encoded_sentence.append(3)\n",
    "            tokenized_sentence[ind] = tokens[3]\n",
    "            #print(\"token unknown\")\n",
    "            \n",
    "            \n",
    "                \n",
    "    #print(len(num_encoded_sentence))\n",
    "\n",
    "        \n",
    "    return tokenized_sentence, num_encoded_sentence, cap_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6061 unique words in the captions dataset\n",
      "id: ScdUht-pM6s_53_63.avi processed\n",
      "id: wkgGxsuNVSg_34_41.avi processed\n",
      "id: BtQtRGI0F2Q_15_20.avi processed\n",
      "id: k06Ge9ANKM8_5_16.avi processed\n",
      "id: sZf3VDsdDPM_107_114.avi processed\n",
      "id: shPymuahrsc_5_12.avi processed\n",
      "id: XOAgUVVwKEA_8_20.avi processed\n",
      "id: ufFT2BWh3BQ_0_8.avi processed\n",
      "id: 5YJaS2Eswg0_22_26.avi processed\n",
      "id: lw7pTwpx0K0_38_48.avi processed\n",
      "id: UbmZAe5u5FI_132_141.avi processed\n",
      "id: xCFCXzDUGjY_5_9.avi processed\n",
      "id: He7Ge7Sogrk_47_70.avi processed\n",
      "id: tJHUH9tpqPg_113_118.avi processed\n",
      "id: n016q1w8Q30_2_11.avi processed\n",
      "id: RjpbFlOHFps_8_25.avi processed\n",
      "id: 6JnGBs88sL0_4_10.avi processed\n",
      "id: EpMuCrbxE8A_107_115.avi processed\n",
      "id: HAjwXjwN9-A_16_24.avi processed\n",
      "id: 4xVGpDmA4lE_23_33.avi processed\n",
      "id: k5OKBX2e7xA_19_32.avi processed\n",
      "id: Jag7oTemldY_12_25.avi processed\n",
      "id: 8MVo7fje_oE_125_130.avi processed\n",
      "id: bqMmyY1ImkI_0_14.avi processed\n",
      "id: jTnrm338_KY_34_42.avi processed\n",
      "id: UdcObAQ5OOM_15_30.avi processed\n",
      "id: 4PcL6-mjRNk_11_18.avi processed\n",
      "id: 3qqEKTPxLNs_1_15.avi processed\n",
      "id: glrijRGnmc0_211_215.avi processed\n",
      "id: q7pOFn8s4zc_263_273.avi processed\n",
      "id: mtrCf667KDk_134_176.avi processed\n",
      "id: 0lh_UWF9ZP4_62_69.avi processed\n",
      "id: JntMAcTlOF0_50_70.avi processed\n",
      "id: 7NNg0_n-bS8_21_30.avi processed\n",
      "id: IhwPQL9dFYc_124_129.avi processed\n",
      "id: BAf3LXFUaGs_28_38.avi processed\n",
      "id: 6q1dX6thX3E_286_295.avi processed\n",
      "id: RZL9irxnhZ0_34_40.avi processed\n",
      "id: WWf0Z6ak3Dg_5_15.avi processed\n",
      "id: PeUHy0A1GF0_114_121.avi processed\n",
      "id: klteYv1Uv9A_27_33.avi processed\n",
      "id: e-j59PqJjSM_405_416.avi processed\n",
      "id: 778mkceE0UQ_40_46.avi processed\n",
      "id: 77iDIp40m9E_126_131.avi processed\n",
      "id: e-j59PqJjSM_50_98.avi processed\n",
      "id: Dgf0VHMEtNs_57_66.avi processed\n",
      "id: f9Won2JpOEU_60_80.avi processed\n",
      "id: dfOuTx66bJU_34_39.avi processed\n",
      "id: 04Gt01vatkk_248_265.avi processed\n",
      "id: rl1rVk_xIOs_1_16.avi processed\n",
      "id: v7iIZXtpIb8_5_15.avi processed\n",
      "id: DhwrBs96Kgk_120_124.avi processed\n",
      "id: qLwgb3F0aPU_298_305.avi processed\n",
      "id: qeKX-N1nKiM_0_5.avi processed\n",
      "id: 1Sp2__RCT0c_11_15.avi processed\n",
      "id: Fe4tO5vW9_E_64_70.avi processed\n",
      "id: mmSQTI6gMNQ_120_128.avi processed\n",
      "id: HV12kTtdTT4_5_14.avi processed\n",
      "id: 0lh_UWF9ZP4_27_31.avi processed\n",
      "id: Je3V7U5Ctj4_569_576.avi processed\n",
      "id: 30GeJHYoerk_121_126.avi processed\n",
      "id: 04Gt01vatkk_308_321.avi processed\n",
      "id: zulPFoY64wE_26_33.avi processed\n",
      "id: MrQd1zUVRUM_103_110.avi processed\n",
      "id: xxHx6s_DbUo_216_222.avi processed\n",
      "id: 71soiLO6I9U_15_24.avi processed\n",
      "id: UXs3eq68ZjE_250_255.avi processed\n",
      "id: jbzaMtPYtl8_48_58.avi processed\n",
      "id: 8HB7ywgJuTg_131_142.avi processed\n",
      "id: Cjf21Y19aUQ_82_86.avi processed\n",
      "id: qvg9eM4Hmzk_4_10.avi processed\n",
      "id: 5HAf_INrFy0_3_25.avi processed\n",
      "id: YmXCfQm0_CA_277_284.avi processed\n",
      "id: 88DOMJ11q2M_84_87.avi processed\n",
      "id: NUYu9c9XsgY_7_21.avi processed\n",
      "id: N3A7944_UJw_63_70.avi processed\n",
      "id: uJPupV4oLZ0_4_12.avi processed\n",
      "id: cnsjm3fNEec_4_10.avi processed\n",
      "id: J_evFB7RIKA_104_120.avi processed\n",
      "id: g1Gldu1KS44_8_14.avi processed\n",
      "id: s1ZABV7AQdA_38_48.avi processed\n",
      "id: tcxhOGyrCtI_15_21.avi processed\n",
      "id: inzk2fTUe1w_1_15.avi processed\n",
      "id: j2Dhf-xFUxU_13_20.avi processed\n",
      "id: MTjrZthHwJQ_2_11.avi processed\n",
      "id: J---aiyznGQ_0_6.avi processed\n",
      "id: ZbtpcGi2DWY_161_170.avi processed\n",
      "id: RSx5G0_xH48_12_17.avi processed\n",
      "id: ecm9gf2Pgkc_1_24.avi processed\n",
      "id: pW9DFPqoIsI_26_50.avi processed\n",
      "id: N2Cm0SLr0ZE_18_29.avi processed\n",
      "id: sJSmRik2c-c_1_7.avi processed\n",
      "id: zv2RIbUsnSw_335_341.avi processed\n",
      "id: aM-RcQj0a7I_37_55.avi processed\n",
      "id: TZ860P4iTaM_15_28.avi processed\n",
      "id: lo4KcsBN--A_0_10.avi processed\n",
      "id: u4T76jsPin0_0_11.avi processed\n",
      "id: 7HcYJKMxpcg_20_28.avi processed\n",
      "id: CGllPWAwmUo_1_15.avi processed\n",
      "id: WTf5EgVY5uU_124_128.avi processed\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_ids_testtest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b7f9d9ffe07b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                                                                            \u001b[0mall_enc_sents_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                                                                                            \u001b[0mall_cap_len_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                                                                                            all_ids_testtest)                                                        \n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_ids_testtest' is not defined"
     ]
    }
   ],
   "source": [
    "filename_train = 'MLDS_hw2_1_data/training_label.json'\n",
    "feat_filepath_train = \"MLDS_hw2_1_data/training_data/feat/{}.npy\"\n",
    "filename_test = 'MLDS_hw2_1_data/testing_label.json'\n",
    "feat_filepath_test = \"MLDS_hw2_1_data/testing_data/feat/{}.npy\"\n",
    "\n",
    "batch_size_test = 100\n",
    "\n",
    "\n",
    "ckpt_path = 'saved_model/trained_model.ckpt'\n",
    "\n",
    "MAX_WORDS = max_caption_len #max number of words in a caption\n",
    "n_features = n_inputs\n",
    "no_of_frames = n_frames\n",
    "sizeof_sentence= MAX_WORDS\n",
    "n_hidden = 1000\n",
    "\n",
    "#### Extract TRAINING CAPTIONS #####\n",
    "\n",
    "sentence_set = extract_sentences(filename_train)\n",
    "\n",
    "_, tokens, values, token2index, index2token, n_words, bias_init_vector = listVocab(sentence_set)\n",
    "print(\"There are %d unique words in the captions dataset\" % n_words)\n",
    "\n",
    "# #### PARSE TESTING DATA #####\n",
    "\n",
    "# Extracting captions for each video\n",
    "#sentence_set = extract_sentences(filename_test)\n",
    "\n",
    "mult_vids_test, all_sents_test, all_enc_sents_test, all_cap_len_test, all_ids_test = parse_data_into_lists(filename_test,\\\n",
    "                                                                                  batch_size_test,\\\n",
    "                                                                                  feat_filepath_test,\\\n",
    "                                                                                  index2token,\\\n",
    "                                                                                  tokens)                                                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 1594, 1093, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 4, 7, 5, 163, 14, 22, 147, 9, 6, 121, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 4, 90, 5, 42, 68, 26, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 4, 5987, 709, 48, 4915, 6, 2037, 768, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 4, 28, 5, 43, 422, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "['a missile explodes', 'a man is jumping with his hands on the ground', 'a monkey is walking through water', 'a bomb went off beyond the palm trees', 'a girl is eating noodles']\n",
      "parsed batch 0\n",
      "parsed batch 1\n",
      "parsed batch 2\n",
      "parsed batch 3\n",
      "parsed batch 4\n",
      "parsed batch 5\n",
      "parsed batch 6\n",
      "parsed batch 7\n",
      "parsed batch 8\n",
      "parsed batch 9\n",
      "parsed batch 10\n",
      "parsed batch 11\n",
      "parsed batch 12\n",
      "parsed batch 13\n",
      "parsed batch 14\n",
      "parsed batch 15\n"
     ]
    }
   ],
   "source": [
    "vid_batch_test, sent_batch_test, intencode_batch_test, cap_len_batch_test, n_batches_test, id_batch_test = parse_data_into_batches(filename_test,\\\n",
    "                                                                                           batch_size_test,\\\n",
    "                                                                                           feat_filepath_test,\\\n",
    "                                                                                           index2token, \\\n",
    "                                                                                           tokens, \\\n",
    "                                                                                           mult_vids_test, \\\n",
    "                                                                                           all_sents_test, \\\n",
    "                                                                                           all_enc_sents_test, \\\n",
    "                                                                                           all_cap_len_test,\\\n",
    "                                                                                           all_ids_test)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_sampling(sampling_prob, cap_len_batch):\n",
    "\n",
    "        sampling = np.ones(max_caption_len, dtype = bool)\n",
    "        for l in range(max_caption_len):\n",
    "            if np.random.uniform(0,1,1) < sampling_prob:\n",
    "                sampling[l] = True\n",
    "            else:\n",
    "                sampling[l] = False\n",
    "         \n",
    "        sampling[0] = True\n",
    "        return sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sigmoid(num_epo):\n",
    "\n",
    "    # 0.88 to 0.12 (-2.0 to 2.0)\n",
    "    x = np.arange(-2.0, 2.0, (4.0/num_epo))\n",
    "    y = 1/(1 + np.e**x)\n",
    "    #y = np.ones(num_epo)\n",
    "    #print(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_print(pred, cap_len, label, idx2word, batch_size, id_batch):\n",
    "    \n",
    "    print_this = np.random.randint(batch_size,size=(1, 10))\n",
    "    seq=[]\n",
    "    for i in range(0,batch_size):\n",
    "        eos_pred = max_caption_len - 1\n",
    "        eos = cap_len[i] - 1\n",
    "        for j in range(0, max_caption_len):\n",
    "                if pred[i][j] == special_tokens['<EOS>']:\n",
    "                    eos_pred = j\n",
    "                    break\n",
    "        myid = id_batch[i]\n",
    "        pre = list( map (lambda x: idx2word[x] , pred[i][0:eos_pred])  )\n",
    "        lab = list( map (lambda x: idx2word[x] , label[i][0:eos])  )\n",
    "        \n",
    "        pre_no_eos = list( map (lambda x: idx2word[x] , pred[i][0:(eos_pred)])  )\n",
    "        sen = ' '.join([w for w in pre_no_eos])\n",
    "        seq.append(sen)\n",
    "        if i in print_this:      \n",
    "            print('\\nid: ' + str(myid) + '\\nanswer: ' + str(lab) + '\\nprediction: ' + str(pre))\n",
    "            \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-37b144c7fd88>:42: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-3-37b144c7fd88>:104: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from tqdm import tqdm\n",
    "val_graph = tf.Graph()\n",
    "\n",
    "gpu_config = tf.ConfigProto()\n",
    "gpu_config.gpu_options.allow_growth = True\n",
    "\n",
    "vocab_num = n_words\n",
    "num_epochs = 10\n",
    "num_display_steps = 15\n",
    "num_saver_epochs = 3\n",
    "output_filename = 'output.txt'\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "with val_graph.as_default():\n",
    "    feat_val = tf.placeholder(tf.float32, [None, n_frames, n_inputs], name='video_features')\n",
    "    captions_val = tf.placeholder(tf.int32, [None, max_caption_len], name='captions')\n",
    "    cap_len_val = tf.placeholder(tf.int32, [None], name='cap_len')\n",
    "\n",
    "    model_val = S2VT(vocab_num=vocab_num, lr=learning_rate)\n",
    "    logits_val, loss_op_val, summary_val = model_val.build_model(feat_val, \n",
    "                captions_val, cap_len_val, phase=phases['val'])\n",
    "    dec_pred_val = model_val.inference(logits_val)\n",
    "\n",
    "    val_saver = tf.train.Saver(max_to_keep=3)\n",
    "    \n",
    "val_sess = tf.Session(graph=val_graph, config=gpu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_model/trained_model.ckpt-959\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [2000,4000] rhs shape= [1200,2400]\n\t [[node save/Assign_1 (defined at /home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'save/Assign_1':\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-0a254dac7834>\", line 27, in <module>\n    val_saver = tf.train.Saver(max_to_keep=3)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n    self.build()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [2000,4000] rhs shape= [1200,2400]\n\t [[{{node save/Assign_1}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1290\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1291\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [2000,4000] rhs shape= [1200,2400]\n\t [[node save/Assign_1 (defined at /home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'save/Assign_1':\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-0a254dac7834>\", line 27, in <module>\n    val_saver = tf.train.Saver(max_to_keep=3)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n    self.build()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4ea2ad27e7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'saved_model/trained_model.ckpt-959'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mval_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0;31m# We add a more reasonable error message here to help users (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1326\u001b[0;31m           err, \"a mismatch between the current graph and the graph\")\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [2000,4000] rhs shape= [1200,2400]\n\t [[node save/Assign_1 (defined at /home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'save/Assign_1':\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-0a254dac7834>\", line 27, in <module>\n    val_saver = tf.train.Saver(max_to_keep=3)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n    self.build()\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = 'saved_model/trained_model.ckpt-959'\n",
    "\n",
    "val_saver.restore(val_sess, ckpt_path)\n",
    "val_sess.run(init)\n",
    "\n",
    "\n",
    "epo_loss_val = 0\n",
    "txt = open(output_filename, 'w')\n",
    "\n",
    "total_loss_val = 0\n",
    "\n",
    "for j in range(0,n_batches_test):\n",
    "    data_batch_val = np.array(vid_batch_test[j])\n",
    "    label_batch_val = np.array(intencode_batch_test[j])\n",
    "    id_batch_val = id_batch_test[j]\n",
    "    caption_lens_batch_val = np.array(cap_len_batch_test[j])\n",
    "\n",
    "    loss_val, p_val, summ = val_sess.run([loss_op_val, dec_pred_val, summary_val], \n",
    "                                feed_dict={feat_val: data_batch_val,\n",
    "                                           captions_val: label_batch_val,\n",
    "                                           cap_len_val: caption_lens_batch_val})\n",
    "    \n",
    "    print(loss_val)\n",
    "\n",
    "    seq_val = pred_print(p_val, caption_lens_batch_val, label_batch_val, index2token, batch_size_test, id_batch_val)\n",
    "    total_loss_val += loss_val\n",
    "\n",
    "    for k in range(0, batch_size_test):\n",
    "            txt.write(id_batch_val[k] + \",\" + seq_val[k] + \"\\n\")\n",
    "\n",
    "print('\\nSave file: ' + output_filename)\n",
    "txt.close()\n",
    "from subprocess import call\n",
    "\n",
    "call(['python3', 'MLDS_hw2_1_data/bleu_eval.py', output_filename])\n",
    "\n",
    "print(\"Validation: \" + str((j+1) * batch_size_test) + \"/\" + \\\n",
    "        str(n_batches_test) + \", done...\" \\\n",
    "        + \"Total Loss: \" + \"{:.4f}\".format(total_loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = pred_print(p, caption_lens_batch, label_batch, index2token, batch_size, id_batch_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(batch_size,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Video_Caption_Generator(dim_image=n_features, \n",
    "#                                 n_words = n_words, \n",
    "#                                 dim_hidden = n_hidden, \n",
    "#                                 batch_size=batch_size, \n",
    "#                                 n_lstm_steps=80,\n",
    "#                                 n_video_lstm_step=80,\n",
    "#                                 n_caption_lstm_step=80,\n",
    "#                                 bias_init_vector=bias_init_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_loss, tf_video, tf_video_mask, tf_caption, tf_caption_mask, tf_probs = model.build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_words = n_words\n",
    "\n",
    "# with tf.Graph().as_default() as graph:\n",
    "    \n",
    "\n",
    "#     weights_enc = tf.Variable(tf.random_uniform([n_features, n_hidden],-0.1,0.1),name=\"weights_enc\")\n",
    "#     bias_enc = tf.Variable(tf.zeros([n_hidden]),name=\"bias_enc\")\n",
    "\n",
    "#     weights_dec = tf.Variable(tf.random_uniform([n_hidden, n_words],-0.1,0.1),name=\"weights_dec\")\n",
    "#     bias_dec = tf.Variable(tf.zeros([n_words]),name=\"bias_dec\")\n",
    "\n",
    "\n",
    "#     x_video = tf.placeholder(tf.float32, (None, no_of_frames, n_features),'video_features') #inputs\n",
    "\n",
    "#     batch_size = tf.shape(x_video)[0]\n",
    "    \n",
    "#     x_video_drop = tf.nn.dropout(x_video, 0.5)\n",
    "    \n",
    "#     x_video_flat = tf.reshape(x_video_drop,[-1,n_features])\n",
    "\n",
    "#     y_label = tf.placeholder(tf.int32,(None, sizeof_sentence),'captions') #outputs\n",
    "\n",
    "\n",
    "#     #sampling = tf.placeholder(tf.bool, [sizeof_sentence], name='sampling')\n",
    "#     padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "#     loss = 0.0\n",
    "\n",
    "#     ########## DATA ###########\n",
    "#     # Example: For i = 0\n",
    "#     #batch_x = np.array(vid_batch[0])\n",
    "#     #batch_y = np.array(intencode_batch[0])\n",
    "#     ###########################\n",
    "\n",
    "#     input_embedding = tf.matmul(x_video_flat,weights_enc) + bias_enc\n",
    "#     input_embedding = tf.reshape(input_embedding,[-1, no_of_frames,n_hidden])\n",
    "#     input_embed = tf.transpose(input_embedding, perm=[1, 0, 2])\n",
    "\n",
    "#     with tf.device(\"/cpu:0\"):\n",
    "#         output_embedding = tf.Variable(tf.random_uniform((n_words, n_hidden),-0.1,0.1), name='dec_embedding')\n",
    "#     # output_embed = tf.nn.embedding_lookup(output_embedding,y_label)\n",
    "    \n",
    "#     ## ENCODING #################################\n",
    "    \n",
    "#     with tf.variable_scope(\"LSTM1\"):\n",
    "#         lstm1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden,state_is_tuple=True)\n",
    "#         lstm1 = tf.contrib.rnn.DropoutWrapper(lstm1, output_keep_prob=0.5)    \n",
    "\n",
    "#     with tf.variable_scope(\"LSTM2\"):\n",
    "#         lstm2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "#         lstm2 = tf.contrib.rnn.DropoutWrapper(lstm2, output_keep_prob=0.5)    \n",
    "\n",
    "\n",
    "#     state1 = lstm1.zero_state(batch_size, dtype=tf.float32)\n",
    "#     state2 = lstm2.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "#     for i in range(0, no_of_frames):\n",
    "        \n",
    "#         if i > 0:\n",
    "#                 tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "#         with tf.variable_scope(\"LSTM1\"):\n",
    "#             output1, state1 = lstm1(input_embed[i,:,:], state1)\n",
    "\n",
    "#         with tf.variable_scope(\"LSTM2\"):\n",
    "#             output2, state2 = lstm2(tf.concat([padding, output1], axis=1), state2)\n",
    "    \n",
    "#     ## DECODING ##################################\n",
    "    \n",
    "#     bos = tf.ones([batch_size, n_hidden])\n",
    "#     padding_in = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "#     logits = []\n",
    "#     cross_ent_list=[]\n",
    "#     max_prob_index = None\n",
    "\n",
    "\n",
    "#     for i in range(0, MAX_WORDS):\n",
    "        \n",
    "#         tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        \n",
    "#         with tf.variable_scope(\"LSTM1\"):\n",
    "#             output1, state1 = lstm1(padding_in, state1)\n",
    "            \n",
    "#         if i == 0:\n",
    "            \n",
    "#             with tf.variable_scope(\"LSTM2\"):\n",
    "#                 con = tf.concat([bos, output1], axis=1)\n",
    "#                 output2, state2 = lstm2(con, state2)\n",
    "                \n",
    "#         else:\n",
    "            \n",
    "#             with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "#                 feed_in = y_label[:,i]\n",
    "#                 #feed_in = tf.argmax()\n",
    "#                 output_embed = tf.nn.embedding_lookup(output_embedding,feed_in)\n",
    "                \n",
    "#             with tf.variable_scope(\"LSTM2\"):\n",
    "#                 con = tf.concat([output_embed, output1], axis=1)\n",
    "#                 output2, state2 = lstm2(con, state2)\n",
    "\n",
    "#         logit_words = tf.matmul(output2, weights_dec) + bias_dec\n",
    "#         logits.append(logit_words)\n",
    "\n",
    "#         word_i = y_label[:,i]\n",
    "\n",
    "#         one_hot_labels = tf.one_hot(word_i, n_words, on_value = 1, off_value = None, axis = 1) \n",
    "#         cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=one_hot_labels)\n",
    "#         cross_ent_list.append(cross_entropy)\n",
    "        \n",
    "        \n",
    "#         #current_loss = tf.reduce_sum(cross_entropy)/batch_size\n",
    "#         #loss = loss + current_loss\n",
    "\n",
    "#     cross_entropy_tensor = tf.stack(cross_ent_list, 1)\n",
    "#     loss = tf.reduce_sum(cross_entropy_tensor, axis=1)\n",
    "#     loss = tf.divide(loss, tf.cast(tf.Variable(sizeof_sentence), tf.float32))\n",
    "\n",
    "#     loss = tf.reduce_mean(loss, axis=0)\n",
    "    \n",
    "#     summary = tf.summary.scalar('training_loss', loss)\n",
    "\n",
    "#     params = tf.trainable_variables()\n",
    "#     #optimizer = tf.train.AdamOptimizer(learning_rate)#.minimize(loss_op)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     train_op = optimizer.minimize(loss)\n",
    "\n",
    "#     #train_step = optimizer.minimize(loss)\n",
    "    \n",
    "# #     gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "# #     gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "# #     train_op = optimizer.apply_gradients(zip(gradients, params))\n",
    "    \n",
    "# #     logits = tf.stack(logits, axis = 0)\n",
    "# #     logits = tf.reshape(logits, (sizeof_sentence, batch_size, n_words))\n",
    "# #     logits = tf.transpose(logits, [1, 0, 2])\n",
    "# #     preds = tf.argmax(logits,2)\n",
    "# #     correct_pred = tf.equal(tf.argmax(preds,1), tf.argmax(y_label,1))\n",
    "# #     accuracy = tf.reduce_mean(correct_pred)\n",
    "\n",
    "#     logits = tf.stack(logits,axis=0)\n",
    "#     logits = tf.transpose(logits, [1, 0, 2])\n",
    "#     output_preds = tf.argmax(logits,2)\n",
    "    \n",
    "#     #correct_pred = tf.equal(tf.argmax(output_preds, 1), tf.argmax(y_label, 1))\n",
    "#     #accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "#     saver = tf.train.Saver(max_to_keep=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "# gpu_config = tf.ConfigProto()\n",
    "\n",
    "# with tf.Session(graph=graph,config=gpu_config) as sess:\n",
    "\n",
    "#     loss_list_train = []\n",
    "#     loss_list_test = []\n",
    "#     preds_dict = {}\n",
    "\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     epochs = 10\n",
    "\n",
    "#     #training\n",
    "#     n=0\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "\n",
    "#         for i in range(n_batches):\n",
    "\n",
    "#             batch_x = np.array(vid_batch[i])\n",
    "#             #batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "#             batch_y = np.array(intencode_batch[i])\n",
    "\n",
    "#             _, batch_loss, preds = sess.run([train_op, loss, logits], feed_dict = {x_video: batch_x, y_label: batch_y})        \n",
    "\n",
    "#             loss_list_train.append(batch_loss)\n",
    "#             print(\"train: %f \" % (batch_loss))\n",
    "\n",
    "            \n",
    "#             n = n+1\n",
    "\n",
    "       \n",
    "#     #testing\n",
    "    \n",
    "#         saver.save(sess,ckpt_path, global_step=n)\n",
    "#         print('Model saved at ' + ckpt_path)\n",
    "        \n",
    "    \n",
    "#     for i in range(n_batches_test):\n",
    "\n",
    "#         batch_x_test = np.array(vid_batch_test[i])\n",
    "#         #batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "#         batch_y_test = np.array(intencode_batch_test[i])\n",
    "\n",
    "#         acc = sess.run(accuracy, feed_dict = {x_video: batch_x_test, y_label: batch_y_test})        \n",
    "#         print(\"accuracy %f\" % acc)\n",
    "\n",
    "# #         loss_list_test.append(batch_loss)\n",
    "# #         print(\"test:\", batch_loss)\n",
    "    \n",
    "# #         preds_dict[i] = batch_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"encoding\") as encoding_scope:\n",
    "#     lstm_enc = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "#     _, last_state = tf.nn.dynamic_rnn(lstm_enc, inputs=input_embed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "#     # TODO: create the decoder LSTMs, this is very similar to the above\n",
    "#     # you will need to set initial_state=last_state from the encoder\n",
    "#     lstm_dec = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "#     dec_outputs, _ = tf.nn.dynamic_rnn(lstm_dec,inputs=output_embed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #connect outputs to \n",
    "# logits = tf.contrib.layers.fully_connected(dec_outputs, num_outputs=len(index2token), activation_fn=None) \n",
    "\n",
    "# with tf.name_scope(\"optimization\"):\n",
    "#     # Loss function\n",
    "#     loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, sizeof_sentence]))\n",
    "#     # Optimizer\n",
    "#     optimizer = tf.train.RMSPropOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dec.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dec[0].get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_video.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from utilities import show_graph\n",
    "# show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def RNN(x, weights1, biases1):\n",
    "    \n",
    "#     x = tf.unstack(x,no_of_frames,1)\n",
    "    \n",
    "#     lstm_encoder = tf.keras.layers.LSTM(n_hidden, return_state=True) #reuse=tf.AUTO_REUSE)\n",
    "#     output_encoder,state_h,state_c = lstm_encoder(x) #,dtype=tf.float32)\n",
    "#     encoder_states = [state_h,state_c]\n",
    "    \n",
    "#     decoder\n",
    "    \n",
    "#     return tf.matmul(output1[-1],weights1) + bias1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(vid_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = RNN(x_video,weights1,bias1)\n",
    "# prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "# loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_label))\n",
    "\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "# train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# # Evaluate model (with test logits, for dropout to be disabled)\n",
    "# correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_label, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_y = np.array(intencode_batch[1])\n",
    "# np.shape(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_x = np.array(vid_batch[0])\n",
    "# print(np.shape(batch_x))\n",
    "# batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "# np.shape(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_dict[0]\n",
    "# def predicted_sentence(preds_dict):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_x = np.array(vid_batch[0])\n",
    "# batch_y = np.array(intencode_batch[0])\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "\n",
    "#     sess.run(train_op, feed_dict={x_video: batch_x, y_label: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# image_emb = tf.nn.xw_plus_b(x_video, weights1, bias1) \n",
    "# #image_emb = tf.reshape(image_emb, [batch_size, no_of_frames, n_hidden])\n",
    "\n",
    "# #lstm2 = tf.keras.layers.LSTMCell(n_hidden)\n",
    "\n",
    "# padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "\n",
    "# #Only read the frames\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "                \n",
    "# logit_words = tf.nn.xw_plus_b(output2, weights2, bias2)\n",
    "# cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logit_words,onehot_encoded)\n",
    "\n",
    "# loss = tf.reduce_sum(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     with sess.as_default():\n",
    "#         print(tf.nn.embedding_lookup(onehot_encoded,[1]).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Class",
   "language": "python",
   "name": "tf_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
