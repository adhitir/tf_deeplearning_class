{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.legacy_seq2seq as seq2seq\n",
    "from utilities import show_graph\n",
    "#from util import inv_sigmoid, linear_decay, dec_print_train, dec_print_val, dec_print_test\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import collections\n",
    "import json\n",
    "import string\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "n_inputs        = 4096\n",
    "n_hidden        = 600\n",
    "val_batch_size  = 100 #100\n",
    "n_frames        = 80\n",
    "max_caption_len = 50\n",
    "forget_bias_red = 1.0\n",
    "forget_bias_gre = 1.0\n",
    "dropout_prob    = 0.5\n",
    "n_attention     = n_hidden\n",
    "\n",
    "special_tokens  = {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "phases = {'train': 0, 'val': 1, 'test': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following function was taken from: https://github.com/AdrianHsu/S2VT-seq2seq-video-captioning-attention\n",
    "\n",
    "class S2VT:\n",
    "    def __init__(self, vocab_num = 0,lr = 1e-4):\n",
    "\n",
    "        self.vocab_num = vocab_num\n",
    "        self.learning_rate = lr\n",
    "\n",
    "     \n",
    "    def build_model(self, feat, captions=None, cap_len=None, sampling=None, phase=0):\n",
    "\n",
    "        weights = {\n",
    "            'W_feat': tf.Variable( tf.random_uniform([n_inputs, n_hidden], -0.1, 0.1), name='W_feat'), \n",
    "            'W_dec': tf.Variable(tf.random_uniform([n_hidden, self.vocab_num], -0.1, 0.1), name='W_dec')\n",
    "        }\n",
    "        biases = {\n",
    "            'b_feat':  tf.Variable( tf.zeros([n_hidden]), name='b_feat'),\n",
    "            'b_dec': tf.Variable(tf.zeros([self.vocab_num]), name='b_dec')\n",
    "        }   \n",
    "        embeddings = {\n",
    "         'emb': tf.Variable(tf.random_uniform([self.vocab_num, n_hidden], -0.1, 0.1), name='emb')\n",
    "        }\n",
    "\n",
    "        batch_size = tf.shape(feat)[0]\n",
    "\n",
    "        # cap_len: (250, 1) -> (250, 50)\n",
    "        cap_mask = tf.sequence_mask(cap_len, max_caption_len, dtype=tf.float32)\n",
    "     \n",
    "        if phase == phases['train']: #  add noise\n",
    "            noise = tf.random_uniform(tf.shape(feat), -0.1, 0.1, dtype=tf.float32)\n",
    "            feat = feat + noise\n",
    "\n",
    "        if phase == phases['train']:\n",
    "            feat = tf.nn.dropout(feat, dropout_prob)\n",
    "\n",
    "        feat = tf.reshape(feat, [-1, n_inputs])\n",
    "        image_emb = tf.matmul(feat, weights['W_feat']) + biases['b_feat']\n",
    "        image_emb = tf.reshape(image_emb, [-1, n_frames, n_hidden])\n",
    "        image_emb = tf.transpose(image_emb, perm=[1, 0, 2])\n",
    "        \n",
    "        with tf.variable_scope('LSTM1'):\n",
    "            lstm_red = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=forget_bias_red, state_is_tuple=True)\n",
    "            if phase == phases['train']:\n",
    "                lstm_red = tf.contrib.rnn.DropoutWrapper(lstm_red, output_keep_prob=dropout_prob)    \n",
    "        with tf.variable_scope('LSTM2'):\n",
    "            lstm_gre = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=forget_bias_gre, state_is_tuple=True)\n",
    "            if phase == phases['train']:\n",
    "                lstm_gre = tf.contrib.rnn.DropoutWrapper(lstm_gre, output_keep_prob=dropout_prob)    \n",
    "\n",
    "        state_red = lstm_red.zero_state(batch_size, dtype=tf.float32)\n",
    "        state_gre = lstm_gre.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "        padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "#         h_src = []\n",
    "        for i in range(0, n_frames):\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output_red, state_red = lstm_red(image_emb[i,:,:], state_red)\n",
    "            \n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output_gre, state_gre = lstm_gre(tf.concat([padding, output_red], axis=1), state_gre)\n",
    "#                 h_src.append(output_gre) # even though padding is augmented, output_gre/state_gre's shape not change\n",
    "\n",
    "#         h_src = tf.stack(h_src, axis = 0)\n",
    "\n",
    "        bos = tf.ones([batch_size, n_hidden])\n",
    "        padding_in = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "        logits = []\n",
    "        max_prob_index = None\n",
    "\n",
    "        \n",
    "\n",
    "        cross_ent_list = []\n",
    "        for i in range(0, max_caption_len):\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output_red, state_red = lstm_red(padding_in, state_red)\n",
    "\n",
    "            if i == 0:\n",
    "                with tf.variable_scope(\"LSTM2\"):\n",
    "                    con = tf.concat([bos, output_red], axis=1)\n",
    "                    output_gre, state_gre = lstm_gre(con, state_gre)\n",
    "            else:\n",
    "                if phase == phases['train']:\n",
    "                    if sampling[i] == True:\n",
    "                        feed_in = captions[:, i - 1]\n",
    "                    else:\n",
    "                        feed_in = tf.argmax(logit_words, 1)\n",
    "                else:\n",
    "                    feed_in = tf.argmax(logit_words, 1)\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    embed_result = tf.nn.embedding_lookup(embeddings['emb'], feed_in)\n",
    "                with tf.variable_scope(\"LSTM2\"):\n",
    "                    con = tf.concat([embed_result, output_red], axis=1)\n",
    "                    output_gre, state_gre = lstm_gre(con, state_gre)\n",
    "\n",
    "            logit_words = tf.matmul(output_gre, weights['W_dec']) + biases['b_dec']\n",
    "            logits.append(logit_words)\n",
    "\n",
    "#             if phase != phases['test']:\n",
    "            labels = captions[:, i]\n",
    "            one_hot_labels = tf.one_hot(labels, self.vocab_num, on_value = 1, off_value = None, axis = 1) \n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=one_hot_labels)\n",
    "            cross_entropy = cross_entropy * cap_mask[:, i]\n",
    "            cross_ent_list.append(cross_entropy)\n",
    "        \n",
    "        loss = 0.0\n",
    "#         if phase != phases['test']:\n",
    "        cross_entropy_tensor = tf.stack(cross_ent_list, 1)\n",
    "        loss = tf.reduce_sum(cross_entropy_tensor, axis=1)\n",
    "        loss = tf.divide(loss, tf.cast(cap_len, tf.float32))\n",
    "        loss = tf.reduce_mean(loss, axis=0)\n",
    "\n",
    "        logits = tf.stack(logits, axis = 0)\n",
    "        logits = tf.reshape(logits, (max_caption_len, batch_size, self.vocab_num))\n",
    "        logits = tf.transpose(logits, [1, 0, 2])\n",
    "        \n",
    "        summary = None\n",
    "        if phase == phases['train']:\n",
    "            summary = tf.summary.scalar('training_loss', loss)\n",
    "        elif phase == phases['val']:\n",
    "            summary = tf.summary.scalar('validation_loss', loss)\n",
    "            \n",
    "\n",
    "        return logits, loss, summary\n",
    "\n",
    "    def inference(self, logits):\n",
    "        \n",
    "        #print('using greedy search...')\n",
    "        dec_pred = tf.argmax(logits, 2)\n",
    "        return dec_pred\n",
    "\n",
    "    def optimize(self, loss_op):\n",
    "\n",
    "        params = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)#.minimize(loss_op)\n",
    "        gradients, variables = zip(*optimizer.compute_gradients(loss_op))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        train_op = optimizer.apply_gradients(zip(gradients, params))\n",
    "\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line,token='word'):\n",
    "    if token == 'word':\n",
    "        return [line.split(' ')]\n",
    "    elif token == 'char':\n",
    "        return [list(line)]\n",
    "    else:\n",
    "        print('ERROR: unknown token type '+token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokanized_sentences):\n",
    "    # Flatten a list of token lists into a list of tokens\n",
    "    tokens = [tk for line in tokanized_sentences for tk in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_into_lists(filename,batch_size,feat_filepath, index2token, tokens):\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "           \n",
    "    #sentence_set = extract_sentences(filename)\n",
    "    \n",
    "    mult_vids = []\n",
    "    all_sents = []\n",
    "    all_enc_sents = []\n",
    "    all_cap_len = []\n",
    "    all_ids = []\n",
    "    \n",
    "    for data in datastore:\n",
    "        \n",
    "        sentences = data[\"caption\"]\n",
    "        sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "        table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "        sentences = [word.translate(table) for word in sentences]\n",
    "        \n",
    "        num_sent = len(sentences)\n",
    "        \n",
    "        all_sents.extend(sentences)\n",
    "        \n",
    "        enc_sents = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            \n",
    "            #print(sentence)\n",
    "            tokenized_sentence, encoded_sentence, cap_len = num_encode(sentence,index2token,tokens)\n",
    "            #print(tokenized_sentence)\n",
    "            #print(encoded_sentence)\n",
    "            #print(cap_len)\n",
    "            #print(encoded_sentence)\n",
    "            encoded_sentence = list(encoded_sentence)\n",
    "\n",
    "            enc_sents.append(encoded_sentence)\n",
    "            all_cap_len.append(cap_len)\n",
    "            \n",
    "        all_enc_sents.extend(enc_sents)\n",
    "\n",
    "\n",
    "#         print(all_sents[0])\n",
    "#         print(all_enc_sents[0])\n",
    "#         print(np.shape(all_enc_sents))\n",
    "#         print(np.shape(all_sents))\n",
    "#         print(np.shape(all_cap_len))\n",
    "\n",
    "        #print(len(all_enc_sents))\n",
    "\n",
    "        #sentence_set[i] = sentences \n",
    "        #### Extracting all feature vectors per video\n",
    "        \n",
    "        video_id = data[\"id\"]\n",
    "        features = np.load(feat_filepath.format(video_id))\n",
    "        \n",
    "        for n in range(0,num_sent):\n",
    "            mult_vids.append(features)\n",
    "            all_ids.append(video_id)\n",
    "        \n",
    "        print(\"id: \" + str(video_id) + \" processed\")\n",
    "\n",
    "            \n",
    "    return mult_vids, all_sents, all_enc_sents, all_cap_len, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_data_into_batches(filename,batch_size,feat_filepath, index2token, tokens, mult_vids, all_sents, all_enc_sents, all_cap_len, all_ids):\n",
    "    #print(all_enc_sents[0:10])\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "           \n",
    "    vid_batch = {}\n",
    "    sent_batch = {}\n",
    "    enc_sent_batch = {}\n",
    "    cap_len_batch = {}\n",
    "    id_batch = {}\n",
    "    \n",
    "    #sentence_set = extract_sentences(filename)\n",
    "    \n",
    "#     mult_vids = []\n",
    "#     all_sents = []\n",
    "#     all_enc_sents = []\n",
    "#     all_cap_len = []\n",
    "    \n",
    "#     for data in datastore:\n",
    "        \n",
    "#         sentences = data[\"caption\"]\n",
    "#         sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "#         table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "#         sentences = [word.translate(table) for word in sentences]\n",
    "        \n",
    "#         num_sent = len(sentences)\n",
    "        \n",
    "#         all_sents.extend(sentences)\n",
    "        \n",
    "#         for sentence in sentences:\n",
    "#             tokenized_sentence, encoded_sentence, cap_len = num_encode(sentence,index2token,tokens)\n",
    "            \n",
    "#             #print(encoded_sentence)\n",
    "#             all_enc_sents.append(encoded_sentence)\n",
    "#             all_cap_len.append(cap_len)\n",
    "\n",
    "#         #print(all_enc_sents[0])\n",
    "#         #print(len(all_enc_sents))\n",
    "\n",
    "#         #sentence_set[i] = sentences \n",
    "#         #### Extracting all feature vectors per video\n",
    "        \n",
    "#         video_id = data[\"id\"]\n",
    "#         features = np.load(feat_filepath.format(video_id))\n",
    "        \n",
    "#         for n in range(0,num_sent):\n",
    "#             mult_vids.append(features)\n",
    "            \n",
    "#         #print(\"id: \" + str(data[\"id\"]) + \" processed\")\n",
    "\n",
    "\n",
    "    random.Random(30).shuffle(mult_vids)\n",
    "    random.Random(30).shuffle(all_sents)\n",
    "    random.Random(30).shuffle(all_enc_sents)\n",
    "    random.Random(30).shuffle(all_cap_len)\n",
    "    random.Random(30).shuffle(all_ids)\n",
    "    \n",
    "    #print(all_enc_sents[0:5])\n",
    "    #print(all_sents[0:5])\n",
    "    \n",
    "    batches = len(mult_vids)/batch_size\n",
    "    batches = int(batches)\n",
    "        \n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    for n in range(0,batches):\n",
    "        if j not in vid_batch:\n",
    "            vid_batch[j] = []\n",
    "            sent_batch[j] = []\n",
    "            enc_sent_batch[j]=[]\n",
    "            cap_len_batch[j] = []\n",
    "\n",
    "            \n",
    "        vid_batch[j] = mult_vids[i:i+batch_size]\n",
    "        sent_batch[j] = all_sents[i:i+batch_size]\n",
    "        enc_sent_batch[j] = all_enc_sents[i:i+batch_size]\n",
    "        cap_len_batch[j] = all_cap_len[i:i+batch_size]\n",
    "        id_batch[j] = all_ids[i:i+batch_size]\n",
    "\n",
    "        print('parsed batch %d' %j)\n",
    "        i = i+batch_size\n",
    "        j = j+1\n",
    "    \n",
    "#     if j not in vid_batch:\n",
    "#         vid_batch[j] = []\n",
    "\n",
    "#     vid_batch[j].append(features)\n",
    "                \n",
    "#     i = i+1\n",
    "\n",
    "#     if i%batch_size == 0:\n",
    "#         j = j+1 \n",
    "    #print(sent_batch[0])\n",
    "    #print(enc_sent_batch[0])\n",
    "            \n",
    "    return vid_batch, sent_batch, enc_sent_batch, cap_len_batch, batches, id_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename):\n",
    "    \n",
    "    sentence_set = {}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "        \n",
    "    i = 0\n",
    "    for data in datastore:\n",
    "        \n",
    "        #### Extracting only a single sentence per video into a standalone dict\n",
    "\n",
    "        sentences = data[\"caption\"]\n",
    "        sentences = [word.lower() for word in sentences] #Normalize the case\n",
    "        table = str.maketrans('', '', string.punctuation) #Normalize the punctuation\n",
    "        sentences = [word.translate(table) for word in sentences]\n",
    "\n",
    "        sentence_set[i] = sentences #0 for only the first sentence\\\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "    return sentence_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping string tokens to numertical indices.\n",
    "def listVocab(sentence_set):\n",
    "    \n",
    "    PAD_token = 0\n",
    "    BOS_token = 1\n",
    "    EOS_token = 2\n",
    "    UNK_token = 3\n",
    "    \n",
    "    all_tokens = []\n",
    "    word_count = {}\n",
    "    token2index = {\"<PAD>\": 0,\"<BOS>\":1,\"<EOS>\":2,\"<UNK>\":3}\n",
    "    index2token = {PAD_token: \"<PAD>\", BOS_token: \"<BOS>\", EOS_token: \"<EOS>\", UNK_token: \"<UNK>\"}\n",
    "    \n",
    "    for set_i in sentence_set:\n",
    "        sentence_set_i = sentence_set[set_i]\n",
    "        for line in sentence_set_i:\n",
    "#             line = sentence_set[n]\n",
    "            tokenized_captions = tokenize(line) #Seperate the words\n",
    "            all_tokens += tokenized_captions\n",
    "    \n",
    "    counter = count_tokens(all_tokens) #Count the word repeatitions in each set\n",
    "    \n",
    "    counter_dict = counter.items()\n",
    "    counter_sort = sorted(counter_dict, key=lambda x:x[1],reverse=True) #sort by frequency of occurance \n",
    "    #print(counter_sort)\n",
    "\n",
    "    i = len(index2token)\n",
    "    values = [0,1,2,3]\n",
    "    tokens = [\"<PAD>\",\"<BOS>\",\"<EOS>\",\"<UNK>\"]\n",
    "    \n",
    "    for token, freq in counter_sort:\n",
    "        word_count[token] = freq\n",
    "        index2token[i] = token\n",
    "        token2index[token] = i\n",
    "        values += [i]\n",
    "        tokens += [token]\n",
    "        i+=1\n",
    "        \n",
    "    word_count['<PAD>'] = i\n",
    "    word_count['<BOS>'] = i\n",
    "    word_count['<EOS>'] = i\n",
    "    word_count['<UNK>'] = i\n",
    "    \n",
    "    bias_init_vector = np.array([1.0 * word_count[ index2token[i] ] for i in index2token])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
    "    \n",
    "    return [word_count, tokens, values, token2index, index2token, len(index2token),bias_init_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenList(nestedList,output): \n",
    "    for i in nestedList: \n",
    "        if type(i) == list: \n",
    "            flattenList(i,output) \n",
    "        else: \n",
    "            output.append(i) \n",
    "            \n",
    "    return output\n",
    "\n",
    "def num_encode(test_sentence,index2token,tokens,tokenized_sentence=[],num_encoded_sentence=[]):\n",
    "    \n",
    "    tokenized_sentence.clear()\n",
    "    num_encoded_sentence.clear()\n",
    "    \n",
    "    tokenized_sentence = [\"<BOS>\"] + tokenize(test_sentence) + [\"<EOS>\"]\n",
    "    #print(tokenized_sentence)\n",
    "    output=[]\n",
    "    tokenized_sentence = flattenList(tokenized_sentence,output)\n",
    "    \n",
    "    cap_len = len(tokenized_sentence)\n",
    "    \n",
    "    while len(tokenized_sentence) < MAX_WORDS:\n",
    "        tokenized_sentence.append(\"<PAD>\")    \n",
    "    \n",
    "    #print(len(tokenized_sentence))\n",
    "    \n",
    "    for ind, token in enumerate(tokenized_sentence):\n",
    "        if token in tokens:\n",
    "            for i in range(0,len(index2token)):\n",
    "                if token == index2token[i]: \n",
    "                    num_encoded_sentence.append(i) \n",
    "                    \n",
    "            #print(\"token exists\")\n",
    "        else:\n",
    "            num_encoded_sentence.append(3)\n",
    "            tokenized_sentence[ind] = tokens[3]\n",
    "            #print(\"token unknown\")\n",
    "            \n",
    "            \n",
    "                \n",
    "    #print(len(num_encoded_sentence))\n",
    "\n",
    "        \n",
    "    return tokenized_sentence, num_encoded_sentence, cap_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def parse_sentence_data_into_batches(sentence_set, index2token,tokens,batch_size):\n",
    "\n",
    "#     tokenizedsentence_batch = {}\n",
    "#     intencode_batch = {}\n",
    "#     cap_len_batch = {}\n",
    "\n",
    "#     ii = 0\n",
    "#     jj = 0  \n",
    "\n",
    "#     for n in sentence_set:\n",
    "#         sentence = sentence_set[n]\n",
    "\n",
    "#         tokenized_sentence,encoded_sentence, cap_len = num_encode(sentence,index2token,tokens)\n",
    "        \n",
    "#         #print(np.shape(encoded_sentence))\n",
    "\n",
    "#         tokenized_sentence = list(tokenized_sentence)\n",
    "#         encoded_sentence = list(encoded_sentence)\n",
    "\n",
    "#         if jj not in intencode_batch:\n",
    "#             #onehot_batch[jj] = []\n",
    "#             intencode_batch[jj] = []\n",
    "#             tokenizedsentence_batch[jj] = []\n",
    "#             cap_len_batch[jj] = []\n",
    "\n",
    "#         #print(np.shape(onehot_encoded_sentence))    \n",
    "#         #onehot_batch[jj].append(onehot_encoded_sentence)\n",
    "#         intencode_batch[jj].append(encoded_sentence)\n",
    "#         tokenizedsentence_batch[jj].append(tokenized_sentence)\n",
    "#         cap_len_batch[jj].append(cap_len)\n",
    "\n",
    "#         ii = ii+1\n",
    "\n",
    "#         if ii%batch_size == 0:\n",
    "#             jj = jj+1\n",
    "            \n",
    "        \n",
    "#     return tokenizedsentence_batch, intencode_batch, cap_len_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(filename_train, 'r') as f:\n",
    "#     datastore = json.load(f)\n",
    "    \n",
    "# for data in datastore:\n",
    "#     print(data[\"id\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6061 unique words in the captions dataset\n"
     ]
    }
   ],
   "source": [
    "filename_train = 'MLDS_hw2_1_data/training_label.json'\n",
    "feat_filepath_train = \"MLDS_hw2_1_data/training_data/feat/{}.npy\"\n",
    "\n",
    "ckpt_path = 'saved_model/trained_model.ckpt'\n",
    "\n",
    "# forget_bias_red = 1.0\n",
    "# forget_bias_gre = 1.0\n",
    "# dropout_prob    = 0.5\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "MAX_WORDS = max_caption_len #max number of words in a caption\n",
    "n_features = n_inputs\n",
    "no_of_frames = n_frames\n",
    "sizeof_sentence= MAX_WORDS\n",
    "learning_rate = 0.0001\n",
    "n_hidden = n_hidden\n",
    "\n",
    "#### PARSE TRAINING DATA #####\n",
    "\n",
    "# Extracting captions for each video\n",
    "sentence_set = extract_sentences(filename_train)\n",
    "\n",
    "word_count, tokens, values, token2index, index2token, n_words, bias_init_vector = listVocab(sentence_set)\n",
    "print(\"There are %d unique words in the captions dataset\" % n_words)\n",
    "\n",
    "# mult_vids, all_sents, all_enc_sents, all_cap_len, all_ids = parse_data_into_lists(filename_train,\\\n",
    "#                                                                          batch_size,feat_filepath_train,\\\n",
    "#                                                                          index2token, tokens)\n",
    "\n",
    "# vid_batch, sent_batch, intencode_batch, cap_len_batch, n_batches, id_batch = parse_data_into_batches(filename_train,\\\n",
    "#                                                                                            batch_size,\\\n",
    "#                                                                                            feat_filepath_train,\\\n",
    "#                                                                                            index2token, \\\n",
    "#                                                                                            tokens, \\\n",
    "#                                                                                            mult_vids, \\\n",
    "#                                                                                            all_sents, \\\n",
    "#                                                                                            all_enc_sents, \\\n",
    "#                                                                                            all_cap_len,\\\n",
    "#                                                                                           all_ids)\n",
    "\n",
    "# #Parse Training Data into batches\n",
    "# # vid_batch, sent_batch, intencode_batch, cap_len_batch, n_batches = parse_data_into_batches(filename_train,batch_size,feat_filepath_train,index2token, tokens)\n",
    "\n",
    "# print(\"The number of videos in the training set are %d and each video has 80 frames with 4096 features/units each\" % (n_batches*batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: ScdUht-pM6s_53_63.avi processed\n",
      "id: wkgGxsuNVSg_34_41.avi processed\n",
      "id: BtQtRGI0F2Q_15_20.avi processed\n",
      "id: k06Ge9ANKM8_5_16.avi processed\n",
      "id: sZf3VDsdDPM_107_114.avi processed\n",
      "id: shPymuahrsc_5_12.avi processed\n",
      "id: XOAgUVVwKEA_8_20.avi processed\n",
      "id: ufFT2BWh3BQ_0_8.avi processed\n",
      "id: 5YJaS2Eswg0_22_26.avi processed\n",
      "id: lw7pTwpx0K0_38_48.avi processed\n",
      "id: UbmZAe5u5FI_132_141.avi processed\n",
      "id: xCFCXzDUGjY_5_9.avi processed\n",
      "id: He7Ge7Sogrk_47_70.avi processed\n",
      "id: tJHUH9tpqPg_113_118.avi processed\n",
      "id: n016q1w8Q30_2_11.avi processed\n",
      "id: RjpbFlOHFps_8_25.avi processed\n",
      "id: 6JnGBs88sL0_4_10.avi processed\n",
      "id: EpMuCrbxE8A_107_115.avi processed\n",
      "id: HAjwXjwN9-A_16_24.avi processed\n",
      "id: 4xVGpDmA4lE_23_33.avi processed\n",
      "id: k5OKBX2e7xA_19_32.avi processed\n",
      "id: Jag7oTemldY_12_25.avi processed\n",
      "id: 8MVo7fje_oE_125_130.avi processed\n",
      "id: bqMmyY1ImkI_0_14.avi processed\n",
      "id: jTnrm338_KY_34_42.avi processed\n",
      "id: UdcObAQ5OOM_15_30.avi processed\n",
      "id: 4PcL6-mjRNk_11_18.avi processed\n",
      "id: 3qqEKTPxLNs_1_15.avi processed\n",
      "id: glrijRGnmc0_211_215.avi processed\n",
      "id: q7pOFn8s4zc_263_273.avi processed\n",
      "id: mtrCf667KDk_134_176.avi processed\n",
      "id: 0lh_UWF9ZP4_62_69.avi processed\n",
      "id: JntMAcTlOF0_50_70.avi processed\n",
      "id: 7NNg0_n-bS8_21_30.avi processed\n",
      "id: IhwPQL9dFYc_124_129.avi processed\n",
      "id: BAf3LXFUaGs_28_38.avi processed\n",
      "id: 6q1dX6thX3E_286_295.avi processed\n",
      "id: RZL9irxnhZ0_34_40.avi processed\n",
      "id: WWf0Z6ak3Dg_5_15.avi processed\n",
      "id: PeUHy0A1GF0_114_121.avi processed\n",
      "id: klteYv1Uv9A_27_33.avi processed\n",
      "id: e-j59PqJjSM_405_416.avi processed\n",
      "id: 778mkceE0UQ_40_46.avi processed\n",
      "id: 77iDIp40m9E_126_131.avi processed\n",
      "id: e-j59PqJjSM_50_98.avi processed\n",
      "id: Dgf0VHMEtNs_57_66.avi processed\n",
      "id: f9Won2JpOEU_60_80.avi processed\n",
      "id: dfOuTx66bJU_34_39.avi processed\n",
      "id: 04Gt01vatkk_248_265.avi processed\n",
      "id: rl1rVk_xIOs_1_16.avi processed\n",
      "id: v7iIZXtpIb8_5_15.avi processed\n",
      "id: DhwrBs96Kgk_120_124.avi processed\n",
      "id: qLwgb3F0aPU_298_305.avi processed\n",
      "id: qeKX-N1nKiM_0_5.avi processed\n",
      "id: 1Sp2__RCT0c_11_15.avi processed\n",
      "id: Fe4tO5vW9_E_64_70.avi processed\n",
      "id: mmSQTI6gMNQ_120_128.avi processed\n",
      "id: HV12kTtdTT4_5_14.avi processed\n",
      "id: 0lh_UWF9ZP4_27_31.avi processed\n",
      "id: Je3V7U5Ctj4_569_576.avi processed\n",
      "id: 30GeJHYoerk_121_126.avi processed\n",
      "id: 04Gt01vatkk_308_321.avi processed\n",
      "id: zulPFoY64wE_26_33.avi processed\n",
      "id: MrQd1zUVRUM_103_110.avi processed\n",
      "id: xxHx6s_DbUo_216_222.avi processed\n",
      "id: 71soiLO6I9U_15_24.avi processed\n",
      "id: UXs3eq68ZjE_250_255.avi processed\n",
      "id: jbzaMtPYtl8_48_58.avi processed\n",
      "id: 8HB7ywgJuTg_131_142.avi processed\n",
      "id: Cjf21Y19aUQ_82_86.avi processed\n",
      "id: qvg9eM4Hmzk_4_10.avi processed\n",
      "id: 5HAf_INrFy0_3_25.avi processed\n",
      "id: YmXCfQm0_CA_277_284.avi processed\n",
      "id: 88DOMJ11q2M_84_87.avi processed\n",
      "id: NUYu9c9XsgY_7_21.avi processed\n",
      "id: N3A7944_UJw_63_70.avi processed\n",
      "id: uJPupV4oLZ0_4_12.avi processed\n",
      "id: cnsjm3fNEec_4_10.avi processed\n",
      "id: J_evFB7RIKA_104_120.avi processed\n",
      "id: g1Gldu1KS44_8_14.avi processed\n",
      "id: s1ZABV7AQdA_38_48.avi processed\n",
      "id: tcxhOGyrCtI_15_21.avi processed\n",
      "id: inzk2fTUe1w_1_15.avi processed\n",
      "id: j2Dhf-xFUxU_13_20.avi processed\n",
      "id: MTjrZthHwJQ_2_11.avi processed\n",
      "id: J---aiyznGQ_0_6.avi processed\n",
      "id: ZbtpcGi2DWY_161_170.avi processed\n",
      "id: RSx5G0_xH48_12_17.avi processed\n",
      "id: ecm9gf2Pgkc_1_24.avi processed\n",
      "id: pW9DFPqoIsI_26_50.avi processed\n",
      "id: N2Cm0SLr0ZE_18_29.avi processed\n",
      "id: sJSmRik2c-c_1_7.avi processed\n",
      "id: zv2RIbUsnSw_335_341.avi processed\n",
      "id: aM-RcQj0a7I_37_55.avi processed\n",
      "id: TZ860P4iTaM_15_28.avi processed\n",
      "id: lo4KcsBN--A_0_10.avi processed\n",
      "id: u4T76jsPin0_0_11.avi processed\n",
      "id: 7HcYJKMxpcg_20_28.avi processed\n",
      "id: CGllPWAwmUo_1_15.avi processed\n",
      "id: WTf5EgVY5uU_124_128.avi processed\n"
     ]
    }
   ],
   "source": [
    "# #### PARSE TESTING DATA #####\n",
    "\n",
    "filename_test = 'MLDS_hw2_1_data/testing_label.json'\n",
    "feat_filepath_test = \"MLDS_hw2_1_data/testing_data/feat/{}.npy\"\n",
    "batch_size_test = 100\n",
    "\n",
    "\n",
    "# #Parse Testing Data into batches\n",
    "# vid_batch_test, n_batches_test = parse_vid_data_into_batches(filename_test,batch_size,feat_filepath_test)\n",
    "# print(\"The number of videos in the test set are %d and each video has 80 frames with 4096 features/units each\" % (n_batches_test*batch_size))\n",
    "\n",
    "# # Extracting captions for each video\n",
    "# sentence_set_test = extract_sentences(filename_test,feat_filepath_test)\n",
    "# tokenizedsentence_batch_test, intencode_batch_test, cap_len_batch_test = parse_sentence_data_into_batches(sentence_set_test,index2token,tokens,batch_size)\n",
    "\n",
    "#### PARSE TRAINING DATA #####\n",
    "\n",
    "# Extracting captions for each video\n",
    "sentence_set = extract_sentences(filename_test)\n",
    "\n",
    "mult_vids_test, all_sents_test, all_enc_sents_test, all_cap_len_test, all_ids_test = parse_data_into_lists(filename_test,\\\n",
    "                                                                                  batch_size_test,\\\n",
    "                                                                                  feat_filepath_test,\\\n",
    "                                                                                  index2token,\\\n",
    "                                                                                  tokens)\n",
    "\n",
    "# print(all_sents[0:3])\n",
    "# print(all_cap_len[0:3])\n",
    "# print(all_ids[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 1594, 1093, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 4, 7, 5, 163, 14, 22, 147, 9, 6, 121, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 4, 90, 5, 42, 68, 26, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 4, 5987, 709, 48, 4915, 6, 2037, 768, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 4, 28, 5, 43, 422, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "['a missile explodes', 'a man is jumping with his hands on the ground', 'a monkey is walking through water', 'a bomb went off beyond the palm trees', 'a girl is eating noodles']\n",
      "parsed batch 0\n",
      "parsed batch 1\n",
      "parsed batch 2\n",
      "parsed batch 3\n",
      "parsed batch 4\n",
      "parsed batch 5\n",
      "parsed batch 6\n",
      "parsed batch 7\n",
      "parsed batch 8\n",
      "parsed batch 9\n",
      "parsed batch 10\n",
      "parsed batch 11\n",
      "parsed batch 12\n",
      "parsed batch 13\n",
      "parsed batch 14\n",
      "parsed batch 15\n"
     ]
    }
   ],
   "source": [
    "vid_batch_test, sent_batch_test, intencode_batch_test, cap_len_batch_test, n_batches_test, id_batch_test = parse_data_into_batches(filename_test,\\\n",
    "                                                                                           batch_size_test,\\\n",
    "                                                                                           feat_filepath_test,\\\n",
    "                                                                                           index2token, \\\n",
    "                                                                                           tokens, \\\n",
    "                                                                                           mult_vids_test, \\\n",
    "                                                                                           all_sents_test, \\\n",
    "                                                                                           all_enc_sents_test, \\\n",
    "                                                                                           all_cap_len_test,\\\n",
    "                                                                                           all_ids_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_sampling(sampling_prob, cap_len_batch):\n",
    "\n",
    "        sampling = np.ones(max_caption_len, dtype = bool)\n",
    "        for l in range(max_caption_len):\n",
    "            if np.random.uniform(0,1,1) < sampling_prob:\n",
    "                sampling[l] = True\n",
    "            else:\n",
    "                sampling[l] = False\n",
    "         \n",
    "        sampling[0] = True\n",
    "        return sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sigmoid(num_epo):\n",
    "\n",
    "    # 0.88 to 0.12 (-2.0 to 2.0)\n",
    "    x = np.arange(-2.0, 2.0, (4.0/num_epo))\n",
    "    y = 1/(1 + np.e**x)\n",
    "    #y = np.ones(num_epo)\n",
    "    #print(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_print(pred, cap_len, label, idx2word, batch_size, id_batch):\n",
    "    \n",
    "    print_this = np.random.randint(batch_size,size=(1, 10))\n",
    "    seq=[]\n",
    "    for i in range(0,batch_size):\n",
    "        eos_pred = max_caption_len - 1\n",
    "        eos = cap_len[i] - 1\n",
    "        for j in range(0, max_caption_len):\n",
    "                if pred[i][j] == special_tokens['<EOS>']:\n",
    "                    eos_pred = j\n",
    "                    break\n",
    "        myid = id_batch[i]\n",
    "        pre = list( map (lambda x: idx2word[x] , pred[i][0:eos_pred])  )\n",
    "        lab = list( map (lambda x: idx2word[x] , label[i][0:eos])  )\n",
    "        \n",
    "        pre_no_eos = list( map (lambda x: idx2word[x] , pred[i][0:(eos_pred)])  )\n",
    "        sen = ' '.join([w for w in pre_no_eos])\n",
    "        seq.append(sen)\n",
    "        if i in print_this:      \n",
    "            print('\\nid: ' + str(myid) + '\\nanswer: ' + str(lab) + '\\nprediction: ' + str(pre))\n",
    "            \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_prob = inv_sigmoid(num_epochs)\n",
    "# samp_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp = schedule_sampling(samp_prob[epo], caption_lens_batch)\n",
    "# samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_graph: start\n",
      "WARNING:tensorflow:From <ipython-input-3-37b144c7fd88>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-3-37b144c7fd88>:42: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-3-37b144c7fd88>:104: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/adhitir/.conda/envs/tf_class/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from tqdm import tqdm\n",
    "train_graph = tf.Graph()\n",
    "val_graph = tf.Graph()\n",
    "\n",
    "gpu_config = tf.ConfigProto()\n",
    "gpu_config.gpu_options.allow_growth = True\n",
    "\n",
    "print('train_graph: start')\n",
    "\n",
    "vocab_num = n_words\n",
    "num_epochs = 10\n",
    "num_display_steps = 15\n",
    "num_saver_epochs = 3\n",
    "output_filename = 'output.txt'\n",
    "\n",
    "with train_graph.as_default():\n",
    "    feat = tf.placeholder(tf.float32, [None, n_frames, n_inputs], name='video_features')\n",
    "    captions = tf.placeholder(tf.int32, [None, max_caption_len], name='captions')\n",
    "    sampling = tf.placeholder(tf.bool, [max_caption_len], name='sampling')\n",
    "    cap_len = tf.placeholder(tf.int32, [None], name='cap_len')\n",
    "    model = S2VT(vocab_num=vocab_num, lr=learning_rate)\n",
    "    logits, loss_op, summary = model.build_model(feat, captions, cap_len, sampling, phases['train'])\n",
    "    dec_pred = model.inference(logits)\n",
    "    train_op = model.optimize(loss_op)\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "train_sess = tf.Session(graph=train_graph, config=gpu_config)\n",
    "\n",
    "\n",
    "with val_graph.as_default():\n",
    "    feat_val = tf.placeholder(tf.float32, [None, n_frames, n_inputs], name='video_features')\n",
    "    captions_val = tf.placeholder(tf.int32, [None, max_caption_len], name='captions')\n",
    "    cap_len_val = tf.placeholder(tf.int32, [None], name='cap_len')\n",
    "\n",
    "    model_val = S2VT(vocab_num=vocab_num, lr=learning_rate)\n",
    "    logits_val, loss_op_val, summary_val = model_val.build_model(feat_val, \n",
    "                captions_val, cap_len_val, phase=phases['val'])\n",
    "    dec_pred_val = model_val.inference(logits_val)\n",
    "\n",
    "    val_saver = tf.train.Saver(max_to_keep=3)\n",
    "    \n",
    "val_sess = tf.Session(graph=val_graph, config=gpu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#train_sess.run(init)\n",
    "\n",
    "# samp_prob = inv_sigmoid(num_epochs)\n",
    "# pbar = tqdm(range(0, num_epochs))\n",
    "\n",
    "# loss_list = []\n",
    "\n",
    "# for epo in pbar:\n",
    "#     num_steps = n_batches\n",
    "#     epo_loss = 0\n",
    "#     for i in range(0, num_steps):\n",
    "#         data_batch = np.array(vid_batch[i])\n",
    "#         label_batch = np.array(intencode_batch[i])\n",
    "#         id_batch_train = id_batch[i]\n",
    "        \n",
    "#         caption_lens_batch = np.array(cap_len_batch[i])\n",
    "        \n",
    "#         #data_batch, label_batch, caption_lens_batch, id_batch = datasetTrain.next_batch()\n",
    "        \n",
    "#         samp = schedule_sampling(samp_prob[epo], caption_lens_batch)\n",
    "        \n",
    "#         if i % num_display_steps == 1:\n",
    "#             # training \n",
    "#             run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "#             _, loss, p, summ = train_sess.run([train_op, loss_op, dec_pred, summary], \n",
    "#                             feed_dict={feat: data_batch,\n",
    "#                                        captions: label_batch,\n",
    "#                                        cap_len: caption_lens_batch,\n",
    "#                                        sampling: samp},\n",
    "#                             options=run_options)\n",
    "            \n",
    "#             #print(p)\n",
    "            \n",
    "#             loss_list.append(loss)\n",
    "            \n",
    "#             #summary_writer.add_summary(summ, global_step=(epo * num_steps) + i)\n",
    "#             print(\"\\n[Train. Prediction] Epoch \" + str(epo) + \", step \" + str(i) + \"/\" + str(num_steps) + \"......\",)\n",
    "            \n",
    "#             seq = pred_print(p, caption_lens_batch, label_batch, index2token, batch_size, id_batch_train)\n",
    "\n",
    "#         else:\n",
    "#             _, loss, p = train_sess.run([train_op, loss_op, dec_pred], \n",
    "#                             feed_dict={feat: data_batch,\n",
    "#                                        captions: label_batch,\n",
    "#                                        cap_len: caption_lens_batch,\n",
    "#                                        sampling: samp})\n",
    "            \n",
    "#             loss_list.append(loss)\n",
    "\n",
    "\n",
    "#         epo_loss += loss\n",
    "    \n",
    "#         pbar.set_description(\"Epoch \" + str(epo) + \", step \" + str(i) + \"/\" + str(num_steps) + \\\n",
    "#             \", (Training Loss: \" + \"{:.4f}\".format(loss) + \\\n",
    "#             \", samp_prob: \" + \"{:.4f}\".format(samp_prob[epo]) + \")\" )\n",
    "\n",
    "#     print(\"\\n[FINISHED] Epoch \" + str(epo) + \", (Training Loss (per epoch): \" + \"{:.4f}\".format(epo_loss) + \" samp_prob: \" + \"{:.4f}\".format(samp_prob[epo]) + \")\")\n",
    "\n",
    "\n",
    "# ckpt_path = saver.save(train_sess,ckpt_path, global_step=(epo * num_steps) + num_steps - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = saver.save(train_sess,ckpt_path, global_step=(epo * num_steps) + num_steps - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax2 = fig, ax = plt.subplots()\n",
    "\n",
    "# ax2.plot(loss_list,label='loss')\n",
    "\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylim((0,9))\n",
    "\n",
    "# leg2 = ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(intencode_batch_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(sent_batch_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saver path: saved_model/\n",
      "INFO:tensorflow:Restoring parameters from saved_model/trained_model.ckpt-959\n",
      "\n",
      "id: DhwrBs96Kgk_120_124.avi\n",
      "answer: ['<BOS>', 'a', 'monkey', 'is', 'walking', 'through', 'water']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'running', 'a', 'a']\n",
      "\n",
      "id: RjpbFlOHFps_8_25.avi\n",
      "answer: ['<BOS>', 'a', 'bomb', 'went', 'off', 'beyond', 'the', 'palm', 'trees']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'riding', 'a']\n",
      "\n",
      "id: 71soiLO6I9U_15_24.avi\n",
      "answer: ['<BOS>', 'a', 'hedgehog', 'is', 'playing']\n",
      "prediction: ['<BOS>', 'a', 'person', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: sZf3VDsdDPM_107_114.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'and', 'a', 'woman', 'are', 'sitting', 'down', 'eating', 'with', '<UNK>']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'talking', 'a']\n",
      "\n",
      "id: Dgf0VHMEtNs_57_66.avi\n",
      "answer: ['<BOS>', 'a', 'group', 'of', 'doctors', 'wearing', '<UNK>', '<UNK>', 'are', 'working', 'on', 'a', 'man']\n",
      "prediction: ['<BOS>', 'a', 'person', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: WWf0Z6ak3Dg_5_15.avi\n",
      "answer: ['<BOS>', 'a', 'dog', 'is', 'chasing', 'a', 'ball']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: PeUHy0A1GF0_114_121.avi\n",
      "answer: ['<BOS>', 'a', 'person', 'peels', 'shrimp']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'cutting', 'shrimp']\n",
      "\n",
      "id: JntMAcTlOF0_50_70.avi\n",
      "answer: ['<BOS>', 'a', 'boy', 'is', 'being', 'chased']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'in']\n",
      "\n",
      "id: HV12kTtdTT4_5_14.avi\n",
      "answer: ['<BOS>', 'a', 'cat', 'is', 'watching', 'tv']\n",
      "prediction: ['<BOS>', 'a', 'boy', 'is', 'playing', 'a']\n",
      "\n",
      "id: NUYu9c9XsgY_7_21.avi\n",
      "answer: ['<BOS>', 'the', 'person', 'is', 'typing', 'on', 'a', 'keyboard']\n",
      "prediction: ['<BOS>', 'a', 'person', 'is', 'playing']\n",
      "\n",
      "id: e-j59PqJjSM_405_416.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'mixing', 'ingredients', 'in', 'a', 'large', 'mixing', 'bowl']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'mixing', 'a', 'a', 'a']\n",
      "\n",
      "id: k5OKBX2e7xA_19_32.avi\n",
      "answer: ['<BOS>', 'a', 'stunt', 'biker', 'spins', 'around', 'on', 'one', 'wheel']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'riding', 'a']\n",
      "\n",
      "id: He7Ge7Sogrk_47_70.avi\n",
      "answer: ['<BOS>', 'an', 'elephant', 'holding', 'a', '<UNK>', 'brush', 'with', 'his', 'trunk', 'is', 'painting', 'on', 'a', 'white', 'sheet', 'of', 'paper', '<UNK>', 'on', 'an', '<UNK>', 'board']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: WTf5EgVY5uU_124_128.avi\n",
      "answer: ['<BOS>', 'a', 'woman', 'is', 'pouring', 'ingredients', 'into', 'a', 'frying', 'pan']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'cooking', 'in']\n",
      "\n",
      "id: zv2RIbUsnSw_335_341.avi\n",
      "answer: ['<BOS>', 'the', 'paramedics', 'brought', 'a', 'lady', 'into', 'the', 'hospital', 'on', 'a', 'gurney']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'the', 'a']\n",
      "\n",
      "id: 04Gt01vatkk_308_321.avi\n",
      "answer: ['<BOS>', 'a', 'woman', 'is', 'dicing', 'peppers']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: N2Cm0SLr0ZE_18_29.avi\n",
      "answer: ['<BOS>', 'the', 'boy', 'played', 'a', 'small', 'guitar']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'playing', 'a']\n",
      "\n",
      "id: 88DOMJ11q2M_84_87.avi\n",
      "answer: ['<BOS>', 'a', 'cartoon', 'woman', 'dives', 'into', 'a', 'pool']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a']\n",
      "\n",
      "id: RZL9irxnhZ0_34_40.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'talking', 'to', 'numerous', 'reporters']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a']\n",
      "\n",
      "id: J_evFB7RIKA_104_120.avi\n",
      "answer: ['<BOS>', 'the', 'man', 'is', 'slicing', 'and', 'cleaning', 'a', 'green', 'bell', 'pepper']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'slicing', 'a']\n",
      "\n",
      "id: 0lh_UWF9ZP4_62_69.avi\n",
      "answer: ['<BOS>', 'a', 'woman', 'is', 'mixing', 'ingredients', 'in', 'a', 'bowl']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'eggs', 'eggs']\n",
      "\n",
      "id: 71soiLO6I9U_15_24.avi\n",
      "answer: ['<BOS>', 'an', 'animal', 'digs', 'under', 'its', 'house']\n",
      "prediction: ['<BOS>', 'a', 'person', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: v7iIZXtpIb8_5_15.avi\n",
      "answer: ['<BOS>', 'the', 'boy', 'practiced', 'the', 'piano']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: J---aiyznGQ_0_6.avi\n",
      "answer: ['<BOS>', 'a', 'cat', 'is', 'playing', 'a', 'piano']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'a']\n",
      "\n",
      "id: Je3V7U5Ctj4_569_576.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'sprinkling', 'something', 'on', 'food']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'putting', 'a', 'a', 'a']\n",
      "\n",
      "id: MrQd1zUVRUM_103_110.avi\n",
      "answer: ['<BOS>', 'two', 'women', 'are', 'kissing']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: IhwPQL9dFYc_124_129.avi\n",
      "answer: ['<BOS>', 'the', 'lady', 'sliced', 'the', 'cheese']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'a']\n",
      "\n",
      "id: Je3V7U5Ctj4_569_576.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'sprinkles', 'cheese', 'on', 'a', 'soft', 'tortilla']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'putting', 'a', 'a', 'a']\n",
      "\n",
      "id: 7HcYJKMxpcg_20_28.avi\n",
      "answer: ['<BOS>', 'a', 'lion', 'is', 'walking', 'in', 'circles']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: sJSmRik2c-c_1_7.avi\n",
      "answer: ['<BOS>', 'a', 'train', 'is', 'moving', 'very', 'fast']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'riding', 'a', 'a']\n",
      "\n",
      "id: MrQd1zUVRUM_103_110.avi\n",
      "answer: ['<BOS>', 'two', 'women', 'are', 'kissing', 'each', 'others', 'lips']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: e-j59PqJjSM_405_416.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'stirring', 'a', 'bowl', 'of', 'food']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'mixing', 'a', 'a', 'a']\n",
      "\n",
      "id: Je3V7U5Ctj4_569_576.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'sprinkling', 'some', 'cheese']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'putting', 'a', 'a', 'a']\n",
      "\n",
      "id: v7iIZXtpIb8_5_15.avi\n",
      "answer: ['<BOS>', 'a', 'little', 'boy', 'is', 'playing', 'the', 'piano']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: zv2RIbUsnSw_335_341.avi\n",
      "answer: ['<BOS>', 'a', 'person', 'is', 'being', 'pushed', 'on', 'a', 'stretcher']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'the', 'a']\n",
      "\n",
      "id: s1ZABV7AQdA_38_48.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'walking', 'toward', 'a', 'crowd', 'of', 'people', 'and', 'for', 'some', '<UNK>', 'he', 'turns', 'around', 'and', 'begins', 'to', 'run', 'with', 'the', 'crowd', 'giving', 'chase']\n",
      "prediction: ['<BOS>', 'people', 'are', 'are', 'dancing']\n",
      "\n",
      "id: Dgf0VHMEtNs_57_66.avi\n",
      "answer: ['<BOS>', 'some', '<UNK>', 'are', 'operating', 'on', 'a', 'man']\n",
      "prediction: ['<BOS>', 'a', 'person', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: 5HAf_INrFy0_3_25.avi\n",
      "answer: ['<BOS>', 'the', 'cat', 'boxed', 'his', 'paws', 'like', 'the', 'boxers', 'on', 'the', 'television', 'he', 'was', 'watching']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: N2Cm0SLr0ZE_18_29.avi\n",
      "answer: ['<BOS>', 'a', 'child', 'is', 'playing', 'a', 'guitar', 'and', 'singing']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'playing', 'a']\n",
      "\n",
      "id: 5HAf_INrFy0_3_25.avi\n",
      "answer: ['<BOS>', 'the', 'cat', 'is', 'watching', 'a', 'boxing', 'match', 'on', 'tv']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: rl1rVk_xIOs_1_16.avi\n",
      "answer: ['<BOS>', 'a', 'toddler', 'dances', 'with', 'a', 'group', 'of', 'kids']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'a']\n",
      "\n",
      "id: 0lh_UWF9ZP4_27_31.avi\n",
      "answer: ['<BOS>', 'a', 'woman', 'is', 'slicing', 'ginger']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'slicing', 'a']\n",
      "\n",
      "id: 6JnGBs88sL0_4_10.avi\n",
      "answer: ['<BOS>', 'a', 'helicopter', 'is', 'landing']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: wkgGxsuNVSg_34_41.avi\n",
      "answer: ['<BOS>', 'a', 'young', 'child', 'is', 'running', 'around', 'in', 'circles', 'trying', 'to', '<UNK>', 'the', 'fish', 'on', 'the', 'rod', 'he', 'is', 'holding', 'when', 'he', 'drops', 'the', 'pole', 'and', 'runs', 'off']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: WWf0Z6ak3Dg_5_15.avi\n",
      "answer: ['<BOS>', 'a', 'dog', 'is', 'playing', 'with', 'a', 'very', 'large', 'ball']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: mmSQTI6gMNQ_120_128.avi\n",
      "answer: ['<BOS>', 'two', 'men', '<UNK>', 'behind', 'a', 'bush', 'as', 'the', 'ladies', 'passed', 'by']\n",
      "prediction: ['<BOS>', 'two', 'men', 'are', 'are', 'are']\n",
      "\n",
      "id: bqMmyY1ImkI_0_14.avi\n",
      "answer: ['<BOS>', 'the', 'boys', 'are', 'playing', 'in', 'a', 'portable', 'pool']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'a']\n",
      "\n",
      "id: 77iDIp40m9E_126_131.avi\n",
      "answer: ['<BOS>', 'a', 'dog', 'is', 'riding', 'a', 'skateboard']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: J_evFB7RIKA_104_120.avi\n",
      "answer: ['<BOS>', 'someone', 'sliced', 'the', 'green', 'bell', 'pepper']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'slicing', 'a']\n",
      "\n",
      "id: RZL9irxnhZ0_34_40.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'speaking']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "id: JntMAcTlOF0_50_70.avi\n",
      "answer: ['<BOS>', 'the', 'boy', 'ran', 'away', 'from', 'the', 'group', 'of', 'boys']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'in']\n",
      "\n",
      "id: N3A7944_UJw_63_70.avi\n",
      "answer: ['<BOS>', 'a', 'chef', 'cooking', 'food']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: u4T76jsPin0_0_11.avi\n",
      "answer: ['<BOS>', 'athletes', 'are', 'competing', 'in', 'hundred', 'meter', 'race']\n",
      "prediction: ['<BOS>', 'the', 'are', 'are', 'playing', 'playing']\n",
      "\n",
      "id: jTnrm338_KY_34_42.avi\n",
      "answer: ['<BOS>', 'a', '<UNK>', 'is', 'placed', 'in', 'a', 'box']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'cutting', 'a']\n",
      "\n",
      "id: qeKX-N1nKiM_0_5.avi\n",
      "answer: ['<BOS>', 'someone', 'is', 'pouring', 'sauce', 'on', 'a', 'dish', 'of', 'food']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'seasoning', 'some']\n",
      "\n",
      "id: UbmZAe5u5FI_132_141.avi\n",
      "answer: ['<BOS>', 'a', 'woman', 'is', '<UNK>', 'a', 'fish']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'cutting', 'meat', 'meat']\n",
      "\n",
      "id: 77iDIp40m9E_126_131.avi\n",
      "answer: ['<BOS>', 'the', 'dog', 'rode', 'the', 'skateboard']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: g1Gldu1KS44_8_14.avi\n",
      "answer: ['<BOS>', 'the', 'baby', 'elephant', 'is', 'eating']\n",
      "prediction: ['<BOS>', 'a', 'baby', 'is', 'is', 'walking', 'in']\n",
      "\n",
      "id: k5OKBX2e7xA_19_32.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'wheeling', 'a', 'small', 'motorcycle']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'riding', 'a']\n",
      "\n",
      "id: 1Sp2__RCT0c_11_15.avi\n",
      "answer: ['<BOS>', 'a', 'rocket', 'is', 'exploding']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'riding', 'a', 'a', 'a']\n",
      "\n",
      "id: ecm9gf2Pgkc_1_24.avi\n",
      "answer: ['<BOS>', 'the', 'woman', 'is', 'petting', 'the', 'rabbit']\n",
      "prediction: ['<BOS>', 'a', 'cat', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: k5OKBX2e7xA_19_32.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'does', 'a', 'wheelie', 'trick', 'on', 'a', 'motorcycle']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'riding', 'a']\n",
      "\n",
      "id: sZf3VDsdDPM_107_114.avi\n",
      "answer: ['<BOS>', 'the', 'man', 'smiled', 'as', 'he', 'took', 'a', 'bite', 'of', 'food']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'talking', 'a']\n",
      "\n",
      "id: Fe4tO5vW9_E_64_70.avi\n",
      "answer: ['<BOS>', 'a', 'cook', 'mixes', 'some', 'butter', 'in', 'a', 'pan']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'putting', 'eggs', 'into']\n",
      "\n",
      "id: lo4KcsBN--A_0_10.avi\n",
      "answer: ['<BOS>', 'a', 'turtle', 'is', 'walking', 'underwater']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a']\n",
      "\n",
      "id: He7Ge7Sogrk_47_70.avi\n",
      "answer: ['<BOS>', 'an', 'elephant', 'is', 'painting', 'on', 'a', 'piece', 'of', 'paper']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: N2Cm0SLr0ZE_18_29.avi\n",
      "answer: ['<BOS>', 'a', 'boy', 'is', 'singing', 'and', 'playing', 'a', 'guitar']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'playing', 'a']\n",
      "\n",
      "id: Fe4tO5vW9_E_64_70.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'cooking', 'some', 'dish']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'putting', 'eggs', 'into']\n",
      "\n",
      "id: j2Dhf-xFUxU_13_20.avi\n",
      "answer: ['<BOS>', 'a', 'person', 'is', 'slicing', '<UNK>', '', 'into', 'pieces']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'slicing', 'a', 'a']\n",
      "\n",
      "id: Fe4tO5vW9_E_64_70.avi\n",
      "answer: ['<BOS>', 'the', 'cook', 'is', 'melting', 'butter']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'putting', 'eggs', 'into']\n",
      "\n",
      "id: 8HB7ywgJuTg_131_142.avi\n",
      "answer: ['<BOS>', 'the', 'woman', 'is', 'seasoning', 'her', 'dish']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'frying']\n",
      "\n",
      "id: sJSmRik2c-c_1_7.avi\n",
      "answer: ['<BOS>', 'a', 'yellow', 'train', 'is', 'speeding', 'down', 'a', 'track']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'riding', 'a', 'a']\n",
      "\n",
      "id: MrQd1zUVRUM_103_110.avi\n",
      "answer: ['<BOS>', 'two', 'young', 'women', 'are', 'kissing']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: J_evFB7RIKA_104_120.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'slicing', 'a', 'pepper']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'slicing', 'a']\n",
      "\n",
      "id: g1Gldu1KS44_8_14.avi\n",
      "answer: ['<BOS>', 'a', 'baby', 'elephant', 'is', 'eating', 'leaves', 'off', 'a', 'small', 'tree']\n",
      "prediction: ['<BOS>', 'a', 'baby', 'is', 'is', 'walking', 'in']\n",
      "\n",
      "id: 04Gt01vatkk_248_265.avi\n",
      "answer: ['<BOS>', 'a', 'woman', 'chops', 'a', 'white', 'onion']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'a']\n",
      "\n",
      "id: e-j59PqJjSM_405_416.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'stirs', 'all', 'the', 'ingredients', 'of', 'a', '<UNK>', 'salad', 'placed', 'in', 'a', 'large', 'bowl', 'with', 'a', 'flat', 'steel', 'spoon', 'to', 'mix', 'them', 'well']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'mixing', 'a', 'a', 'a']\n",
      "\n",
      "id: ufFT2BWh3BQ_0_8.avi\n",
      "answer: ['<BOS>', 'two', 'pandas', 'are', 'playing']\n",
      "prediction: ['<BOS>', 'a', 'panda', 'panda', 'is', 'on', 'on', 'on']\n",
      "\n",
      "id: tcxhOGyrCtI_15_21.avi\n",
      "answer: ['<BOS>', 'a', 'cat', 'walks', 'across', 'the', 'grass']\n",
      "prediction: ['<BOS>', 'a', 'cat', 'is', 'is', 'playing', 'a', 'a', 'a']\n",
      "\n",
      "id: e-j59PqJjSM_405_416.avi\n",
      "answer: ['<BOS>', 'the', 'man', 'stirred', 'the', 'vegetables', 'in', 'the', 'big', 'bowl']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'mixing', 'a', 'a', 'a']\n",
      "\n",
      "id: pW9DFPqoIsI_26_50.avi\n",
      "answer: ['<BOS>', 'someone', 'is', 'sitting', 'on', 'the', 'floor', 'where', 'they', 'open', 'a', 'package', 'with', 'a', 'pair', 'of', 'scissors', 'and', 'remove', 'a', 'headset']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'cutting', 'a']\n",
      "\n",
      "id: J_evFB7RIKA_104_120.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'cut', 'a', 'bell', 'pepper', 'in', 'half', 'and', 'cleaned', 'out', 'the', 'seeds']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'slicing', 'a']\n",
      "\n",
      "id: bqMmyY1ImkI_0_14.avi\n",
      "answer: ['<BOS>', 'three', 'boys', 'are', 'playing', 'in', 'a', 'pool']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'a']\n",
      "\n",
      "id: 1Sp2__RCT0c_11_15.avi\n",
      "answer: ['<BOS>', 'a', 'rocket', 'bursts', 'into', 'flames', 'in', 'the', '<UNK>']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'riding', 'a', 'a', 'a']\n",
      "\n",
      "id: bqMmyY1ImkI_0_14.avi\n",
      "answer: ['<BOS>', 'kids', 'are', 'playing', 'in', 'a', 'pool']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'a']\n",
      "\n",
      "id: 77iDIp40m9E_126_131.avi\n",
      "answer: ['<BOS>', 'a', 'dog', 'is', 'riding', 'a', 'skateboard']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: lw7pTwpx0K0_38_48.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'putting', 'together', 'a', '<UNK>', '<UNK>', 'while', 'talking']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: f9Won2JpOEU_60_80.avi\n",
      "answer: ['<BOS>', 'a', 'cat', 'is', 'cleaning', 'itself']\n",
      "prediction: ['<BOS>', 'a', 'cat', 'is', 'playing', 'a', 'a']\n",
      "\n",
      "id: k06Ge9ANKM8_5_16.avi\n",
      "answer: ['<BOS>', 'a', 'dog', 'is', 'popping', 'balloons']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'playing', 'a']\n",
      "\n",
      "id: qLwgb3F0aPU_298_305.avi\n",
      "answer: ['<BOS>', 'the', 'men', 'are', 'exercising']\n",
      "prediction: ['<BOS>', 'people', 'are', 'are', 'are']\n",
      "\n",
      "id: qeKX-N1nKiM_0_5.avi\n",
      "answer: ['<BOS>', 'the', 'woman', 'is', 'pouring', 'soy', 'sauce', 'over', 'the', 'dish']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'seasoning', 'some']\n",
      "\n",
      "id: shPymuahrsc_5_12.avi\n",
      "answer: ['<BOS>', 'a', 'small', 'animal', 'is', 'looking', 'at', 'a', 'container']\n",
      "prediction: ['<BOS>', 'a', 'dog', 'is', 'is', 'the', 'in']\n",
      "\n",
      "id: q7pOFn8s4zc_263_273.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'and', 'a', 'woman', 'are', 'dancing']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'a']\n",
      "\n",
      "id: WWf0Z6ak3Dg_5_15.avi\n",
      "answer: ['<BOS>', 'a', 'french', 'bulldog', 'is', 'running', 'fast', 'and', 'playing', 'with', 'a', 'blue', 'yoga', 'ball', 'all', 'by', 'himself', 'in', 'a', 'field']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: wkgGxsuNVSg_34_41.avi\n",
      "answer: ['<BOS>', 'a', 'child', 'is', 'running', 'from', 'a', 'fish', 'on', 'the', 'end', 'of', 'a', 'fishing', 'pole']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: v7iIZXtpIb8_5_15.avi\n",
      "answer: ['<BOS>', 'a', 'boy', 'plays', 'a', 'piano']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: sZf3VDsdDPM_107_114.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'and', 'woman', 'eat']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'talking', 'a']\n",
      "\n",
      "id: dfOuTx66bJU_34_39.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'finding', 'something']\n",
      "prediction: ['<BOS>', 'a', 'man', 'and', 'and', 'a', 'a']\n",
      "\n",
      "id: 7NNg0_n-bS8_21_30.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'singing', 'and', 'playing', 'a', 'guitar']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'singing', 'a']\n",
      "\n",
      "id: 88DOMJ11q2M_84_87.avi\n",
      "answer: ['<BOS>', 'an', 'animated', 'woman', 'is', 'diving']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a']\n",
      "\n",
      "id: 7NNg0_n-bS8_21_30.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'playing', 'guitar', 'while', 'sitting', 'on', 'a', 'stool']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'singing', 'a']\n",
      "\n",
      "id: 8HB7ywgJuTg_131_142.avi\n",
      "answer: ['<BOS>', 'a', 'person', 'is', 'stirring', 'onions', 'in', 'a', 'pan']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'frying']\n",
      "\n",
      "id: e-j59PqJjSM_50_98.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'cutting', 'up', 'a', 'cucumber']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'slicing', 'a']\n",
      "\n",
      "id: k5OKBX2e7xA_19_32.avi\n",
      "answer: ['<BOS>', 'a', 'kid', 'is', 'doing', 'stunts', 'on', 'a', 'motorcycle']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'riding', 'a']\n",
      "\n",
      "id: q7pOFn8s4zc_263_273.avi\n",
      "answer: ['<BOS>', 'two', 'people', 'are', 'slow', 'dancing']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'a']\n",
      "\n",
      "id: EpMuCrbxE8A_107_115.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'playing', 'a', 'guitar']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'a']\n",
      "\n",
      "id: lw7pTwpx0K0_38_48.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'assembling', 'a', 'machine']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: qeKX-N1nKiM_0_5.avi\n",
      "answer: ['<BOS>', 'someone', 'is', 'pouring', 'sauce', 'over', 'food']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'seasoning', 'some']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "id: 7HcYJKMxpcg_20_28.avi\n",
      "answer: ['<BOS>', 'a', 'lion', 'is', 'walking', 'in', 'the', 'cage']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: IhwPQL9dFYc_124_129.avi\n",
      "answer: ['<BOS>', 'a', 'girl', 'is', 'slicing', 'a', 'butter', 'into', 'two', 'pieces']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'a']\n",
      "\n",
      "id: qeKX-N1nKiM_0_5.avi\n",
      "answer: ['<BOS>', 'a', 'person', 'pours', 'sauce', 'onto', 'a', 'plate']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'seasoning', 'some']\n",
      "\n",
      "id: wkgGxsuNVSg_34_41.avi\n",
      "answer: ['<BOS>', 'a', 'little', 'boy', 'has', 'a', 'fishing', 'rod', 'in', 'his', 'hand', 'with', 'a', 'fish', 'on', 'the', 'end', 'of', 'the', 'line', 'and', 'hes', 'running', 'around', 'in', 'a', 'circle']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: 4PcL6-mjRNk_11_18.avi\n",
      "answer: ['<BOS>', 'a', 'dog', 'returns', 'his', 'ball', 'to', 'a', 'machine']\n",
      "prediction: ['<BOS>', 'a', 'dog', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: wkgGxsuNVSg_34_41.avi\n",
      "answer: ['<BOS>', 'a', 'small', 'kid', 'dodging', 'the', 'fish', 'at', 'one', 'end', 'of', 'a', 'fishing', 'rod', 'while', 'swirling', 'it', 'runs', 'away', 'after', 'dropping', 'the', 'rod']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: inzk2fTUe1w_1_15.avi\n",
      "answer: ['<BOS>', 'a', 'person', 'is', 'peeling', 'a', 'banana']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a']\n",
      "\n",
      "id: aM-RcQj0a7I_37_55.avi\n",
      "answer: ['<BOS>', 'someone', 'boiled', 'food', 'in', '', 'water']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'cooking', 'in']\n",
      "\n",
      "id: q7pOFn8s4zc_263_273.avi\n",
      "answer: ['<BOS>', 'a', 'couple', 'dancing', 'together']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'a']\n",
      "\n",
      "id: 1Sp2__RCT0c_11_15.avi\n",
      "answer: ['<BOS>', 'something', 'is', '<UNK>']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'riding', 'a', 'a', 'a']\n",
      "\n",
      "id: DhwrBs96Kgk_120_124.avi\n",
      "answer: ['<BOS>', 'a', 'monkey', 'walking', '<UNK>', 'a', 'shallow', 'body', 'of', 'water']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'running', 'a', 'a']\n",
      "\n",
      "id: EpMuCrbxE8A_107_115.avi\n",
      "answer: ['<BOS>', 'the', 'group', '<UNK>', 'plays', 'a', 'song', 'onstage']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'a']\n",
      "\n",
      "id: sZf3VDsdDPM_107_114.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'and', 'a', 'woman', 'are', 'eating']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'talking', 'a']\n",
      "\n",
      "id: 6q1dX6thX3E_286_295.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'speaking', 'on', 'a', 'telephone']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a']\n",
      "\n",
      "id: 88DOMJ11q2M_84_87.avi\n",
      "answer: ['<BOS>', 'a', 'animated', 'woman', 'dived', 'off', 'a', 'diving', 'board']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a']\n",
      "\n",
      "id: aM-RcQj0a7I_37_55.avi\n",
      "answer: ['<BOS>', 'the', 'man', 'is', 'frying', 'food']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'cooking', 'in']\n",
      "\n",
      "id: ZbtpcGi2DWY_161_170.avi\n",
      "answer: ['<BOS>', 'a', 'polar', 'bear', 'walks', 'around', 'a', 'rockface']\n",
      "prediction: ['<BOS>', 'a', 'panda', 'is', 'is']\n",
      "\n",
      "id: IhwPQL9dFYc_124_129.avi\n",
      "answer: ['<BOS>', 'a', 'woman', 'is', 'slicing', 'through', 'a', 'slab', 'of', 'tofu']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'a']\n",
      "\n",
      "id: uJPupV4oLZ0_4_12.avi\n",
      "answer: ['<BOS>', 'a', 'tiny', 'toad', 'is', 'sitting', 'in', 'the', 'palm', 'of', 'someones', 'hand']\n",
      "prediction: ['<BOS>', 'a', 'baby', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: Je3V7U5Ctj4_569_576.avi\n",
      "answer: ['<BOS>', 'the', 'man', 'sprinkled', 'cheese', 'on', 'the', '<UNK>']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'putting', 'a', 'a', 'a']\n",
      "\n",
      "id: BtQtRGI0F2Q_15_20.avi\n",
      "answer: ['<BOS>', 'a', 'power', '<UNK>', 'is', '<UNK>', 'a', 'man']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'is', 'on', 'on']\n",
      "\n",
      "id: glrijRGnmc0_211_215.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'sets', 'the', '<UNK>', '<UNK>', 'on', 'the', '<UNK>', 'of', 'the', 'microwave', '<UNK>', 'in', 'the', 'kitchen', 'and', 'is', 'waiting']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: Dgf0VHMEtNs_57_66.avi\n",
      "answer: ['<BOS>', 'four', 'doctors', '<UNK>', 'on', 'a', 'patient']\n",
      "prediction: ['<BOS>', 'a', 'person', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: MrQd1zUVRUM_103_110.avi\n",
      "answer: ['<BOS>', 'two', 'girls', 'are', 'kissing']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: dfOuTx66bJU_34_39.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'running', 'along', 'a', 'wooded', 'path']\n",
      "prediction: ['<BOS>', 'a', 'man', 'and', 'and', 'a', 'a']\n",
      "\n",
      "id: 778mkceE0UQ_40_46.avi\n",
      "answer: ['<BOS>', 'a', '<UNK>', 'car', 'is', 'driving', 'down', 'a', 'highway']\n",
      "prediction: ['<BOS>', 'a', 'car', 'is', 'car', 'a', 'car', 'car', 'car']\n",
      "\n",
      "id: 77iDIp40m9E_126_131.avi\n",
      "answer: ['<BOS>', 'a', 'dog', 'rides', 'a', 'skateboard']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: uJPupV4oLZ0_4_12.avi\n",
      "answer: ['<BOS>', 'the', 'man', 'held', 'a', 'tiny', 'frog', 'in', 'his', 'hand']\n",
      "prediction: ['<BOS>', 'a', 'baby', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: 77iDIp40m9E_126_131.avi\n",
      "answer: ['<BOS>', 'a', 'dog', 'is', 'doing', 'skating', 'on', 'a', 'skateboard', 'on', 'the', 'road']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: 5HAf_INrFy0_3_25.avi\n",
      "answer: ['<BOS>', 'a', 'cat', 'paws', 'at', 'a', 'small', 'television']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a']\n",
      "\n",
      "id: 7HcYJKMxpcg_20_28.avi\n",
      "answer: ['<BOS>', 'the', 'lion', '<UNK>', 'in', 'his', 'pen']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: g1Gldu1KS44_8_14.avi\n",
      "answer: ['<BOS>', 'a', 'baby', 'elephant', 'eats', 'a', 'plant']\n",
      "prediction: ['<BOS>', 'a', 'baby', 'is', 'is', 'walking', 'in']\n",
      "\n",
      "id: k5OKBX2e7xA_19_32.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'riding', 'a', 'motorcycle', 'in', 'circles', 'while', 'balancing', 'on', 'its', 'rear', 'wheel']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'riding', 'a']\n",
      "\n",
      "id: j2Dhf-xFUxU_13_20.avi\n",
      "answer: ['<BOS>', '<UNK>', 'are', 'cut']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'slicing', 'a', 'a']\n",
      "\n",
      "id: dfOuTx66bJU_34_39.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'running']\n",
      "prediction: ['<BOS>', 'a', 'man', 'and', 'and', 'a', 'a']\n",
      "\n",
      "id: 0lh_UWF9ZP4_27_31.avi\n",
      "answer: ['<BOS>', 'the', 'lady', 'cut', 'the', 'vegetable', 'up', 'finely']\n",
      "prediction: ['<BOS>', 'a', 'woman', 'is', 'slicing', 'a']\n",
      "\n",
      "id: qvg9eM4Hmzk_4_10.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'lifting', 'a', 'car']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a', 'a', 'a']\n",
      "\n",
      "id: Je3V7U5Ctj4_569_576.avi\n",
      "answer: ['<BOS>', 'a', 'man', 'is', 'making', 'a', 'pizza']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'putting', 'a', 'a', 'a']\n",
      "\n",
      "id: lo4KcsBN--A_0_10.avi\n",
      "answer: ['<BOS>', 'a', 'turtle', 'is', 'walking', 'under', 'water']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'a']\n",
      "\n",
      "id: 5YJaS2Eswg0_22_26.avi\n",
      "answer: ['<BOS>', 'a', 'young', 'girl', 'jumps', 'rope']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'riding', 'a', 'a', 'a']\n",
      "\n",
      "id: zv2RIbUsnSw_335_341.avi\n",
      "answer: ['<BOS>', 'medical', 'personnel', 'wheel', 'a', 'woman', 'into', 'a', 'hospital', 'room']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'is', 'the', 'a']\n",
      "\n",
      "id: EpMuCrbxE8A_107_115.avi\n",
      "answer: ['<BOS>', 'someone', 'is', 'playing', 'guitar']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'a']\n",
      "\n",
      "id: dfOuTx66bJU_34_39.avi\n",
      "answer: ['<BOS>', 'the', 'man', 'is', 'running']\n",
      "prediction: ['<BOS>', 'a', 'man', 'and', 'and', 'a', 'a']\n",
      "\n",
      "id: k06Ge9ANKM8_5_16.avi\n",
      "answer: ['<BOS>', 'a', 'dog', 'is', 'running', 'around', 'biting', 'and', 'popping', 'many', 'balloons', 'lying', 'on', 'a', 'floor']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'playing', 'a']\n",
      "\n",
      "id: N2Cm0SLr0ZE_18_29.avi\n",
      "answer: ['<BOS>', 'a', 'little', 'boy', 'is', 'playing', 'guitar']\n",
      "prediction: ['<BOS>', 'a', 'man', 'is', 'playing', 'playing', 'a']\n",
      "\n",
      "id: qLwgb3F0aPU_298_305.avi\n",
      "answer: ['<BOS>', 'men', 'are', 'exercising', 'in', 'a', 'field']\n",
      "prediction: ['<BOS>', 'people', 'are', 'are', 'are']\n",
      "\n",
      "id: 3qqEKTPxLNs_1_15.avi\n",
      "answer: ['<BOS>', 'a', 'baby', 'plays', 'on', 'a', 'bed']\n",
      "prediction: ['<BOS>', 'a', 'baby', 'is', 'is']\n",
      "\n",
      "id: HV12kTtdTT4_5_14.avi\n",
      "answer: ['<BOS>', 'a', 'cat', 'is', 'watching', 'television']\n",
      "prediction: ['<BOS>', 'a', 'boy', 'is', 'playing', 'a']\n",
      "\n",
      "Save file: output.txt\n",
      "Validation: 1600/16, done...Total Loss: 60.5976\n"
     ]
    }
   ],
   "source": [
    "saver_path = 'saved_model/'\n",
    "print('saver path: ' + saver_path)\n",
    "latest_checkpoint = tf.train.latest_checkpoint(saver_path)\n",
    "\n",
    "ckpt_path = 'saved_model/trained_model.ckpt-959'\n",
    "\n",
    "val_saver.restore(val_sess, ckpt_path)\n",
    "\n",
    "epo_loss_val = 0\n",
    "txt = open(output_filename, 'w')\n",
    "\n",
    "total_loss_val = 0\n",
    "\n",
    "for j in range(0,n_batches_test):\n",
    "    data_batch_val = np.array(vid_batch_test[j])\n",
    "    label_batch_val = np.array(intencode_batch_test[j])\n",
    "    id_batch_val = id_batch_test[j]\n",
    "    caption_lens_batch_val = np.array(cap_len_batch_test[j])\n",
    "\n",
    "    p_val, summ = val_sess.run([dec_pred_val, summary_val], \n",
    "                                feed_dict={feat_val: data_batch_val,\n",
    "                                           captions_val: label_batch_val,\n",
    "                                           cap_len_val: caption_lens_batch_val})\n",
    "    \n",
    "    #print(loss_val)\n",
    "\n",
    "    seq_val = pred_print(p_val, caption_lens_batch_val, label_batch_val, index2token, batch_size_test, id_batch_val)\n",
    "    total_loss_val += loss_val\n",
    "\n",
    "    for k in range(0, batch_size_test):\n",
    "            txt.write(id_batch_val[k] + \",\" + seq_val[k] + \"\\n\")\n",
    "\n",
    "print('\\nSave file: ' + output_filename)\n",
    "txt.close()\n",
    "\n",
    "from subprocess import call\n",
    "\n",
    "call(['python3', 'MLDS_hw2_1_data/bleu_eval.py', output_filename])\n",
    "\n",
    "print(\"Validation: \" + str((j+1) * batch_size_test) + \"/\" + \\\n",
    "        str(n_batches_test) + \", done...\" \\\n",
    "        + \"Total Loss: \" + \"{:.4f}\".format(total_loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq = pred_print(p, caption_lens_batch, label_batch, index2token, batch_size, id_batch_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.randint(batch_size,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Video_Caption_Generator(dim_image=n_features, \n",
    "#                                 n_words = n_words, \n",
    "#                                 dim_hidden = n_hidden, \n",
    "#                                 batch_size=batch_size, \n",
    "#                                 n_lstm_steps=80,\n",
    "#                                 n_video_lstm_step=80,\n",
    "#                                 n_caption_lstm_step=80,\n",
    "#                                 bias_init_vector=bias_init_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_loss, tf_video, tf_video_mask, tf_caption, tf_caption_mask, tf_probs = model.build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_words = n_words\n",
    "\n",
    "# with tf.Graph().as_default() as graph:\n",
    "    \n",
    "\n",
    "#     weights_enc = tf.Variable(tf.random_uniform([n_features, n_hidden],-0.1,0.1),name=\"weights_enc\")\n",
    "#     bias_enc = tf.Variable(tf.zeros([n_hidden]),name=\"bias_enc\")\n",
    "\n",
    "#     weights_dec = tf.Variable(tf.random_uniform([n_hidden, n_words],-0.1,0.1),name=\"weights_dec\")\n",
    "#     bias_dec = tf.Variable(tf.zeros([n_words]),name=\"bias_dec\")\n",
    "\n",
    "\n",
    "#     x_video = tf.placeholder(tf.float32, (None, no_of_frames, n_features),'video_features') #inputs\n",
    "\n",
    "#     batch_size = tf.shape(x_video)[0]\n",
    "    \n",
    "#     x_video_drop = tf.nn.dropout(x_video, 0.5)\n",
    "    \n",
    "#     x_video_flat = tf.reshape(x_video_drop,[-1,n_features])\n",
    "\n",
    "#     y_label = tf.placeholder(tf.int32,(None, sizeof_sentence),'captions') #outputs\n",
    "\n",
    "\n",
    "#     #sampling = tf.placeholder(tf.bool, [sizeof_sentence], name='sampling')\n",
    "#     padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "#     loss = 0.0\n",
    "\n",
    "#     ########## DATA ###########\n",
    "#     # Example: For i = 0\n",
    "#     #batch_x = np.array(vid_batch[0])\n",
    "#     #batch_y = np.array(intencode_batch[0])\n",
    "#     ###########################\n",
    "\n",
    "#     input_embedding = tf.matmul(x_video_flat,weights_enc) + bias_enc\n",
    "#     input_embedding = tf.reshape(input_embedding,[-1, no_of_frames,n_hidden])\n",
    "#     input_embed = tf.transpose(input_embedding, perm=[1, 0, 2])\n",
    "\n",
    "#     with tf.device(\"/cpu:0\"):\n",
    "#         output_embedding = tf.Variable(tf.random_uniform((n_words, n_hidden),-0.1,0.1), name='dec_embedding')\n",
    "#     # output_embed = tf.nn.embedding_lookup(output_embedding,y_label)\n",
    "    \n",
    "#     ## ENCODING #################################\n",
    "    \n",
    "#     with tf.variable_scope(\"LSTM1\"):\n",
    "#         lstm1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden,state_is_tuple=True)\n",
    "#         lstm1 = tf.contrib.rnn.DropoutWrapper(lstm1, output_keep_prob=0.5)    \n",
    "\n",
    "#     with tf.variable_scope(\"LSTM2\"):\n",
    "#         lstm2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "#         lstm2 = tf.contrib.rnn.DropoutWrapper(lstm2, output_keep_prob=0.5)    \n",
    "\n",
    "\n",
    "#     state1 = lstm1.zero_state(batch_size, dtype=tf.float32)\n",
    "#     state2 = lstm2.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "#     for i in range(0, no_of_frames):\n",
    "        \n",
    "#         if i > 0:\n",
    "#                 tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "#         with tf.variable_scope(\"LSTM1\"):\n",
    "#             output1, state1 = lstm1(input_embed[i,:,:], state1)\n",
    "\n",
    "#         with tf.variable_scope(\"LSTM2\"):\n",
    "#             output2, state2 = lstm2(tf.concat([padding, output1], axis=1), state2)\n",
    "    \n",
    "#     ## DECODING ##################################\n",
    "    \n",
    "#     bos = tf.ones([batch_size, n_hidden])\n",
    "#     padding_in = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "#     logits = []\n",
    "#     cross_ent_list=[]\n",
    "#     max_prob_index = None\n",
    "\n",
    "\n",
    "#     for i in range(0, MAX_WORDS):\n",
    "        \n",
    "#         tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        \n",
    "#         with tf.variable_scope(\"LSTM1\"):\n",
    "#             output1, state1 = lstm1(padding_in, state1)\n",
    "            \n",
    "#         if i == 0:\n",
    "            \n",
    "#             with tf.variable_scope(\"LSTM2\"):\n",
    "#                 con = tf.concat([bos, output1], axis=1)\n",
    "#                 output2, state2 = lstm2(con, state2)\n",
    "                \n",
    "#         else:\n",
    "            \n",
    "#             with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "#                 feed_in = y_label[:,i]\n",
    "#                 #feed_in = tf.argmax()\n",
    "#                 output_embed = tf.nn.embedding_lookup(output_embedding,feed_in)\n",
    "                \n",
    "#             with tf.variable_scope(\"LSTM2\"):\n",
    "#                 con = tf.concat([output_embed, output1], axis=1)\n",
    "#                 output2, state2 = lstm2(con, state2)\n",
    "\n",
    "#         logit_words = tf.matmul(output2, weights_dec) + bias_dec\n",
    "#         logits.append(logit_words)\n",
    "\n",
    "#         word_i = y_label[:,i]\n",
    "\n",
    "#         one_hot_labels = tf.one_hot(word_i, n_words, on_value = 1, off_value = None, axis = 1) \n",
    "#         cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=one_hot_labels)\n",
    "#         cross_ent_list.append(cross_entropy)\n",
    "        \n",
    "        \n",
    "#         #current_loss = tf.reduce_sum(cross_entropy)/batch_size\n",
    "#         #loss = loss + current_loss\n",
    "\n",
    "#     cross_entropy_tensor = tf.stack(cross_ent_list, 1)\n",
    "#     loss = tf.reduce_sum(cross_entropy_tensor, axis=1)\n",
    "#     loss = tf.divide(loss, tf.cast(tf.Variable(sizeof_sentence), tf.float32))\n",
    "\n",
    "#     loss = tf.reduce_mean(loss, axis=0)\n",
    "    \n",
    "#     summary = tf.summary.scalar('training_loss', loss)\n",
    "\n",
    "#     params = tf.trainable_variables()\n",
    "#     #optimizer = tf.train.AdamOptimizer(learning_rate)#.minimize(loss_op)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     train_op = optimizer.minimize(loss)\n",
    "\n",
    "#     #train_step = optimizer.minimize(loss)\n",
    "    \n",
    "# #     gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "# #     gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "# #     train_op = optimizer.apply_gradients(zip(gradients, params))\n",
    "    \n",
    "# #     logits = tf.stack(logits, axis = 0)\n",
    "# #     logits = tf.reshape(logits, (sizeof_sentence, batch_size, n_words))\n",
    "# #     logits = tf.transpose(logits, [1, 0, 2])\n",
    "# #     preds = tf.argmax(logits,2)\n",
    "# #     correct_pred = tf.equal(tf.argmax(preds,1), tf.argmax(y_label,1))\n",
    "# #     accuracy = tf.reduce_mean(correct_pred)\n",
    "\n",
    "#     logits = tf.stack(logits,axis=0)\n",
    "#     logits = tf.transpose(logits, [1, 0, 2])\n",
    "#     output_preds = tf.argmax(logits,2)\n",
    "    \n",
    "#     #correct_pred = tf.equal(tf.argmax(output_preds, 1), tf.argmax(y_label, 1))\n",
    "#     #accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "#     saver = tf.train.Saver(max_to_keep=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "# gpu_config = tf.ConfigProto()\n",
    "\n",
    "# with tf.Session(graph=graph,config=gpu_config) as sess:\n",
    "\n",
    "#     loss_list_train = []\n",
    "#     loss_list_test = []\n",
    "#     preds_dict = {}\n",
    "\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     epochs = 10\n",
    "\n",
    "#     #training\n",
    "#     n=0\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "\n",
    "#         for i in range(n_batches):\n",
    "\n",
    "#             batch_x = np.array(vid_batch[i])\n",
    "#             #batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "#             batch_y = np.array(intencode_batch[i])\n",
    "\n",
    "#             _, batch_loss, preds = sess.run([train_op, loss, logits], feed_dict = {x_video: batch_x, y_label: batch_y})        \n",
    "\n",
    "#             loss_list_train.append(batch_loss)\n",
    "#             print(\"train: %f \" % (batch_loss))\n",
    "\n",
    "            \n",
    "#             n = n+1\n",
    "\n",
    "       \n",
    "#     #testing\n",
    "    \n",
    "#         saver.save(sess,ckpt_path, global_step=n)\n",
    "#         print('Model saved at ' + ckpt_path)\n",
    "        \n",
    "    \n",
    "#     for i in range(n_batches_test):\n",
    "\n",
    "#         batch_x_test = np.array(vid_batch_test[i])\n",
    "#         #batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "#         batch_y_test = np.array(intencode_batch_test[i])\n",
    "\n",
    "#         acc = sess.run(accuracy, feed_dict = {x_video: batch_x_test, y_label: batch_y_test})        \n",
    "#         print(\"accuracy %f\" % acc)\n",
    "\n",
    "# #         loss_list_test.append(batch_loss)\n",
    "# #         print(\"test:\", batch_loss)\n",
    "    \n",
    "# #         preds_dict[i] = batch_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"encoding\") as encoding_scope:\n",
    "#     lstm_enc = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "#     _, last_state = tf.nn.dynamic_rnn(lstm_enc, inputs=input_embed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "#     # TODO: create the decoder LSTMs, this is very similar to the above\n",
    "#     # you will need to set initial_state=last_state from the encoder\n",
    "#     lstm_dec = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "#     dec_outputs, _ = tf.nn.dynamic_rnn(lstm_dec,inputs=output_embed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #connect outputs to \n",
    "# logits = tf.contrib.layers.fully_connected(dec_outputs, num_outputs=len(index2token), activation_fn=None) \n",
    "\n",
    "# with tf.name_scope(\"optimization\"):\n",
    "#     # Loss function\n",
    "#     loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, sizeof_sentence]))\n",
    "#     # Optimizer\n",
    "#     optimizer = tf.train.RMSPropOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dec.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dec[0].get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_video.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from utilities import show_graph\n",
    "# show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def RNN(x, weights1, biases1):\n",
    "    \n",
    "#     x = tf.unstack(x,no_of_frames,1)\n",
    "    \n",
    "#     lstm_encoder = tf.keras.layers.LSTM(n_hidden, return_state=True) #reuse=tf.AUTO_REUSE)\n",
    "#     output_encoder,state_h,state_c = lstm_encoder(x) #,dtype=tf.float32)\n",
    "#     encoder_states = [state_h,state_c]\n",
    "    \n",
    "#     decoder\n",
    "    \n",
    "#     return tf.matmul(output1[-1],weights1) + bias1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(vid_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = RNN(x_video,weights1,bias1)\n",
    "# prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "# loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_label))\n",
    "\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "# train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# # Evaluate model (with test logits, for dropout to be disabled)\n",
    "# correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_label, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_y = np.array(intencode_batch[1])\n",
    "# np.shape(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_x = np.array(vid_batch[0])\n",
    "# print(np.shape(batch_x))\n",
    "# batch_x = np.reshape(batch_x,[-1,n_features])\n",
    "# np.shape(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_dict[0]\n",
    "# def predicted_sentence(preds_dict):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_x = np.array(vid_batch[0])\n",
    "# batch_y = np.array(intencode_batch[0])\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "\n",
    "#     sess.run(train_op, feed_dict={x_video: batch_x, y_label: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# image_emb = tf.nn.xw_plus_b(x_video, weights1, bias1) \n",
    "# #image_emb = tf.reshape(image_emb, [batch_size, no_of_frames, n_hidden])\n",
    "\n",
    "# #lstm2 = tf.keras.layers.LSTMCell(n_hidden)\n",
    "\n",
    "# padding = tf.zeros([batch_size, n_hidden])\n",
    "\n",
    "\n",
    "# #Only read the frames\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "                \n",
    "# logit_words = tf.nn.xw_plus_b(output2, weights2, bias2)\n",
    "# cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logit_words,onehot_encoded)\n",
    "\n",
    "# loss = tf.reduce_sum(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     with sess.as_default():\n",
    "#         print(tf.nn.embedding_lookup(onehot_encoded,[1]).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Class",
   "language": "python",
   "name": "tf_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
